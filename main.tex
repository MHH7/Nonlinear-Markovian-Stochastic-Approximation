\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{outline} \usepackage{pmgraph} \usepackage[normalem]{ulem}
\usepackage{graphicx} \usepackage{verbatim}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{tgbonum}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{lastpage}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
% \usepackage{minted} % need `-shell-escape' argument for local compile



\newcommand{\norm}[1]{\|#1 \|}
\newcommand{\Exs}{\mathbb{E}}
\newcommand{\thetastar}{\theta^*}
\newcommand{\constLPH}[1]{L_{PH}^{(#1)}}
\newcommand{\constT}[1]{T_{#1}}
\newcommand{\constTprime}[1]{T_{#1}^{\prime}}
\newcommand{\mwlcomment}[1]{{\color{orange} #1}}
\newcommand{\stepsize}{\alpha}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

\begin{document}

\fontfamily{cmr}\selectfont
\title{ \normalsize \textsc{}
	\\ [2.0cm]
	\HRule{0.5pt} \\
	\LARGE \textbf{\uppercase{Nonlinear Markovian Stochastic Approximation}
		\HRule{2pt} \\ [0.5cm]
		\normalsize \today \vspace*{5\baselineskip}}
}

\date{}

\author{
	Mohammadhadi Hadavi \\ 
	Prof. Hoi-To Wai - Chinese University of Hong Kong \\
	Prof. Wenlong Mou - University of Toronto}

\maketitle
\newpage
%\tableofcontents
%\newpage

\section{Preliminaries}

\textbf{Notations} The Euclidean norm is denoted by $\norm{.}$. The lowercase letter $c$ and its derivatives $c^{\prime}, c_{0},$ etc. denote universal numerical constants, whose value may change from line to line. As we are primarily interested in dependence of $\stepsize$ and $k$, we adopt the following big-$O$ notation: $\norm{f} = \mathcal{O}\left(h\left(\stepsize, k\right)\right)$ if it holds that $\norm{f} \le s \cdot \norm{h\left(\stepsize, k\right)}$ for some constant $s > 0$.

We use of the following iteration scheme:
\begin{align*}
	\theta_{t + 1} = \theta_{t} + \stepsize\left(g\left(\theta_{t}, X_{t + 1}\right) + \xi_{t + 1}\left(\theta_{t}\right)\right)
\end{align*}


\subsection{Assumptions}
\textbf{Assumption 1} \textit{
	For each $X \in \mathcal{X}$, the function $g\left(\theta, X\right)$ is three times continuously differentiable in $\theta$ with uniformly bounded first to third derivatives, i.e., $\sup_{\theta \in \mathbb{R}^{d}}\norm{g^{(i)}\left(\theta, X\right)} < \infty$ for $i = 1, 2, 3, X \in \mathcal{X}$. Moreover, there exists a constant $L_{1} > 0$ such that (1) $\norm{g^{(i)}\left(\theta, X\right) - g^{(i)}\left(\theta^{\prime}, X\right)} \le L_{1}$, for all $\theta, \theta^{\prime} \in \mathbb{R}^{d}, i = 0, 1, 2$ and $X \in \mathcal{X}$, and (2) $\norm{g\left(0, X\right)} \le L_{1}$ for all $X \in \mathcal{X}$.
}

Assumption 1 implies that $g\left(\theta, X\right)$ is $L_{1}$-Lipschitz w.r.t $\theta$ uniformly in $X$. The above assumption immediately implies that the growth of $\norm{g}$ and $\norm{\bar{g}}$ will be at most linear in $\theta$, i.e., $\norm{g\left(\theta, X\right)} \le L_{1}\left(\norm{\theta - \thetastar} + 1\right)$ and $\norm{\bar{g}\left(\theta\right)} \le L_{1}\left(\norm{\theta - \thetastar} + 1\right)$. 
\\
\\
\textbf{Assumption 2} \textit{
	There exists $\mu > 0$ such that $\left\langle \theta - \theta^{\prime}, \bar{g}(\theta) - \bar{g}(\theta^{\prime}) \right\rangle \le -\mu\norm{\theta - \theta^{\prime}}^{2}, \forall \theta, \theta^{\prime} \in \mathbb{R}^{d}$. Consequently, the target equation $\bar{g}(\theta) = 0$ has a unique solution $\thetastar$.
}
\\

Denote by $\mathcal{F}_{k}$ the filtration generated by $\{X_{t + 1}, \theta_{t}, \xi_{t + 1}\}_{t = 0}^{k - 1} \cup \{X_{k + 1}, \theta_{k}\}$.
\\
\textbf{Assumption 3} \textit{
	Let $p \in \mathbb{Z}_{+}$ be given. The noise sequence $\left(\xi_{k}\right)_{k \ge 1}$ is a collection of i.i.d random fields satisfying the following conditions with $L_{2, p} > 0$:
	$$\Exs\left[\xi_{k + 1}(\theta) | \mathcal{F}_{k}\right] = 0 \quad \text{and} \quad \Exs^{1 / (2p)}\left[\norm{\xi_{1}(\theta)}^{2p}\right] \le L_{2, p}\left(\norm{\theta - \thetastar} + 1\right), \quad \forall \theta \in \mathbb{R}^{d}.$$
	Define $C(\theta) = \Exs\left[\xi_{1}(\theta)^{\otimes 2}\right]$ and assume that $C(\theta)$ is at least twice differentiable. There also exists $M_{\epsilon}, k_{\epsilon} \ge 0$ such that for $\theta \in \mathbb{R}^{d}$, we have $\max_{i = 1, 2}\norm{C^{(i)}(\theta)} \le M_{\epsilon}\{1 + \norm{\theta - \thetastar}^{k_{\epsilon}}\}$.
}
In the sequel, we set $L \coloneq L_{1} + L_{2}$, and without loss of generality, we assume $L \ge 1$.
\\
\\
\textbf{Assumption 4} \textit{
	There exists a Borel measurable function $\hat{g}: \mathbb{R}^{d} \times \mathcal{X} \to \mathbb{R}^{d}$ where for each $\theta \in \mathbb{R}^{d}, X \in \mathcal{X}$,
	\begin{align*}
		\hat{g}\left(\theta, X\right) - P_{\theta}\hat{g}\left(\theta, X\right) = g\left(\theta, X\right) - \bar{g}\left(\theta\right).
	\end{align*}
}
\\
\textbf{Assumption 5} \textit{
	There exists $\constLPH{0} <‌ \infty$ and $\constLPH{1} < \infty$ such that, for all $\theta \in \mathbb{R}^{d}$ and $X \in \mathcal{X}$, one has $\norm{\hat{g}\left(\theta, X\right)} \le \constLPH{0}$, $\norm{P_{\theta}\hat{g}\left(\theta, X\right)} \le \constLPH{0}$. Moreover, for $\left(\theta, \theta^{\prime}\right) \in \mathcal{H}^{2}$,
	\begin{align*}
		\sup_{X \in \mathcal{X}}\norm{P_{\theta}\hat{g}\left(\theta, X\right) - P_{\theta^{\prime}}\hat{g}\left(\theta^{\prime}, X\right)} \le \constLPH{1}\norm{\theta - \theta^{\prime}}.
	\end{align*}
}
\\
\textbf{Assumption 6} \textit{
	For any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$, we have $\sup_{X \in \mathcal{X}}\norm{P_{\theta}\left(X, .\right) - P_{\theta^{\prime}}\left(X, .\right)}_{TV} \le L_{P}\norm{\theta - \theta^{\prime}}$.
}
\\
\textbf{Assumption 7} \textit{
	For any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$, we have $\sup_{X \in \mathcal{X}}\norm{g\left(\theta, X\right) - g\left(\theta^{\prime}, X\right)} \le L_{H}\norm{\theta - \theta^{\prime}}$.
}
\\
\textbf{Assumption 8} \textit{
	There exists $\rho < 1$, $K_{P} < \infty$ such that
	\begin{align*}
		\sup_{\theta \in \mathbb{R}^{d}, X \in \mathcal{X}} \norm{P_{\theta}^{n}\left(X, .\right) - \pi_{\theta}(.)}_{TV} \le \rho^{n}K_{P},
	\end{align*}
}
\\
\textbf{Lemma 1} \textit{
	Assume that assumptions 6-8 hold. Then, for any $\theta \in \mathbb{R}^{d}$ and $X \in \mathcal{X}$,
	\begin{align*}
		\norm{\hat{g}\left(\theta, X\right)} \le \frac{\sigma K_{P}}{1 - \rho},
	\end{align*}
	\begin{align*}
		\norm{P_{\theta}\hat{g}\left(\theta, X\right)} \le \frac{\sigma \rho K_{P}}{1 - \rho}.
	\end{align*}
	Moreover, for any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$ and $X \in \mathcal{X}$,
	\begin{align*}
		\norm{P_{\theta}\hat{g}\left(\theta, X\right) - P_{\theta^{\prime}}\hat{g}\left(\theta^{\prime}, X\right)} \le \norm{\theta - \theta^{\prime}},
	\end{align*}
	where
	\begin{align*}
		\constLPH{1} = \frac{K_{P}^{2}\sigma L_{P}}{(1 - \rho)^{2}}\left(2 + K_{P}\right) + \frac{K_{P}}{1 - \rho}L_{H}.
	\end{align*}
}
Proof of this lemma can be found in \cite{karimi2019non}, Lemma 7.
\section{Error Bound}


\subsection{Base Case}

For the base case analysis, we can write:
\begin{align*}
	& \Exs\left[\norm{\theta_{k +‌ 1} - \thetastar}^{2}\right] - \Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right] = \\
	& 2\stepsize \Exs\left[\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) \right\rangle\right] + \stepsize^{2}\Exs\left[\norm{g\left(\theta_{k}, X_{k + 1}\right)}^{2}\right] +‌ \stepsize^{2}\Exs\left[\norm{\xi_{k +‌ 1}\left(\theta_{k}\right)}^{2}\right] =\\
	& 2\stepsize\Exs\left[\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right)\right\rangle\right] + 2\stepsize\Exs\left[\left\langle \theta_{k} - \thetastar, \bar{g}\left(\theta_{k}\right) \right\rangle\right] + \stepsize^{2}\Exs\left[\norm{g\left(\theta_{k}, X_{k + 1}\right)}\right] + \stepsize^{2}\Exs\left[\norm{\xi_{k + 1}\left(\theta_{k}\right)}^{2}\right].
\end{align*}
It is easy to see that under Strong Monotonicity assumption, we have
\begin{align*}
	\left\langle \theta_{k} - \thetastar, \bar{g}\left(\theta_{k}\right)\right\rangle = \left\langle \theta_{k} - \thetastar, \bar{g}\left(\theta_{k}\right) +‌ \bar{g}\left(\thetastar\right)\right\rangle \le -\mu\norm{\theta_{k} - \thetastar}^{2}.
\end{align*}
\\
Additionally, under Assumption 1 and 3, we have the following upper bound
\begin{align*}
	&‌ \stepsize^{2}\left(\Exs\left[\norm{g\left(\theta_{k}, X_{k + 1}\right)}^{2}\right] + \Exs\left[\norm{\xi_{k + 1}\left(\theta_{k}\right)}^{2}\right]\right)\\
	& \le \stepsize^{2}\left(L_{1}^{2}\Exs\left[\left(\norm{\theta_{k} - \thetastar} + 1\right)^{2}\right] + L_{2}^{2}\Exs\left[\left(\norm{\theta_{k} - \thetastar} + 1\right)^{2}\right]\right)\\
	& \le 2\stepsize^{2}L^{2}\left(\Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right] + 1\right).
\end{align*}
\\
Therefore, we have
\begin{align*}
	\Exs\left[\norm{\theta_{k +‌ 1} - \thetastar}^{2}\right] \le \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)\Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right] +‌ 2\stepsize^{2}L^{2} + 2\stepsize\Exs\left[\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right)\right\rangle\right]
\end{align*}
Solving this recursion gives us the following inequality:
\begin{align*}
	\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] & \le \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{k + 1}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] \\
	& + \sum_{t = 0}^{k}\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{t}2\stepsize^{2}L^{2} \\
	& + \sum_{t = 0}^{k}2\stepsize\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{k - t}\Exs\left[\left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right) \right\rangle\right].
\end{align*}

For notational simplicity we define $\gamma_{t} \coloneq 2\stepsize\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{k - t}$ for $0 \le t \le k$.

The second term above is just a geometric series which is equal to $2\stepsize^{2}L^{2}\left(\stepsize L^{2} + \mu\right)^{k}$.

Now, we can upper bound the third summand using below decomposition:
\begin{align*}
	\Exs\left[ \sum_{t = 0}^{k} \gamma_{t}\left\langle \theta_{t} - \thetastar, g(\theta_{t}, X_{t + 1}) - \bar{g}(\theta_{t}) \right\rangle \right] = \Exs\left[ A_{1} + A_{2} + A_{3} + A_{4} + A_{5}\right]
\end{align*}
with
\begin{align*}
	A_{1} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, \hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) \right\rangle,\\
	A_{2} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) - P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1}, X_{t} \right) \right\rangle,\\
	A_{3} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \theta_{t - 1}, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1}, X_{t}\right) \right\rangle,\\
	A_{4} \coloneq & \sum_{t = 1}^{k}\left(\gamma_{t} - \gamma_{t - 1}\right)\left\langle \theta_{t - 1} - \thetastar, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1} - \thetastar, X_{t}\right) \right\rangle,\\
	A_{5} \coloneq & \gamma_{0}\left\langle \theta_{0} - \thetastar, \hat{g}\left(\theta_{0}, X_{0}\right) \right\rangle + \gamma_{k}\left\langle \theta_{k} - \thetastar, P_{\theta_{k}}\hat{g}\left(\theta_{k}, X_{k + 1}\right)\right\rangle
\end{align*}

For $A_{1}$, we note that $\hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right)$ is a martingale difference sequence [cf. ?] and therefore we have $\Exs[A_{1}] = 0$ by taking the total expectation.

For $A_{2}$, applying Cauchy-Schwarz inequality and \ref{MCLipschitzness}, we have
\begin{align*}
	A_{2} & \le \sum_{t = 1}^{k}\constLPH{1}\gamma_{t}\norm{\theta_{t} - \thetastar}\;\norm{\theta_{t} - \theta_{t - 1}}\\
	& = \sum_{t = 1}^{k}\stepsize \constLPH{1}\gamma_{t}\norm{\theta_{t} - \thetastar}\;\norm{g(\theta_{t}, X_{t + 1}) + \xi_{t +‌1}(\theta_{t})}\\
	& \le \sum_{t = 1}^{k}\stepsize\constLPH{1} \gamma_{t}\norm{\theta_{t} - \thetastar}\left( L_{1}\left(\norm{\theta_{t} - \thetastar} + 1\right) + L_{2}\left(\norm{\theta_{t} - \thetastar} + 1\right)\right)\\
	& \le \sum_{t = 1}^{k}\frac{\stepsize\constLPH{1}\gamma_{t}}{2}\left(3\norm{\theta_{t} - \thetastar}^{2} + 1\right)
\end{align*}




where the third line follows from the Lipschitzness condition and the assumption of
$$\Exs^{1 / 2}\left[\norm{\xi_{t + 1}\left(\theta_{t}\right)}^{2} | \mathcal{F}_{t}\right] \le L_{2}\left(\norm{\theta_{t}} + 1\right)$$
also, last line follows from the identity $u \le \frac{1}{2}(1 + u^{2})$.

For $A_{3}$, we obtain
\begin{align*}
	A_{3} & \le \sum_{t = 1}^{k}\gamma_{t}\norm{\theta_{t} - \theta_{t - 1}} \; \norm{P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1}, X_{t}\right)}\\
	& \le \sum_{t = 1}^{k}\stepsize \constLPH{0}\gamma_{t}\norm{g\left(\theta_{t}, X_{t + 1}\right)‌ + \xi_{t + 1}(\theta_{t})}\\
	& \le \sum_{t = 1}^{k}\stepsize\constLPH{0}\gamma_{t}\left(L_{1}\left(\norm{\theta_{t} - \thetastar} + 1\right) + L_{2}\left(\norm{\theta_{t} - \thetastar} + 1\right)\right)\\
	& \le \sum_{t = 1}^{k}\stepsize L \constLPH{0}\gamma_{t}\left(\norm{\theta_{t} - \thetastar} + 1\right)
\end{align*}
where second line follows from \ref{MCLipschitzness} and third line is similarly done to the previous part, using Lipschitzness condition and noise assumption.

For $A_{4}$, we have
\begin{align*}
	A_{4} & \le \sum_{t = 1}^{k}|\gamma_{t} - \gamma_{t - 1}|\; \norm{\theta_{t - 1} - \thetastar} \; \norm{P_{\theta_{t - 1}\hat{g}\left(\theta_{t - 1}, X_{t}\right)}}\\
	& \le \sum_{t = 1}^{k}\constLPH{0}|\gamma_{t} - \gamma_{t - 1}| \; \norm{\theta_{t - 1} - \thetastar}
\end{align*}

Finally, for $A_{5}$, we obtain
\begin{align*}
	A_{5} & \le \constLPH{0}\left(\gamma_{0}\norm{\theta_{0} - \thetastar} + \gamma_{k}\norm{\theta_{k} - \thetastar}\right)
\end{align*}
which follows from Cacuhy-Scwarz inequality and \ref{MCLipschitzness}.

Combining the above terms and taking expectations, gives us:
\begin{align*}
	\Exs\left[\sum_{t = 0}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1} - \bar{g}\left(\theta_{t}\right)\right)\right\rangle\right] \le & \sum_{t = 1}^{k}\frac{\stepsize\constLPH{1}\gamma_{t}}{2}\left(1 + 3\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] \right) + \sum_{t = 1}^{k}\stepsize L \constLPH{0}\gamma_{t}\left(\Exs\left[\norm{\theta_{t} - \thetastar}\right]‌ + 1\right) +\\
	& \sum_{t = 0}^{k - 1}\constLPH{0}|\gamma_{t} - \gamma_{t + 1}| \; \Exs\left[\norm{\theta_{t} - \thetastar}\right] + \constLPH{0}\left(\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right]\right)
\end{align*}
now it should be noticed that as long as the $\alpha$ satisfies $\stepsize \le \frac{\sqrt{4\mu^{2} + 8L^{2}} - \mu}{4L^{2}}$, we have $\gamma_{t} \le \gamma_{t + 1}$. Thus, we can simplify the above upper bound and write it this way:
\begin{align*}
	\Exs\left[\sum_{t = 0}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1} - \bar{g}\left(\theta_{t}\right)\right)\right\rangle\right] \le & \sum_{t = 1}^{k}\frac{\stepsize\constLPH{1}\gamma_{t}}{2}\left(1 + 3\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] \right) +\\
	& \sum_{t = 1}^{k - 1}\constLPH{0}\left(\left(\stepsize L - 1\right)\gamma_{t} + \gamma_{t + 1}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
	& \sum_{t = 1}^{k}\stepsize L \constLPH{0}\gamma_{t} +  \constLPH{0}\left(\gamma_{1}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \left(\stepsize L + 1\right)\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right]\right)
\end{align*}

Hence, using the derived upper bounds from the above terms, we have:
\begin{align*}
	\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] \le & \sum_{t = 1}^{k}\frac{\stepsize\constLPH{1}\gamma_{t}}{2}\left(1 + 3\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] \right) + \sum_{t = 1}^{k - 1}\constLPH{0}\left(\left(\stepsize L - 1\right)\gamma_{t} + \gamma_{t + 1}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
	& \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{1}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \left(\stepsize L + 1\right)\constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] +\\ 
	& \left(\frac{L}{\constLPH{0}} + 1\right)\frac{\gamma_{1}\left(1 - \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{k}\right)}{\left(1 - \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)\right)} + \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{k}2\stepsize^{2}L^{2}\\
\end{align*}
for further notation simplicity we define $c_{1, t} \coloneq \left(\frac{L}{\constLPH{0}} + 1\right)\frac{\gamma_{1}\left(1 - \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{t}\right)}{\left(1 - \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)\right)} + \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{t}2\stepsize^{2}L^{2}$ for $0 \le t \le k$. Now to write down this upper bound in a way in which it only depends on $\norm{\theta_{0} - \thetastar}$related terms and constants, we can write:
\begin{align*}
	\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] \le & \sum_{t = 1}^{k}\left[\frac{3\stepsize\constLPH{1}\gamma_{t}}{2}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \frac{\stepsize\constLPH{1}\gamma_{t}}{2}\right] + \sum_{t = 1}^{k - 1}\constLPH{0}\left(\left(\stepsize L - 1\right)\gamma_{t} + \gamma_{t + 1}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
	& \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{1}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \left(\stepsize L + 1\right)\constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] + c_{1, k}\\
	= & \sum_{t = 1}^{k}\frac{3\stepsize\constLPH{1}\gamma_{t}}{2}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \frac{\stepsize\constLPH{1}}{2}\sum_{t = 1}^{k}\gamma_{t} + \sum_{t = 1}^{k - 1}\constLPH{0}\left(\left(\stepsize L - 1\right)\gamma_{t} + \gamma_{t + 1}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
	& \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{1}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \left(\stepsize L + 1\right)\constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] + c_{1, k}\\
	= & \sum_{t = 1}^{k}\frac{3\stepsize\constLPH{1}\gamma_{t}}{2}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \sum_{t = 1}^{k - 1}\constLPH{0}\left(\left(\stepsize L - 1\right)\gamma_{t} + \gamma_{t + 1}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
	& \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{1}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \left(\stepsize L + 1\right)\constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] + c_{1, k} + \\ 
	& \frac{\constLPH{1}\gamma_{1}\left[1 - \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{k}\right]}{4\left[1 - \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)\right]}\\
\end{align*}
where the last equality follows from the definition of $\gamma_{t}$s. Similarly we define $c_{2, t} \coloneq \frac{\constLPH{1}\gamma_{1}\left[1 - \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{t}\right]}{4\left[1 - \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)\right]}$ for $0 \le t \le k$. So we can write it as
\begin{align*}
	\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] \le & \sum_{t = 1}^{k}\frac{3\stepsize\constLPH{1}\gamma_{t}}{2}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \sum_{t = 1}^{k - 1}\constLPH{0}\left(\left(\stepsize L - 1\right)\gamma_{t} + \gamma_{t + 1}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
	& \left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{1}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \left(\stepsize L + 1\right)\constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] + c_{1, k} + c_{2, k}
\end{align*}
\\
Now for the second term on RHS, we note that
\begin{align*}
	\big( \stepsize L - 1 \big) \gamma_{t} + \gamma_{t + 1} \leq \stepsize L \gamma_{t + 1}, \quad \Exs  \big[ \norm{\theta_{t} - \thetastar} \big] \leq \sqrt{ \Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big]},
\end{align*}
and consequently
\begin{align*}
	&\frac{1}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k} \sum_{t = 1}^{k - 1} \constLPH{0} \big( ( \stepsize L - 1 \big) \gamma_{t} + \gamma_{t + 1}\big)\Exs \big[ \norm{\theta_{t} - \thetastar} \big] \\
	&\leq 2 \constLPH{0} L \stepsize^2  \sum_{t = 1}^{k - 1} \frac{1}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^{t + 1}} \sqrt{\Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big]}\\
	&\leq 2 \constLPH{0} L \stepsize^2  \Big(  \sum_{t = 1}^{k - 1} \frac{1}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^{t + 1}} \Big)^{1/2} \Big( \sum_{t = 1}^{k - 1} \frac{1}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^{t + 1}} \Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big] \Big)^{1/2}\\
	&\leq 2 \constLPH{0} L \stepsize^2 \cdot \sum_{t = 1}^{k - 1} \frac{1}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^{t + 1}} \Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big] + \frac{1}{\stepsize L^{2} + \mu} \cdot \frac{2 \constLPH{0} L \stepsize}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k}.
\end{align*}
We also note that
\begin{align*}
	\frac{\gamma_k}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k} \Exs \big[ \norm{\theta_k - \thetastar} \big] \leq \stepsize \frac{\Exs \big[ \norm{\theta_k - \thetastar}^2 \big]}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k} + \frac{\stepsize}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k}.
\end{align*}
similarly
\begin{align*}
	\frac{\gamma_1}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k} \Exs \big[ \norm{\theta_0 - \thetastar} \big] \leq \stepsize \frac{\Exs \big[ \norm{\theta_0 - \thetastar}^2 \big]}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^1} + \frac{\stepsize}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^1}.
\end{align*}
and we also define for $0 \le t \le k$
\begin{align*}
	c_{3, t} \coloneq \frac{1}{\stepsize L^{2} + \mu}\frac{2\stepsize\constLPH{0}L}{\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{t}} + \frac{\stepsize\left(\stepsize L + 1\right)\constLPH{0}}{\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{t}} + \frac{\stepsize\constLPH{0}}{\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)}
\end{align*}
to wrap up all the remainder terms.

Substituting back and rearranging yields
\begin{align*}
	\frac{\Exs \big[ \norm{\theta_{k + 1} - \thetastar}^2 \big]}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k} & \leq  \frac{\stepsize \left(\left(\stepsize L + 1\right)\constLPH{0} + \frac{3}{2}\constLPH{1}\right)}{\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{k}}\Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right] + \sum_{t = 1}^{k - 1}\frac{\stepsize\left(\frac{3}{2}\constLPH{1} + 2\stepsize\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{-1}L\constLPH{0}\right)}{\left(1 - 2\stepsize \left(\stepsize L^{2} + \mu\right)\right)^{t}}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \\
	& \left(\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right) + \stepsize\constLPH{1}\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{-1}\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] +‌ c_{1, k} + c_{2, k} + c_{3, k}.
\end{align*}
by choosing $\stepsize \le min\left\{\frac{\sqrt{16\mu^{2} + 16L^{2}} - 4\mu}{8L^{2}}, \frac{1}{2}\right\}$, we have $2\stepsize\left(\stepsize L^{2} + \mu\right) \le \frac{1}{2}$ and thus we have
\begin{align*}
	\stepsize\left(\frac{3}{2}\constLPH{1} + 2\stepsize\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{-1}L\constLPH{0}\right) \le 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + \constLPH{1}\right)
\end{align*}
and again in a similar fashion we have
\begin{align*}
	\left(\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right) + \stepsize\constLPH{1}\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{-1}\right) \le 2\stepsize\constLPH{1} + 1
\end{align*}
using above simplifications we can rewrite our upper bound as
\begin{align*}
	\frac{\Exs \big[ \norm{\theta_{k + 1} - \thetastar}^2 \big]}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k} & \leq 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + \constLPH{1}\right)\sum_{t = 1}^{k}\frac{\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize \left(\stepsize L^{2} + \mu\right)\right)^{t}} + \left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] +‌ c_{1, k} + c_{2, k} + c_{3, k}.
\end{align*}

\pagebreak

For solving the above recursion, we first define $S_{t} \coloneq 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + \constLPH{1}\right)\sum_{l = 1}^{t}\frac{\Exs\left[\norm{\theta_{l} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{l}}$ for $0 \le t \le k$. Also we use $C_{t} \coloneq c_{1, t} + c_{2, t} + c_{3, t}$ and $C^{\prime}_{t} = \sum_{l = 1}^{t}C_{l - 1}$ for $0 \le t \le k$, defining constant terms. Now we can write
\begin{align*}
	\frac{\Exs\left[\norm{\theta_{t + 1} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{t}} \le S_{t} + \left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] +‌ C_{t - 1}.‌
\end{align*}
using this expansion, we can write for $S_{k}$
\begin{align*}
	S_{k} & = 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + \constLPH{1}\right)\sum_{t = 1}^{k}\frac{\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize\left(\stepsize L^{2} + \mu\right)\right)^{t}} \\
	& = 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + \constLPH{1}\right)\sum_{t = 1}^{k}\left[S_{t - 1} + \left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + C_{t - 1}\right]\\
	& = 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + \constLPH{1}\right)\sum_{t = 1}^{k}S_{t - 1} + k\left(2\stepsize\constLPH{1} + 1\right)\left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + C^{\prime}_{k}
\end{align*}
to solve $S_{k}$, we define $C^{\prime\prime}_{t} \coloneq \sum_{l = 1}^{t} l\left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + C^{\prime}_{l}$ for $1 \le t \le k$. Now we can write $S_{k}$ as
\begin{align*}
	S_{k} = C^{\prime\prime}_{k - 1} + \sum_{t = 1}^{k - 1}\left(\frac{\left(t - 1\right)t}{2} + 1\right)\left(4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + \constLPH{1}\right)\right)^{t}C^{\prime\prime}_{k - t}
\end{align*}
solving for $C^{\prime\prime}_{t}$, we have
\begin{align*}
	C^{\prime\prime}_{t} = \frac{t\left(t + 1\right)}{2}\left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \sum_{l = 1}^{t}C^{\prime}_{l} \le 2t^{2}\left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \mathcal{O}\left(t\right)
\end{align*}
where the inequality follows from the fact that $C_{t} = \mathcal{O}(1)$ for each $0 \le t \le k$. Plugging in the above in $S_{k}$ gives us
\begin{align*}
	S_{k} & \le \left(2(k - 1)^{2}\left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \mathcal{O}\left(k - 1\right)\right) +\\
	& \sum_{t = 1}^{k - 1}\left(\frac{(t - 1)t}{2} + 1\right)\left(4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + \constLPH{1}\right)\right)^{t}\left[2t^{2}\left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \mathcal{O}\left(t\right)\right]\\
	& \le \left(2(k - 1)^{2}\left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \mathcal{O}\left(k - 1\right)\right) +\\
	&  \sum_{t = 1}^{k - 1}4t^{2}\left(4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + \constLPH{1}\right)\right)^{t}\left[\left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \mathcal{O}\left(t\right)\right]\\
	& \le \mathcal{O}\left(k^{2}\right)\left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right]\left(1 + \sum_{t = 1}^{k - 1}\left(4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + \constLPH{1}\right)\right)^{t}\right)
\end{align*} 
 defining $Q_{k} \coloneq \left(4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + \constLPH{1}\right)\right)\frac{\left(4\stepsize\left(\left(\stepsize L + 1\right)\right)\constLPH{0} + \constLPH{1}\right)^{k - 1} - 1}{\left(4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + \constLPH{1}\right) - 1\right)}$ and plugging in the above upper bound to our error bound, we know constant $c_{4, k}$ exists that we can write
 \begin{align*}
 	\frac{\Exs \big[ \norm{\theta_{k + 1} - \thetastar}^2 \big]}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k} \le c_{4, k} \cdot Q_{k} \cdot k^{2} \left(2\stepsize\constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + C_{k}
 \end{align*}
 
\subsection{General Case}
In this case, we assume that the moment bound in [??] has been proven for $k \le n - 1$, we now proceed to show that the desired moment convergence holds for $n$ with $2 \le n \le p$.

We start with the following decomposition of $\norm{\theta_{k + 1} - \thetastar}^{2n}$
\begin{align*}
	\norm{\theta_{k + 1} - \thetastar}^{2n} & = \left(\norm{\theta_{k} - \thetastar}^{2} + 2\stepsize \left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)\right\rangle + \stepsize^{2}\norm{g\left(\theta_{x}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)}^{2} \right)^{n}\\
	& = \sum_{\substack{i, j, l \\ i + j + l = n}} \binom{n}{i, j, l}\norm{\theta_{k} - \thetastar}^{2i}\left(2\stepsize \left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)\right\rangle \right)^{j}\left(\stepsize \norm{g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)}\right)^{2l}
\end{align*}
We note the following cases.
\begin{enumerate}
	\item $i = n$, $j = l = 0$. In this case, the summand is simply $\norm{\theta_{k} - \thetastar}^{2i}$.
	\item When $i = n - 1, j = 1$ and $l = 0$. In this case, the summand is of order $\stepsize$, i.e., $$\stepsize 2n\left \langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right) \right\rangle^{j} \norm{\theta_{k} - \thetastar}^{2(n - 1)}.$$ We can further decompose it as
	\begin{align*}
		& 2n\stepsize \left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right) \right\rangle\norm{\theta_{k} - \thetastar}^{2(n - 1)} \\
		& = \underbrace{2n\stepsize\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k+ 1}\right) - \bar{g}\left(\theta_{k}\right) + \xi_{k + 1}\left(\theta_{k}\right) \right\rangle \norm{\theta_{k} - \thetastar}^{2(n - 1)}}_{\constT{1}} + \underbrace{2n\stepsize \left\langle \theta_{k} - \thetastar, \bar{g}\left(\theta_{k}\right) \right\rangle \norm{\theta_{k} - \thetastar}^{2(n - 1)}}_{\constT{2}}.
	\end{align*}
	Note that, when $\left(X_{k}\right)$ is i.i.d or from a martingale noise, we have
	$$\Exs\left[\constT{1} | \theta_{k}\right] = 0$$
	However, when $\left(X_{k}\right)$ is Markovian, the above inequality does not hold and $\constT{1}$ requires careful analysis.\\
	Nonetheless, under the strong monotonicity assumption, we have
	$$\constT{2} \le -2n\stepsize\mu\norm{\theta_{k} - \thetastar}^{2n}.$$
	\item For the remaining terms, we see that they are of higher orders of $\stepsize$. Therefore, when $\stepsize$ is selected sufficiently small, these terms do not raise concern. 
\end{enumerate}

Therefore, to prove the desired moment bound, we spend the remaining section analyzing $\constT{1}$. Immediately, we note that
\begin{align*}
	\Exs\left[\constT{1}\right] &= \Exs\left[2n\stepsize \left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right) + \Exs\left[\xi_{k + 1}\left(\theta_{k}\right) | \theta_{k}\right] \right\rangle\norm{\theta_{k} - \thetastar}^{2(n - 1)}\right]\\
	& = 2n\stepsize\Exs\left[\underbrace{\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right)\right\rangle \norm{\theta_{k} - \thetastar}^{2(n - 1)}}_{\constTprime{1}}\right].
\end{align*}
Subsequently, we focus on analyzing $\constTprime{1}$; but before that, we write the general recursion of the error bound. First, we define $\constTprime{1, t} \coloneq \left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right)\norm{\theta_{t} - \thetastar}^{2(n - 1)}\right\rangle$ to make $\constTprime{1}$ dependent on the iteration index. Now, following the above decomposition and taking the expectations, we have:
\begin{align*}
	\Exs\norm{\theta_{k + 1} - \thetastar}^{2n} \le \Exs\norm{\theta_{k} - \thetastar}^{2n} + \Exs\left[\constTprime{1, k}\right] - 2n\stepsize\mu\Exs\norm{\theta_{k} - \thetastar}^{2n} + o\left(\stepsize\right) = \left(1 - 2n\stepsize\mu\right)\Exs\norm{\theta_{k} - \thetastar}^{2n} + \Exs\left[\constTprime{1, k}\right] + o\left(\stepsize\right)
\end{align*}
similarly to the previous case we define $\gamma_{t} \coloneq 2n\stepsize\left(1 - 2n\stepsize\mu\right)^{k - t}$ for $0 \le t \le k$. Solving the above recursion will give us
\begin{align*}
	\Exs\norm{\theta_{k + 1} - \thetastar}^{2n} \le \sum_{t = 0}^{k}\gamma_{t}\Exs\left[\constTprime{1, t}\right] + \gamma_{0}\Exs\norm{\theta_{0} - \thetastar}^{2n} + o\left(\stepsize\right)
\end{align*}

We have to upper bound the first term in the RHS above. For this purpose, we use a similar decomposition to our base case analysis:
\begin{align*}
	\sum_{t = 0}^{k}\gamma_{t}\Exs\left[\constTprime{1, t}\right] = \sum_{t = 0}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right)\right\rangle\norm{\theta_{t} - \thetastar}^{2(n - 1)} = \Exs\left[ A_{1} + A_{2} + A_{3} + A_{4} + A_{5}\right]
\end{align*} 
with
\begin{align*}
	A_{1} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, \hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) \right\rangle\norm{\theta_{t} - \thetastar}^{2(n - 1)},\\
	A_{2} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) - P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1}, X_{t} \right) \right\rangle\norm{\theta_{t} - \thetastar}^{2(n - 1)},\\
	A_{3} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \theta_{t - 1}, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1}, X_{t}\right) \right\rangle,\norm{\theta_{t} - \thetastar}^{2(n - 1)}\\
	A_{4} \coloneq & \sum_{t = 1}^{k}\left(\gamma_{t} - \gamma_{t - 1}\right)\left\langle \theta_{t - 1} - \thetastar, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1} - \thetastar, X_{t}\right) \right\rangle\norm{\theta_{t} - \thetastar}^{2(n - 1)},\\
	A_{5} \coloneq & \gamma_{0}\left\langle \theta_{0} - \thetastar, \hat{g}\left(\theta_{0}, X_{0}\right) \right\rangle\norm{\theta_{0} - \thetastar}^{2(n - 1)} + \gamma_{k}\left\langle \theta_{k} - \thetastar, P_{\theta_{k}}\hat{g}\left(\theta_{k}, X_{k + 1}\right)\right\rangle\norm{\theta_{k} - \thetastar}^{2(n - 1)}
\end{align*}

\bibliographystyle{ieee}
\bibliography{ref.bib}

\end{document}