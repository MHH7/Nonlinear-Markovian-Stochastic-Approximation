\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{outline} \usepackage{pmgraph} \usepackage[normalem]{ulem}
\usepackage{graphicx} \usepackage{verbatim}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{tgbonum}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{lastpage}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
% \usepackage{minted} % need `-shell-escape' argument for local compile



\newcommand{\norm}[1]{\|#1 \|}
\newcommand{\Exs}{\mathbb{E}}
\newcommand{\thetastar}{\theta^*}
\newcommand{\constLPH}[1]{L_{PH}^{(#1)}}
\newcommand{\mwlcomment}[1]{{\color{orange} #1}}
\newcommand{\stepsize}{\alpha}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

\begin{document}

\fontfamily{cmr}\selectfont
\title{ \normalsize \textsc{}
	\\ [2.0cm]
	\HRule{0.5pt} \\
	\LARGE \textbf{\uppercase{Nonlinear Markovian Stochastic Approximation}
		\HRule{2pt} \\ [0.5cm]
		\normalsize \today \vspace*{5\baselineskip}}
}

\date{}

\author{
	Mohammadhadi Hadavi \\ 
	Prof. Hoi-To Wai - Chinese University of Hong Kong \\
	Prof. Wenlong Mou - University of Toronto}

\maketitle
\newpage
%\tableofcontents
%\newpage

\section{Preliminaries}

\textbf{Notations} The Euclidean norm is denoted by $||.||$. The lowercase letter $c$ and its derivatives $c^{\prime}, c_{0},$ etc. denote universal numerical constants, whose value may change from line to line. As we are primarily interested in dependence of $\alpha$ and $k$, we adopt the following big-$O$ notation: $||f|| = \mathcal{O}\left(h\left(\alpha, k\right)\right)$ if it holds that $||f|| \le s \cdot ||h\left(\alpha, k\right)||$ for some constant $s > 0$.

We use of the following iteration scheme:
\begin{equation}
	\theta_{t + 1} = \theta_{t} + \alpha\left(g\left(\theta_{t}, X_{t + 1}\right) + \xi_{t + 1}\left(\theta_{t}\right)\right)
\end{equation}

\subsection{Assumptions}
\textbf{Assumption 1} \textit{
	For each $X \in \mathcal{X}$, the function $g\left(\theta, X\right)$ is three times continuously differentiable in $\theta$ with uniformly bounded first to third derivatives, i.e., $\sup_{\theta \in \mathbb{R}^{d}}||g^{(i)}\left(\theta, X\right)|| < \infty$ for $i = 1, 2, 3, X \in \mathcal{X}$. Moreover, there exists a constant $L_{1} > 0$ such that (1) $||g^{(i)}\left(\theta, X\right) - g^{(i)}\left(\theta^{\prime}, X\right)|| \le L_{1}$, for all $\theta, \theta^{\prime} \in \mathbb{R}^{d}, i = 0, 1, 2$ and $X \in \mathcal{X}$, and (2) $||g\left(0, X\right)|| \le L_{1}$ for all $X \in \mathcal{X}$.
}

Assumption 1 implies that $g\left(\theta, X\right)$ is $L_{1}$-Lipschitz w.r.t $\theta$ uniformly in $X$. The above assumption immediately implies that the growth of $||g||$ and $||\bar{g}||$ will be at most linear in $\theta$, i.e., $||g\left(\theta, X\right)|| \le L_{1}\left(||\theta - \theta^{*}|| + 1\right)$ and $||\bar{g}\left(\theta\right)|| \le L_{1}\left(||\theta - \theta^{*}|| + 1\right)$. 
\\
\\
\textbf{Assumption 2} \textit{
	There exists $\mu > 0$ such that $\left\langle \theta - \theta^{\prime}, \bar{g}(\theta) - \bar{g}(\theta^{\prime}) \right\rangle \le -\mu||\theta - \theta^{\prime}||^{2}, \forall \theta, \theta^{\prime} \in \mathbb{R}^{d}$. Consequently, the target equation $\bar{g}(\theta) = 0$ has a unique solution $\theta^{*}$.
}
\\

Denote by $\mathcal{F}_{k}$ the filtration generated by $\{X_{t + 1}, \theta_{t}, \xi_{t + 1}\}_{t = 0}^{k - 1} \cup \{X_{k + 1}, \theta_{k}\}$.
\\
\textbf{Assumption 3} \textit{
	Let $p \in \mathbb{Z}_{+}$ be given. The noise sequence $\left(\xi_{k}\right)_{k \ge 1}$ is a collection of i.i.d random fields satisfying the following conditions with $L_{2, p} > 0$:
	$$\mathbb{E}\left[\xi_{k + 1}(\theta) | \mathcal{F}_{k}\right] = 0 \quad \text{and} \quad \mathbb{E}^{1 / (2p)}\left[||\xi_{1}(\theta)^{2p}\right] \le L_{2, p}\left(||\theta - \theta^{*}|| + 1\right), \quad \forall \theta \in \mathbb{R}^{d}.$$
	Define $C(\theta) = \mathbb{E}\left[\xi_{1}(\theta)^{\otimes 2}\right]$ and assume that $C(\theta)$ is at least twice differentiable. There also exists $M_{\epsilon}, k_{\epsilon} \ge 0$ such that for $\theta \in \mathbb{R}^{d}$, we have $\max_{i = 1, 2}||C^{(i)}(\theta)|| \le M_{\epsilon}\{1 + ||\theta - \theta^{*}||^{k_{\epsilon}}\}$.
}
In the sequel, we set $L \coloneq L_{1} + L_{2}$, and without loss of generality, we assume $L \ge 1$.
\\
\\
\textbf{Assumption 4} \textit{
There exists a Borel measurable function $\hat{g}: \mathbb{R}^{d} \times \mathcal{X} \to \mathbb{R}^{d}$ where for each $\theta \in \mathbb{R}^{d}, X \in \mathcal{X}$,
\begin{equation}
	\hat{g}\left(\theta, X\right) - P_{\theta}\hat{g}\left(\theta, X\right) = g\left(\theta, X\right) - \bar{g}\left(\theta\right).
\end{equation}
}
\\
\textbf{Assumption 5} \textit{
There exists $L_{PH}^{(0)} <‌ \infty$ and $L_{PH}^{(1)} < \infty$ such that, for all $\theta \in \mathbb{R}^{d}$ and $X \in \mathcal{X}$, one has $||\hat{g}\left(\theta, X\right)|| \le L_{PH}^{(0)}$, $||P_{\theta}\hat{g}\left(\theta, X\right)|| \le L_{PH}^{(0)}$. Moreover, for $\left(\theta, \theta^{\prime}\right) \in \mathcal{H}^{2}$,
\begin{equation}
	\sup_{X \in \mathcal{X}}||P_{\theta}\hat{g}\left(\theta, X\right) - P_{\theta^{\prime}}\hat{g}\left(\theta^{\prime}, X\right)|| \le L_{PH}^{(1)}||\theta - \theta^{\prime}||.
\end{equation}
}
\\
\textbf{Assumption 6} \textit{
For any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$, we have $\sup_{X \in \mathcal{X}}||P_{\theta}\left(X, .\right) - P_{\theta^{\prime}}\left(X, .\right)||_{TV} \le L_{P}||\theta - \theta^{\prime}||$.
}
\\
\textbf{Assumption 7} \textit{
For any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$, we have $\sup_{X \in \mathcal{X}}||g\left(\theta, X\right) - g\left(\theta^{\prime}, X\right)|| \le L_{H}||\theta - \theta^{\prime}||$.
}
\\
\textbf{Assumption 8} \textit{
There exists $\rho < 1$, $K_{P} < \infty$ such that
\begin{equation}
	\sup_{\theta \in \mathbb{R}^{d}, X \in \mathcal{X}} ||P_{\theta}^{n}\left(X, .\right) - \pi_{\theta}(.)||_{TV} \le \rho^{n}K_{P},
\end{equation}
}
\\
\textbf{Lemma 1} \textit{
Assume that assumptions 6-8 hold. Then, for any $\theta \in \mathbb{R}^{d}$ and $X \in \mathcal{X}$,
\begin{equation}
	||\hat{g}\left(\theta, X\right)|| \le \frac{\sigma K_{P}}{1 - \rho},
\end{equation}
\begin{equation}
	||P_{\theta}\hat{g}\left(\theta, X\right)|| \le \frac{\sigma \rho K_{P}}{1 - \rho}.
\end{equation}
Moreover, for any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$ and $X \in \mathcal{X}$,
\begin{equation}
	||P_{\theta}\hat{g}\left(\theta, X\right) - P_{\theta^{\prime}}\hat{g}\left(\theta^{\prime}, X\right)|| \le L_{PH}^{(1)}||\theta - \theta^{\prime}||,
\end{equation}
where
\begin{equation}
	L_{PH}^{(1)} = \frac{K_{P}^{2}\sigma L_{P}}{(1 - \rho)^{2}}\left(2 + K_{P}\right) + \frac{K_{P}}{1 - \rho}L_{H}.
\end{equation}
}
Proof of this lemma can be found in \cite{karimi2019non}, Lemma 7.
\section{Error Bound}
\mwlcomment{General comments about writing math:
\begin{itemize}
	\item Use macros as much as possible, including symbols appearing throughout the proof such as $\constLPH{1}, \constLPH{0}, \thetastar, \stepsize$ (this makes life easier when we want to change notation), and basic symbols $\| \cdot\|, \Exs$, etc. Do not use $||$, use $\|$ instead.
	\item Try to avoid extremely long equations. This proof is not that complicated. Simplify things in the middle as much as you can as we don't care about universal constant factors. If the calculation has to be that complicated, break it down into several equations.
	\item Use lemmas to encapsulate your intermediate results. Structure the proofs as a tree (e.g. the proof of main theorem involves Lemmas 1,2,3. You state the lemmas in the middle of the proof, and put the proof of lemmas at the end of the theorems.)
	\item Use align environment instead of equation so that the numbering is for each line. Number it only when it's going to be referenced. Otherwise use align*. 
\end{itemize}


}
\subsection{Base Case}

For the base case analysis, we can write:
\begin{equation}
	\begin{split}
		& \mathbb{E}\left[||\theta_{k +‌ 1} - \theta^{*}||^{2}\right] - \mathbb{E}\left[||\theta_{k} - \theta^{*}||^{2}\right] = \\
		& 2\alpha \mathbb{E}\left[\left\langle \theta_{k} - \theta^{*}, g\left(\theta_{k}, X_{k + 1}\right) \right\rangle\right] + \alpha^{2}\mathbb{E}\left[||g\left(\theta_{k}, X_{k + 1}\right)||^{2}\right] +‌ \alpha^{2}\mathbb{E}\left[||\xi_{k +‌ 1}\left(\theta_{k}\right)||^{2}\right] =\\
		& 2\alpha\mathbb{E}\left[\left\langle \theta_{k} - \theta^{*}, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right)\right\rangle\right] + 2\alpha\mathbb{E}\left[\left\langle \theta_{k} - \theta^{*}, \bar{g}\left(\theta_{k}\right) \right\rangle\right] + \alpha^{2}\mathbb{E}\left[||g\left(\theta_{k}, X_{k + 1}\right)\right] + \alpha^{2}\mathbb{E}\left[||\xi_{k + 1}\left(\theta_{k}\right)||^{2}\right].
	\end{split}
\end{equation}
It is easy to see that under Strong Monotonicity assumption, we have
\begin{equation}
	\left\langle \theta_{k} - \theta^{*}, \bar{g}\left(\theta_{k}\right)\right\rangle = \left\langle \theta_{k} - \theta^{*}, \bar{g}\left(\theta_{k}\right) +‌ \bar{g}\left(\theta^{*}\right)\right\rangle \le -\mu||\theta_{k} - \theta^{*}||^{2}.
\end{equation}
\\
Additionally, under Assumption 1 and 3, we have the following upper bound
\begin{equation}
	\begin{split}
		&‌ \alpha^{2}\left(\mathbb{E}\left[||g\left(\theta_{k}, X_{k + 1}\right)||^{2}\right] + \mathbb{E}\left[||\xi_{k + 1}\left(\theta_{k}\right)||^{2}\right]\right)\\
		& \le \alpha^{2}\left(L_{1}^{2}\mathbb{E}\left[\left(||\theta_{k} - \theta^{*}|| + 1\right)^{2}\right] + L_{2}^{2}\mathbb{E}\left[\left(||\theta_{k} - \theta^{*}|| + 1\right)^{2}\right]\right)\\
		& \le 2\alpha^{2}L^{2}\left(\mathbb{E}\left[||\theta_{k} - \theta^{*}||^{2}\right] + 1\right).
	\end{split}
\end{equation}
\\
Therefore, we have
\begin{equation}
	\mathbb{E}\left[||\theta_{k +‌ 1} - \theta^{*}||^{2}\right] \le \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)\mathbb{E}\left[||\theta_{k} - \theta^{*}||^{2}\right] +‌ 2\alpha^{2}L^{2} + 2\alpha\mathbb{E}\left[\left\langle \theta_{k} - \theta^{*}, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right)\right\rangle\right]
\end{equation}
Solving this recursion gives us the following inequality:
\begin{equation}
	\begin{split}
		\mathbb{E}\left[||\theta_{k + 1} - \theta^{*}||^{2}\right] & \le \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{k + 1}\mathbb{E}\left[||\theta_{0} - \theta^{*}||^{2}\right] \\
		& + \sum_{t = 0}^{k}\left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{t}2\alpha^{2}L^{2} \\
		& + \sum_{t = 0}^{k}2\alpha\left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{k - t}\mathbb{E}\left[\left\langle \theta_{t} - \theta^{*}, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right) \right\rangle\right].
	\end{split}
\end{equation}

For notational simplicity we define $\gamma_{t} \coloneq 2\alpha\left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{k - t}$ for $0 \le t \le k$.

The second term above is just a geometric series which is equal to $2\alpha^{2}L^{2}\left(\alpha L^{2} + \mu\right)^{k}$.

Now, we can upper bound the third summand using below decomposition:
\begin{equation}
	\mathbb{E}\left[ \sum_{t = 0}^{k} \gamma_{t}\left\langle \theta_{t} - \theta^{*}, g(\theta_{t}, X_{t + 1}) - \bar{g}(\theta_{t}) \right\rangle \right] = \mathbb{E}\left[ A_{1} + A_{2} + A_{3} + A_{4} + A_{5}\right]
\end{equation}
with
\begin{equation*}
	\begin{split}
		A_{1} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \theta^{*}, \hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) \right\rangle,\\
		A_{2} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \theta^{*}, P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) - P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1}, X_{t} \right) \right\rangle,\\
		A_{3} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \theta_{t - 1}, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1}, X_{t}\right) \right\rangle,\\
		A_{4} \coloneq & \sum_{t = 1}^{k}\left(\gamma_{t} - \gamma_{t - 1}\right)\left\langle \theta_{t - 1} - \theta^{*}, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1} - \theta^{*}, X_{t}\right) \right\rangle,\\
		A_{5} \coloneq & \gamma_{0}\left\langle \theta_{0} - \theta^{*}, \hat{g}\left(\theta_{0}, X_{0}\right) \right\rangle + \gamma_{k}\left\langle \theta_{t} - \theta^{*}, P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t + 1}\right)\right\rangle
	\end{split}
\end{equation*}

For $A_{1}$, we note that $\hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right)$ is a martingale difference sequence [cf. ?] and therefore we have $\mathbb{E}[A_{1}] = 0$ by taking the total expectation.

For $A_{2}$, applying Cauchy-Schwarz inequality and \ref{MCLipschitzness}, we have
\begin{equation}
	\begin{split}
		A_{2} & \le \sum_{t = 1}^{k}L_{PH}^{(1)}\gamma_{t}||\theta_{t} - \theta^{*}||\;||\theta_{t} - \theta_{t - 1}||\\
		& = \sum_{t = 1}^{k}\alpha L_{PH}^{(1)}\gamma_{t}||\theta_{t} - \theta^{*}||\;||g(\theta_{t}, X_{t + 1}) + \xi_{t +‌1}(\theta_{t})||\\
		& \le \sum_{t = 1}^{k}L_{PH}^{(1)} \gamma_{t}||\theta_{t} - \theta^{*}||\left(\alpha L_{1}\left(||\theta_{t} - \theta^{*}|| + 1\right) + \alpha L_{2}\left(||\theta_{t} - \theta^{*}|| + 1\right)\right)\\
		& \le \sum_{t = 1}^{k}\frac{L_{PH}^{(1)}\gamma_{t}}{2}(1 + \alpha L)\left(1 + 3||\theta_{t} - \theta^{*}||^{2} \right)
	\end{split}
\end{equation}
\mwlcomment{The second last step seems wrong and/or loose. You have an $\stepsize$ factor there and you should keep it. Seems to me that it should be something like
\begin{align*}
	 \stepsize \gamma_{t} \norm{\theta_{t} - \thetastar} \Big( L_{1} (\norm{\theta_{t} - \thetastar} + 1 ) + L_{2} (\norm{\theta_{t} - \thetastar} + 1) \Big)
\end{align*}
If we use your bound for $A_2$, the recursion argument cannot go through. But if we keep the $\stepsize$ factor there, it will work.
}




where the third line follows from the Lipschitzness condition and the assumption of
$$\mathbb{E}^{1 / 2}\left[||\xi_{t + 1}\left(\theta_{t}\right)||^{2} | \mathcal{F}_{t}\right] \le L_{2}\left(||\theta_{t}|| + 1\right)$$
also, last line follows from the identity $u \le \frac{1}{2}(1 + u^{2})$.

For $A_{3}$, we obtain
\begin{equation}
	\begin{split}
		A_{3} & \le \sum_{t = 1}^{k}\gamma_{t}||\theta_{t} - \theta_{t - 1}|| \; ||P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1}, X_{t}\right)||\\
		& \le \sum_{t = 1}^{k}\alpha L_{PH}^{(0)}\gamma_{t}||g\left(\theta_{t}, X_{t + 1}\right)‌ + \xi_{t + 1}(\theta_{t})||\\
		& \le \sum_{t = 1}^{k}L_{PH}^{(0)}\gamma_{t}\left(\alpha L_{1}\left(||\theta_{t} - \theta^{*}|| + 1\right) + \alpha L_{2}\left(||\theta_{t} - \theta^{*}|| + 1\right)\right)\\
		& \le \sum_{t = 1}^{k}\alpha L L_{PH}^{(0)}\gamma_{t}\left(||\theta_{t} - \theta^{*}|| + 1\right)
	\end{split}
\end{equation}
where second line follows from \ref{MCLipschitzness} and third line is similarly done to the previous part, using Lipschitzness condition and noise assumption.

For $A_{4}$, we have
\begin{equation}
	\begin{split}
		A_{4} & \le \sum_{t = 1}^{k}|\gamma_{t} - \gamma_{t - 1}|\; ||\theta_{t - 1} - \theta^{*}|| \; ||P_{\theta_{t - 1}\hat{g}\left(\theta_{t - 1}, X_{t}\right)}||\\
		& \le \sum_{t = 1}^{k}L_{PH}^{(0)}|\gamma_{t} - \gamma_{t - 1}| \; ||\theta_{t - 1} - \theta^{*}||
	\end{split}
\end{equation}

Finally, for $A_{5}$, we obtain
\begin{equation}
	\begin{split}
		A_{5} & \le L_{PH}^{(0)}\left(\gamma_{0}||\theta_{0} - \theta^{*}|| + \gamma_{k}||\theta_{k} - \theta^{*}||\right)
	\end{split}
\end{equation}
which follows from Cacuhy-Scwarz inequality and \ref{MCLipschitzness}.

Combining the above terms and taking expectations, gives us:
\begin{equation}
	\begin{split}
		\mathbb{E}\left[\sum_{t = 0}^{k}\gamma_{t}\left\langle \theta_{t} - \theta^{*}, g\left(\theta_{t}, X_{t + 1} - \bar{g}\left(\theta_{t}\right)\right)\right\rangle\right] \le & \sum_{t = 1}^{k}\frac{L_{PH}^{(1)}\gamma_{t}}{2}(1 + \alpha L)\left(1 + 3\mathbb{E}\left[||\theta_{t} - \theta^{*}||^{2}\right] \right) + \sum_{t = 1}^{k}\alpha L L_{PH}^{(0)}\gamma_{t}\left(\mathbb{E}\left[||\theta_{t} - \theta^{*}||\right]‌ + 1\right) +\\
		& \sum_{t = 0}^{k - 1}L_{PH}^{(0)}|\gamma_{t} - \gamma_{t + 1}| \; \mathbb{E}\left[||\theta_{t} - \theta^{*}||\right] + L_{PH}^{(0)}\left(\gamma_{0}\mathbb{E}\left[||\theta_{0} - \theta^{*}||\right] + \gamma_{k}\mathbb{E}\left[||\theta_{k} - \theta^{*}||\right]\right)
	\end{split}
\end{equation}
now it should be noticed that as long as we have $\alpha \le \frac{\sqrt{2\mu^{2} + 4L^{2}} - \mu}{2L^{2}}$, we have $\gamma_{t + 1} \le \gamma_{t}$ \mwlcomment{Seems wrong direction}. Thus, we can simplify the above upper bound and write it this way:
\begin{equation}
	\begin{split}
		\mathbb{E}\left[\sum_{t = 0}^{k}\gamma_{t}\left\langle \theta_{t} - \theta^{*}, g\left(\theta_{t}, X_{t + 1} - \bar{g}\left(\theta_{t}\right)\right)\right\rangle\right] \le & \sum_{t = 1}^{k}\frac{L_{PH}^{(1)}\gamma_{t}}{2}(1 + \alpha L)\left(1 + 3\mathbb{E}\left[||\theta_{t} - \theta^{*}||^{2}\right] \right) +\\ & \sum_{t = 1}^{k - 1}L_{PH}^{(0)}\left(\left(\alpha L + 1\right)\gamma_{t} - \gamma_{t + 1}\right)\mathbb{E}\left[||\theta_{t} - \theta^{*}||\right] +\\
		& \sum_{t = 1}^{k}\alpha L L_{PH}^{(0)}\gamma_{t} +  L_{PH}^{(0)}\left(\left(2\gamma_{0} - \gamma_{1}\right)\mathbb{E}\left[||\theta_{0} - \theta^{*}||\right] + \left(\alpha L + 1\right)\gamma_{k}\mathbb{E}\left[||\theta_{k} - \theta^{*}||\right]\right) 
	\end{split}
\end{equation}

Hence, using the derived upper bounds from the above terms, we have:
\begin{equation}
	\begin{split}
		\mathbb{E}\left[||\theta_{k + 1} - \theta^{*}||^{2}\right] \le & \sum_{t = 1}^{k}\frac{L_{PH}^{(1)}\gamma_{t}}{2}(1 + \alpha L)\left(1 + 3\mathbb{E}\left[||\theta_{t} - \theta^{*}||^{2}\right] \right) + \sum_{t = 1}^{k - 1}L_{PH}^{(0)}\left(\left(\alpha L + 1\right)\gamma_{t} - \gamma_{t + 1}\right)\mathbb{E}\left[||\theta_{t} - \theta^{*}||\right] +\\
		& \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)\gamma_{0}\mathbb{E}\left[||\theta_{0} - \theta^{*}||^{2}\right] + L_{PH}^{(0)}\left(2\gamma_{0} - \gamma_{1}\right)\mathbb{E}\left[||\theta_{0} - \theta^{*}||\right] + \left(\alpha L + 1\right)L_{PH}^{(0)}\gamma_{k}\mathbb{E}\left[||\theta_{k} - \theta^{*}||\right] +\\ 
		& \left(\frac{L_{PH}^{(0)}}{L} + 1\right)\frac{\alpha L^{2}\left(1 - \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{k}\right)}{\left(\alpha L^{2} + \mu\right)} + \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{k}2\alpha^{2}L^{2}\\
	\end{split}
\end{equation}
to write down this upper bound in a way in which it only depends on $||\theta_{0} - \theta^{*}||$related terms and constants, we can write:
\begin{equation}
	\begin{split}
		\mathbb{E}\left[||\theta_{k + 1} - \theta^{*}||^{2}\right] \le & \sum_{t = 1}^{k}\left[\frac{3L_{PH}^{(1)}\gamma_{t}}{2}(1 + \alpha L)\mathbb{E}\left[||\theta_{t} - \theta^{*}||^{2}\right] + \frac{L_{PH}^{(1)}\gamma_{t}}{2}(1 + \alpha L)\right] + \sum_{t = 1}^{k - 1}L_{PH}^{(0)}\left(\left(\alpha L + 1\right)\gamma_{t} - \gamma_{t + 1}\right)\mathbb{E}\left[||\theta_{t} - \theta^{*}||\right] +\\
		& \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)\gamma_{0}\mathbb{E}\left[||\theta_{0} - \theta^{*}||^{2}\right] + L_{PH}^{(0)}\left(2\gamma_{0} - \gamma_{1}\right)\mathbb{E}\left[||\theta_{0} - \theta^{*}||\right] + \left(\alpha L + 1\right)L_{PH}^{(0)}\gamma_{k}\mathbb{E}\left[||\theta_{k} - \theta^{*}||\right] +\\
		& \left(\frac{L_{PH}^{(0)}}{L} + 1\right)\frac{\alpha L^{2}\left(1 - \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{k}\right)}{\left(\alpha L^{2} + \mu\right)} + \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{k}2\alpha^{2}L^{2}\\
		= & \sum_{t = 1}^{k}\frac{3L_{PH}^{(1)}\gamma_{t}}{2}(1 + \alpha L)\mathbb{E}\left[||\theta_{t} - \theta^{*}||^{2}\right] + \frac{L_{PH}^{(1)}}{2}(1 + \alpha L)\sum_{t = 1}^{k}\gamma_{t} + \sum_{t = 1}^{k - 1}L_{PH}^{(0)}\left(\left(\alpha L + 1\right)\gamma_{t} - \gamma_{t + 1}\right)\mathbb{E}\left[||\theta_{t} - \theta^{*}||\right] +\\
		& \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)\gamma_{0}\mathbb{E}\left[||\theta_{0} - \theta^{*}||^{2}\right] + L_{PH}^{(0)}\left(2\gamma_{0} - \gamma_{1}\right)\mathbb{E}\left[||\theta_{0} - \theta^{*}||\right] + \left(\alpha L + 1\right)L_{PH}^{(0)}\gamma_{k}\mathbb{E}\left[||\theta_{k} - \theta^{*}||\right] +\\ 
		& \left(\frac{L_{PH}^{(0)}}{L} + 1\right)\frac{\alpha L^{2}\left(1 - \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{k}\right)}{\left(\alpha L^{2} + \mu\right)} + \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{k}2\alpha^{2}L^{2}\\
		= & \sum_{t = 1}^{k}\frac{3L_{PH}^{(1)}\gamma_{t}}{2}(1 + \alpha L)\mathbb{E}\left[||\theta_{t} - \theta^{*}||^{2}\right] + \sum_{t = 1}^{k - 1}L_{PH}^{(0)}\left(\left(\alpha L + 1\right)\gamma_{t} - \gamma_{t + 1}\right)\mathbb{E}\left[||\theta_{t} - \theta^{*}||\right] +\\
		& \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)\gamma_{0}\mathbb{E}\left[||\theta_{0} - \theta^{*}||^{2}\right] + L_{PH}^{(0)}\left(2\gamma_{0} - \gamma_{1}\right)\mathbb{E}\left[||\theta_{0} - \theta^{*}||\right] + \left(\alpha L + 1\right)L_{PH}^{(0)}\gamma_{k}\mathbb{E}\left[||\theta_{k} - \theta^{*}||\right] +\\ 
		& \left(\frac{L_{PH}^{(0)}}{L} + 1\right)\frac{\alpha L^{2}\left(1 - \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{k}\right)}{\left(\alpha L^{2} + \mu\right)} + \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{k}2\alpha^{2}L^{2} +\\
		& \frac{L_{PH}^{(1)}\left(1 + \alpha L\right)\left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)\left[1 - \left(1 - 2\alpha\left(\alpha L^{2} + \mu\right)\right)^{k}\right]}{4\alpha\left(\alpha L^{2} + \mu\right)}\\
	\end{split}
\end{equation}
\mwlcomment{Where does the last term in the last line come from? This term seems very large and will ruin your bound...}

where the last equality follows from the definition of $\gamma_{t}$'s.




\mwlcomment{BEGIN -- Wenlong edits}

For the second term on RHS, we note that
\begin{align*}
	\big( \stepsize L + 1 \big) \gamma_{t} - \gamma_{t + 1} \leq \stepsize L \gamma_{t}, \quad \Exs  \big[ \norm{\theta_{t} - \thetastar} \big] \leq \sqrt{ \Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big]},
\end{align*}
and consequently
\begin{align*}
	&\frac{1}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k} \sum_{t = 1}^{k - 1} \constLPH{0} \big( (\stepsize L + 1) \gamma_{t} - \gamma_{t + 1}\big)\Exs \big[ \norm{\theta_{t} - \thetastar} \big] \\
	&\leq 2 \constLPH{0} L \stepsize^2  \sum_{t = 1}^{k - 1} \frac{1}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^t} \sqrt{\Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big]}\\
	&\leq 2 \constLPH{0} L \stepsize^2  \Big(  \sum_{t = 1}^{k - 1} \frac{1}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^t} \Big)^{1/2} \Big( \sum_{t = 1}^{k - 1} \frac{1}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^t} \Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big] \Big)^{1/2}\\
	&\leq 2 \constLPH{0} L \stepsize^2 \cdot \sum_{t = 1}^{k - 1} \frac{1}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^t} \Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big] + \frac{1}{\alpha L^{2} + \mu} \cdot \frac{2 \constLPH{0} L \stepsize}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k}.
\end{align*}
We also note that
\begin{align*}
	\frac{\gamma_k}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k} \Exs \big[ \norm{\theta_k - \thetastar} \big] \leq \stepsize \frac{\Exs \big[ \norm{\theta_k - \thetastar}^2 \big]}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k} + \frac{\stepsize}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k}.
\end{align*}


Substituting back and rearranging yields
\begin{multline*}
	\frac{\Exs \big[ \norm{\theta_{k + 1} - \thetastar}^2 \big]}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k} \leq  \Big\{ 6 \stepsize \constLPH{1} (1 + \stepsize L) +  6 \stepsize^2 \constLPH{0} L  \Big\} \sum_{t = 1}^k \frac{\Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big]}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^{t - 1}} + 2 \stepsize \constLPH{0} \frac{\Exs \big[ \norm{\theta_k - \thetastar}^2 \big]}{(1 - 2 \stepsize (\stepsize L^2 + \mu))^{k - 1}} \\
	 +\frac{2 \constLPH{0} L (\alpha L^{2} + \mu)^{-1} + \constLPH{0} (1 + \stepsize L) }{(1 - 2 \stepsize (\stepsize L^2 + \mu))^k} \stepsize +  \left(\frac{L_{PH}^{(0)}}{L} + 1\right)\frac{L^{2}}{\left(\alpha L^{2} + \mu\right)} \stepsize + 2 \stepsize^2 L^2 + 3 \stepsize \Exs \big[ \norm{\theta_0 - \thetastar}^2 \big] + 3\stepsize \big(\constLPH{0} \big)^2.
\end{multline*}
\mwlcomment{The factor in the first term of the first parenthesis should also be some constant times $\stepsize^2$. See my comments on the bound for $A_2$. Using the corrected bound you'll be able to solve this recursion. Please fix this and make necessary downstream edits.}







\mwlcomment{END -- Wenlong edits}


\subsection{General Case}
In this case, we assume that the moment bound in [??] has been proven for $k \le n - 1$, we now proceed to show that the desired moment convergence holds for $n$ with $2 \le n \le p$.

We start with the following decomposition of $||\theta_{k + 1} - \theta^{*}||^{2n}$
\begin{equation*}
	\begin{split}
		||\theta_{k + 1} - \theta^{*}||^{2n} & = \left(||\theta_{k} - \theta^{*}||^{2} + 2\alpha \left\langle \theta_{k} - \theta^{*}, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)\right\rangle + \alpha^{2}||g\left(\theta_{x}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)||^{2} \right)^{n}\\
		& = \sum_{\substack{i, j, l \\ i + j + l = n}} \binom{n}{i, j, l}||\theta_{k} - \theta^{*}||^{2i}\left(2\alpha \left\langle \theta_{k} - \theta^{*}, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)\right\rangle \right)^{j}\left(\alpha ||g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)||\right)^{2l}
	\end{split}
\end{equation*}
We note the following cases.
\begin{enumerate}
	\item $i = n$, $j = l = 0$. In this case, the summand is simply $||\theta_{k} - \theta^{*}||^{2i}$.
	\item When $i = n - 1, j = 1$ and $l = 0$. In this case, the summand is of order $\alpha$, i.e., $\alpha 2n\left \langle \theta_{k} - \theta^{*}, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right) \right\rangle^{j} ||\theta_{k} - \theta^{*}||^{2(n - 1)}$. We can further decompose it as
	\begin{equation*}
		\begin{split}
			& 2n\alpha \left\langle \theta_{k} - \theta^{*}, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right) \right\rangle||\theta_{k} - \theta^{*}||^{2(n - 1)} \\
			& = \underbrace{2n\alpha\left\langle \theta_{k} - \theta^{*}, g\left(\theta_{k}, X_{k+ 1}\right) - \bar{g}\left(\theta_{k}\right) + \xi_{k + 1}\left(\theta_{k}\right) \right\rangle ||\theta_{k} - \theta^{*}||^{2(n - 1)}}_{T_{1}} + \underbrace{2n\alpha \left\langle \theta_{k} - \theta^{*}, \bar{g}\left(\theta_{k}\right) \right\rangle ||\theta_{k} - \theta^{*}||^{2(n - 1)}}_{T_{2}}.
		\end{split}
	\end{equation*}
	Note that, when $\left(X_{k}\right)$ is i.i.d or from a martingale noise, we have
	$$\mathbb{E}\left[T_{1} | \theta_{k}\right] = 0$$
	However, when $\left(X_{k}\right)$ is Markovian, the above inequality does not hold and $T_{1}$ requires careful analysis.\\
	Nonetheless, under the strong monotonicity assumption, we have
	$$T_{2} \le -2n\alpha\mu||\theta_{k} - \theta^{*}||^{2n}.$$
	\item For the remaining terms, we see that they are of higher orders of $\alpha$. Therefore, when $\alpha$ is selected sufficiently small, these terms do not raise concern. 
\end{enumerate}

Therefore, to prove the desired moment bound, we spend the remaining section analyzing $T_{1}$. Immediately, we note that
\begin{equation*}
	\begin{split}
		\mathbb{E}\left[T_{1}\right] &= \mathbb{E}\left[2n\alpha \left\langle \theta_{k} - \theta^{*}, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right) + \mathbb{E}\left[\xi_{k + 1}\left(\theta_{k}\right) | \theta_{k}\right] \right\rangle||\theta_{k} - \theta^{*}||^{2(n - 1)}\right]\\
		& = \mathbb{E}\left[\underbrace{2n\alpha \left\langle \theta_{k} - \theta^{*}, g\left(\theta_{x}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right)||\theta_{k} - \theta^{*}||^{2(n - 1)}\right\rangle}_{T_{1}^{\prime}}\right].
	\end{split}
\end{equation*}
Subsequently, we focus on analyzing $T_{1}^{\prime}$.

\bibliographystyle{ieee}
\bibliography{ref.bib}

\end{document}