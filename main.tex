\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{outline} \usepackage{pmgraph} \usepackage[normalem]{ulem}
\usepackage{graphicx} \usepackage{verbatim}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{tgbonum}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{lastpage}
\usepackage{listings}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
% \usepackage{minted} % need `-shell-escape' argument for local compile



\newcommand{\norm}[1]{\|#1 \|}
\newcommand{\Exs}{\mathbb{E}}
\newcommand{\thetastar}{\theta^*}
\newcommand{\thetainf}{\theta_\infty}
\newcommand{\thetainfpone}{\theta_{\infty + 1}}
\newcommand{\xstar}{X^*}
\newcommand{\xinf}{X_{\infty}}
\newcommand{\xinfPone}{X_{\infty + 1}}
\newcommand{\xinfPtwo}{X_{\infty + 2}}
\newcommand{\constLPH}[1]{L_{PH}^{(#1)}}
\newcommand{\constT}[1]{T_{#1}}
\newcommand{\constTprime}[1]{T_{#1}^{\prime}}
\newcommand{\cB}{C_{B}}
\newcommand{\mwlcomment}[1]{{\color{orange} #1}}
\newcommand*{\htwai}[1]{\textbf{\textcolor{red}{To: #1}}}
\newcommand{\hadi}[1]{{\color{blue} #1}}
\newcommand{\stepsize}{\alpha}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

\begin{document}
	
	\fontfamily{cmr}\selectfont
	\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{0.5pt} \\
		\LARGE \textbf{\uppercase{Nonlinear Markovian Stochastic Approximation}
			\HRule{2pt} \\ [0.5cm]
			\normalsize \today \vspace*{5\baselineskip}}
	}
	
	\date{}
	
	\author{
		Mohammadhadi Hadavi \\ 
		Prof. Hoi-To Wai - Chinese University of Hong Kong \\
		Prof. Wenlong Mou - University of Toronto}
	
	\maketitle
	\newpage
	%\tableofcontents
	%\newpage
	
	\section{Preliminaries}
	
	\textbf{Notations} The Euclidean norm is denoted by $\norm{.}$. The lowercase letter $c$ and its derivatives $c^{\prime}, c_{0},$ etc. denote universal numerical constants, whose value may change from line to line. As we are primarily interested in dependence of $\stepsize$ and $k$, we adopt the following big-$O$ notation: $\norm{f} = \mathcal{O}\left(h\left(\stepsize, k\right)\right)$ if it holds that $\norm{f} \le s \cdot \norm{h\left(\stepsize, k\right)}$ for some constant $s > 0$.
	
	We use of the following iteration scheme:
	\begin{align*}
		\theta_{t + 1} = \theta_{t} + \stepsize\left(g\left(\theta_{t}, X_{t + 1}\right) + \xi_{t + 1}\left(\theta_{t}\right)\right)
	\end{align*}
	where $g: \mathbb{R}^{d} \times \mathcal{X} \to \mathbb{R}^{d}$ is a deterministic function, $\{\xi_{k}\}_{k \ge 1}$ are $i.i.d$ zero-mean random fields, and $\stepsize > 0$ is a constant stepsize. We shall omit the superscript $^{(\stepsize)}$ in $\theta_{k}$ when the dependence on $\stepsize$ is clear from the context. 
	
	In our settings, $\{X_{n}, n \in \mathbb{N}\}$ is a \textit{state-dependent} (or controlled) Markov chain, \textit{i.e.,} for any bounded measurable function $g: \mathbb{R}^{d} \times \mathcal{X} \to \mathbb{R}^{d}$,
	\begin{align*}
		\Exs\left[g\left(\theta_{n}, X_{n + 1}\right) | \mathcal{F}_{n}\right] = P_{\theta_{n}}g\left(\theta_{n}, X_{n}\right) = \int g\left(\theta_{n}, x\right)P_{\theta}\left(X_{n}, dx\right),
	\end{align*} 
	where $P_{\theta}: \mathcal{X} \times \mathcal{X} \to \mathbb{R}_{+}$ is a Markov kernel such that, for each $\theta \in \Theta$, $P_{\theta}$ has a unique stationary distribution $\pi_{\theta}$. Also, $\mathcal{F}_{n}$ denotes the filtration generated by the random variables $\left(\theta_{0}, \{\xi_{m + 1}\}_{m \leq n - 1}, \{X_{m}\}_{m \leq n}\right)$.
	
	\subsection{Assumptions}
	\textbf{Assumption 1} \textit{
		For each $X \in \mathcal{X}$, the function $g\left(\theta, X\right)$ is three times continuously differentiable in $\theta$ with uniformly bounded first to third derivatives, i.e., $\sup_{\theta \in \mathbb{R}^{d}}\norm{g^{(i)}\left(\theta, X\right)} < \infty$ for $i = 1, 2, 3, X \in \mathcal{X}$. Moreover, there exists a constant $L_{1} > 0$ such that (1) $\norm{g^{(i)}\left(\theta, X\right) - g^{(i)}\left(\theta^{\prime}, X\right)} \le L_{1}$, for all $\theta, \theta^{\prime} \in \mathbb{R}^{d}, i = 0, 1, 2$ and $X \in \mathcal{X}$, and (2) $\norm{g^{(i)}\left(0, X\right)} \le L_{1}$ for all $X \in \mathcal{X}$ and $i = 0, 1, 2$.
	}
	
	Assumption 1 implies that $g\left(\theta, X\right)$ is $L_{1}$-Lipschitz w.r.t $\theta$ uniformly in $X$. The above assumption immediately implies that the growth of $\norm{g}$ and $\norm{\bar{g}}$ will be at most linear in $\theta$, i.e., $\norm{g\left(\theta, X\right)} \le L_{1}\left(\norm{\theta - \thetastar} + 1\right)$ and $\norm{\bar{g}\left(\theta\right)} \le L_{1}\left(\norm{\theta - \thetastar} + 1\right)$. Similar implications also hold for the first and second derivatives in the same manner.
	\\
	\\
	\textbf{Assumption 2} \textit{
		There exists $\mu > 0$ such that $\left\langle \theta - \theta^{\prime}, \bar{g}(\theta) - \bar{g}(\theta^{\prime}) \right\rangle \le -\mu\norm{\theta - \theta^{\prime}}^{2}, \forall \theta, \theta^{\prime} \in \mathbb{R}^{d}$.
	}
	\\
	Consequently, the target equation $\bar{g}(\theta) = 0$ has a unique solution $\thetastar$.
	\\
	
	Denote by $\mathcal{F}_{k}$ the filtration generated by $\{X_{t + 1}, \theta_{t}, \xi_{t + 1}\}_{t = 0}^{k - 1} \cup \{X_{k + 1}, \theta_{k}\}$.
	\\
	\textbf{Assumption 3} \textit{
		Let $p \in \mathbb{Z}_{+}$ be given. The noise sequence $\left(\xi_{k}\right)_{k \ge 1}$ is a collection of i.i.d random fields satisfying the following conditions with $L_{2, p} > 0$:
		$$\Exs\left[\xi_{k + 1}(\theta) | \mathcal{F}_{k}\right] = 0 \quad \text{and} \quad \Exs^{1 / (2p)}\left[\norm{\xi_{1}(\theta)}^{2p}\right] \le L_{2, p}\left(\norm{\theta - \thetastar} + 1\right), \quad \forall \theta \in \mathbb{R}^{d}.$$
		Define $C(\theta) = \Exs\left[\xi_{1}(\theta)^{\otimes 2}\right]$ and assume that $C(\theta)$ is at least twice differentiable. There also exists $M_{\epsilon}, k_{\epsilon} \ge 0$ such that for $\theta \in \mathbb{R}^{d}$, we have $\max_{i = 1, 2}\norm{C^{(i)}(\theta)} \le M_{\epsilon}\{1 + \norm{\theta - \thetastar}^{k_{\epsilon}}\}$.
	}
	In the sequel, we set $L \coloneq L_{1} + L_{2}$, and without loss of generality, we assume $L \ge 2\mu$ for some technical reasons.
	\\
	\\
	\textbf{Assumption 4} \textit{
		There exists a Borel measurable function $\hat{g}: \mathbb{R}^{d} \times \mathcal{X} \to \mathbb{R}^{d}$ where for each $\theta \in \mathbb{R}^{d}, X \in \mathcal{X}$,
		\begin{align*}
			\hat{g}\left(\theta, X\right) - P_{\theta}\hat{g}\left(\theta, X\right) = g\left(\theta, X\right) - \bar{g}\left(\theta\right).
		\end{align*}
	}
	\\
	\textbf{Assumption 5} \textit{
		There exists $\constLPH{0} <‌ \infty$ and $\constLPH{1} < \infty$ such that, for all $\theta \in \mathbb{R}^{d}$ and $X \in \mathcal{X}$, one has $\norm{\hat{g}\left(\theta, X\right)} \le \constLPH{0}$, $\norm{P_{\theta}\hat{g}\left(\theta, X\right)} \le \constLPH{0}$. Moreover, for $\left(\theta, \theta^{\prime}\right) \in \mathcal{H}^{2}$,
		\begin{align*}
			\sup_{X \in \mathcal{X}}\norm{P_{\theta}\hat{g}\left(\theta, X\right) - P_{\theta^{\prime}}\hat{g}\left(\theta^{\prime}, X\right)} \le \constLPH{1}\norm{\theta - \theta^{\prime}}.
		\end{align*}
	}
	\\
	\textbf{Assumption 6} \textit{
		For any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$, we have $\sup_{X \in \mathcal{X}}\norm{P_{\theta}\left(X, .\right) - P_{\theta^{\prime}}\left(X, .\right)}_{TV} \le L_{P}\norm{\theta - \theta^{\prime}}$.
	}
	\\
	\textbf{Assumption 7} \textit{
		For any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$, we have $\sup_{X \in \mathcal{X}}\norm{g\left(\theta, X\right) - g\left(\theta^{\prime}, X\right)} \le L_{H}\norm{\theta - \theta^{\prime}}$.
	}
	\\
	\textbf{Assumption 8} \textit{
		There exists $\rho < 1$, $K_{P} < \infty$ such that
		\begin{align*}
			\sup_{\theta \in \mathbb{R}^{d}, X \in \mathcal{X}} \norm{P_{\theta}^{n}\left(X, .\right) - \pi_{\theta}(.)}_{TV} \le \rho^{n}K_{P},
		\end{align*}
	}
	\\
	\textbf{Assumption 9} \textit{
		For any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$, we have $\sup_{X \in \mathcal{X}}\norm{\pi_{\theta}(X) - \pi_{\theta^{\prime}}(X)} \le L_{S}\norm{\theta - \theta^{\prime}}$.
	}
	\\
	\textbf{Lemma 1} \textit{
		Assume that assumptions 6-8 hold. Then, for any $\theta \in \mathbb{R}^{d}$ and $X \in \mathcal{X}$,
		\begin{align*}
			\norm{\hat{g}\left(\theta, X\right)} \le \frac{\sigma K_{P}}{1 - \rho},
		\end{align*}
		\begin{align*}
			\norm{P_{\theta}\hat{g}\left(\theta, X\right)} \le \frac{\sigma \rho K_{P}}{1 - \rho}.
		\end{align*}
		Moreover, for any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$ and $X \in \mathcal{X}$,
		\begin{align*}
			\norm{P_{\theta}\hat{g}\left(\theta, X\right) - P_{\theta^{\prime}}\hat{g}\left(\theta^{\prime}, X\right)} \le \norm{\theta - \theta^{\prime}},
		\end{align*}
		where
		\begin{align*}
			\constLPH{1} = \frac{K_{P}^{2}\sigma L_{P}}{(1 - \rho)^{2}}\left(2 + K_{P}\right) + \frac{K_{P}}{1 - \rho}L_{H}.
		\end{align*}
	}
	Proof of this lemma can be found in \cite{karimi2019non}, Lemma 7.
	\section{Error Bound}
	
	\subsection{Base Case}
	
	We first prove the following lemma because we are going to use that calculation in many different parts of the proof:
	
	\textbf{Lemma2.} \textit{Using Assumptions 1, 3, 5, and for sufficiently small $\stepsize$ and $t \ge 1$, we have
	\begin{align*}
		\Exs\left[\norm{\theta_{t} - \theta_{t - 1}}\right] \leq \stepsize L\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}\right]\ + 1\right).
	\end{align*}
	}
	
	\textbf{Proof.} We have
	\begin{align*}
		\Exs\left[\norm{\theta_{t} - \theta_{t - 1}}\right] &\leq \stepsize\Exs\left[\norm{g\left(\theta_{t - 1}, X_{t}\right) + \xi_{t}\left(\theta_{t - 1}\right)}\right]\\
		& \leq \stepsize L_{1}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}\right] + 1\right) + \stepsize L_{2}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}\right] + 1\right)\\
		& \leq \stepsize L\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}\right] + 1\right)
	\end{align*}
	where the first line follows from ??, second line from the Lipschitzness condition and the assumption of
	\begin{align*}
		\Exs^{1 / 2 }\left[\norm{\xi_{t}\left(\theta_{t - 1}\right)}^{2} | \mathcal{F}_{t - 1}\right] \leq L_{2}\left(\Exs\left[\norm{\theta_{t - 1}}\right] + 1\right),
	\end{align*}
	 and the third line from ??.\\
	
	For the base case analysis, we can write:
	\begin{align*}
		& \Exs\left[\norm{\theta_{k +‌ 1} - \thetastar}^{2}\right] - \Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right] = \\
		& 2\stepsize \Exs\left[\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) \right\rangle\right] + \stepsize^{2}\Exs\left[\norm{g\left(\theta_{k}, X_{k + 1}\right)}^{2}\right] +‌ \stepsize^{2}\Exs\left[\norm{\xi_{k +‌ 1}\left(\theta_{k}\right)}^{2}\right] =\\
		& 2\stepsize\Exs\left[\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right)\right\rangle\right] + 2\stepsize\Exs\left[\left\langle \theta_{k} - \thetastar, \bar{g}\left(\theta_{k}\right) \right\rangle\right] + \stepsize^{2}\Exs\left[\norm{g\left(\theta_{k}, X_{k + 1}\right)}\right] + \stepsize^{2}\Exs\left[\norm{\xi_{k + 1}\left(\theta_{k}\right)}^{2}\right].
	\end{align*}
	It is easy to see that under Strong Monotonicity assumption, we have
	\begin{align*}
		\left\langle \theta_{k} - \thetastar, \bar{g}\left(\theta_{k}\right)\right\rangle = \left\langle \theta_{k} - \thetastar, \bar{g}\left(\theta_{k}\right) - \bar{g}\left(\thetastar\right)\right\rangle \le -\mu\norm{\theta_{k} - \thetastar}^{2}.
	\end{align*}
	Additionally, under Assumption 1 and 3, we have the following upper bound
	\begin{align*}
		&‌ \stepsize^{2}\left(\Exs\left[\norm{g\left(\theta_{k}, X_{k + 1}\right)}^{2}\right] + \Exs\left[\norm{\xi_{k + 1}\left(\theta_{k}\right)}^{2}\right]\right)\\
		& \le \stepsize^{2}\left(L_{1}^{2}\Exs\left[\left(\norm{\theta_{k} - \thetastar} + 1\right)^{2}\right] + L_{2}^{2}\Exs\left[\left(\norm{\theta_{k} - \thetastar} + 1\right)^{2}\right]\right)\\
		& \le 2\stepsize^{2}L^{2}\left(\Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right] + 1\right).
	\end{align*}
	\\
	Therefore, we have
	\begin{align*}
		\Exs\left[\norm{\theta_{k +‌ 1} - \thetastar}^{2}\right] \le \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right] +‌ 2\stepsize^{2}L^{2} + 2\stepsize\Exs\left[\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right)\right\rangle\right]
	\end{align*}
	Solving this recursion gives us the following inequality:
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] & \le \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k + 1}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] \\
		& + \sum_{t = 0}^{k}\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}2\stepsize^{2}L^{2} \\
		& + \sum_{t = 0}^{k}2\stepsize\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k - t}\Exs\left[\left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right) \right\rangle\right].
	\end{align*}
	
	For notational simplicity we define $\gamma_{t} \coloneq 2\stepsize\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k - t}$ for $0 \le t \le k$.
	
	The second term above is just a geometric series. According to Lemma 12 of \cite{kaledin2020finite}, this equals to $\frac{\stepsize L^{2}\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k + 1}\right]}{-\stepsize L^{2} + \mu}$ which is of $\mathcal{O}\left(\stepsize^{2}\right)$.
	
	Now, we can upper bound the third summand using below decomposition:
	\begin{align*}
		\Exs\left[ \sum_{t = 0}^{k} \gamma_{t}\left\langle \theta_{t} - \thetastar, g(\theta_{t}, X_{t + 1}) - \bar{g}(\theta_{t}) \right\rangle \right] = A_{1} + A_{2} + A_{3} + A_{4} + A_{5}
	\end{align*}
	with
	\begin{align*}
		A_{1} \coloneq & \Exs\left[\sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, \hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) \right\rangle\right],\\
		A_{2} \coloneq & \Exs\left[\sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) - P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1}, X_{t} \right) \right\rangle\right],\\
		A_{3} \coloneq & \Exs\left[\sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \theta_{t - 1}, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1}, X_{t}\right) \right\rangle\right],\\
		A_{4} \coloneq & \Exs\left[\sum_{t = 1}^{k}\left(\gamma_{t} - \gamma_{t - 1}\right)\left\langle \theta_{t - 1} - \thetastar, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1} - \thetastar, X_{t}\right) \right\rangle\right],\\
		A_{5} \coloneq & \Exs\left[\gamma_{0}\left\langle \theta_{0} - \thetastar, \hat{g}\left(\theta_{0}, X_{1}\right) \right\rangle\right] - \Exs\left[\gamma_{k}\left\langle \theta_{k} - \thetastar, P_{\theta_{k}}\hat{g}\left(\theta_{k}, X_{k + 1}\right)\right\rangle\right]
	\end{align*}
	
	For $A_{1}$, we note that $\hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right)$ is a martingale difference sequence [cf. ?] and therefore we have $A_{1} = 0$ by taking the total expectation.
	
	For $A_{2}$, applying Cauchy-Schwarz inequality and \ref{MCLipschitzness}, we have
	\begin{align*}
		A_{2} & \leq \sum_{t = 1}^{k}\constLPH{1}\gamma_{t}\Exs\left[\norm{\theta_{t} - \thetastar}\;\norm{\theta_{t} - \theta_{t - 1}}\right]\\
		& = \sum_{t = 1}^{k}\stepsize \constLPH{1}\gamma_{t}\Exs\left[\norm{\theta_{t} - \thetastar}\;\norm{g(\theta_{t - 1}, X_{t}) + \xi_{t}(\theta_{t - 1})}\right]\\
		& \leq \sum_{t = 1}^{k}\stepsize\constLPH{1}\gamma_{t}\Exs\left[\left(\norm{\theta_{t} - \theta_{t - 1}} + \norm{\theta_{t - 1} - \thetastar}\right)\left(\norm{g\left(\theta_{t - 1}, X_{t}\right)} + \norm{\xi_{t}\left(\theta_{t - 1}\right)}\right)\right]\\
		& \leq \sum_{t = 1}^{k} \stepsize\constLPH{1}\gamma_{t}\bigl(L_{1}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2}\right] + \Exs\left[\norm{\theta_{t} - \theta_{t - 1}} \; \norm{\theta_{t - 1} - \thetastar}\right] + \Exs\left[\norm{\theta_{t - 1} - \thetastar}\right] + \Exs\left[\norm{\theta_{t} - \theta_{t - 1}}\right]\right)\\
		& + \Exs\left[\norm{\theta_{t} - \theta_{t - 1}} \; \norm{\xi_{t}\left(\theta_{t - 1}\right)}\right] + \Exs\left[\norm{\theta_{t - 1} - \thetastar} \; \norm{\xi_{t}\left(\theta_{t - 1}\right)}\right]\bigr)
	\end{align*}
	where the second line follows from ?? and the third line follows from the triangle inequality. Now we upper the compound terms in the last line's parentheses:
	\begin{align*}
		\Exs\left[\norm{\theta_{t} - \theta_{t - 1}} \; \norm{\theta_{t - 1} - \thetastar}\right] & = \Exs\left[\Exs\left[\norm{\theta_{t} - \theta_{t - 1}} \; \norm{\theta_{t - 1} - \thetastar} | \mathcal{F}_{t - 1}\right]\right]\\
		& \leq \Exs\left[\stepsize L\left(\norm{\theta_{t - 1} - \thetastar} + 1\right)\norm{\theta_{t - 1} - \thetastar}\right]\\
		& \leq \frac{\stepsize L\left(3\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2}\right] + 1\right)}{2} 
	\end{align*}
	where in the second line we used Lemma 2 and in the last line we used $u \leq \frac{u^{2} + 1}{2}$.
	\begin{align*}
		\Exs\left[\norm{\theta_{t} - \theta_{t - 1}} \; \norm{\xi_{t}\left(\theta_{t - 1}\right)}\right] & \leq \Exs\left[\stepsize \left(\norm{g\left(\theta_{t - 1}, X_{t}\right)} + \norm{\xi_{t}\left(\theta_{t - 1}\right)}\right)\norm{\xi_{t}\left(\theta_{t - 1}\right)}\right]\\
		& \leq \Exs\left[\stepsize\norm{\xi_{t}\left(\theta_{t - 1}\right)}^{2} + \stepsize L_{1}\left(\norm{\theta_{t - 1} - \thetastar} + 1\right)\norm{\xi_{t}\left(\theta_{t - 1}\right)}\right]\\
		& \leq L\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}\right] + 1\right)
	\end{align*}
	where the first inequality follows from ??, second line from Lemma 2 and in the last line we used boundedness property of the noise and sufficiently small $\stepsize$.
	\begin{align*}
		\Exs\left[\norm{\theta_{t - 1} - \thetastar} \; \norm{\xi_{t}\left(\theta_{t - 1}\right)}\right] &= \Exs\left[\Exs\left[\norm{\theta_{t - 1} - \thetastar} \; \norm{\xi_{t}\left(\theta_{t - 1}\right)} | \mathcal{F}_{t - 1}\right]\right]\\
		& \leq \Exs\left[L_{2}\norm{\theta_{t - 1} - \thetastar}\left(\norm{\theta_{t -1} - \thetastar} + 1\right)\right]\\
		& \leq \frac{L_{2}\left(3\norm{\theta_{t - 1} - \thetastar}^{2} + 1\right)}{2}
	\end{align*}
	where the second line follows from ?? and in the last line we used $u \leq \frac{u^{2} + 1}{2}$.
		
	Summing up all these bounds, we can write for $A_{2}$:
	\begin{align*}
		A_{2} & \leq \sum_{t = 1}^{k}\stepsize \constLPH{1}\gamma_{t}\left(2L\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}\right] + 1\right) + 2L\left(3\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2}\right] + 1\right) + \Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2}\right]\right)\\
		& \leq \sum_{t = 1}^{k}\stepsize L \constLPH{1}\gamma_{t}\left(8\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2}\right] + 5\right)
	\end{align*}
	which in the last line we again used $u \leq \frac{u^{2} + 1}{2}$ property.	
	
		
	For $A_{3}$, we obtain
	\begin{align*}
		A_{3} & \le \sum_{t = 1}^{k}\gamma_{t}\Exs\left[\norm{\theta_{t} - \theta_{t - 1}} \; \norm{P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1}, X_{t}\right)}\right]\\
		& \le \sum_{t = 1}^{k}\stepsize \constLPH{0}\gamma_{t}\Exs\left[\norm{g\left(\theta_{t - 1}, X_{t}\right)‌ + \xi_{t}(\theta_{t - 1})}\right]\\
		& \le \sum_{t = 1}^{k}\stepsize\constLPH{0}\gamma_{t}\left(L_{1}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}\right] + 1\right) + L_{2}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}\right] + 1\right)\right)\\
		& \leq \sum_{t = 1}^{k}\stepsize L \constLPH{0}\gamma_{t}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}\right] + 1\right)
	\end{align*}
	where second line follows from \ref{MCLipschitzness} and third line follows from ?? .
	
	For $A_{4}$, we have
	\begin{align*}
		A_{4} & \le \sum_{t = 1}^{k}|\gamma_{t} - \gamma_{t - 1}|\; \Exs\left[\norm{\theta_{t - 1} - \thetastar} \; \norm{P_{\theta_{t - 1}}\hat{g}\left(\theta_{t- 1}, X_{t}\right)}\right]\\
		& \le \sum_{t = 1}^{k}\constLPH{0}|\gamma_{t} - \gamma_{t - 1}| \; \Exs\left[\norm{\theta_{t - 1} - \thetastar}\right]
	\end{align*}
	
	Finally, for $A_{5}$, we obtain
	\begin{align*}
		A_{5} & \le \constLPH{0}\left(\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}\right] - \gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right]\right)
	\end{align*}
	which follows from Cacuhy-Scwarz inequality and \ref{MCLipschitzness}.
	
	Combining the above terms gives us:
	\begin{align*}
		\Exs\left[\sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right)\right\rangle\right] \le & \sum_{t = 0}^{k - 1}\stepsize L \constLPH{1}\gamma_{t + 1}\left(5 + 8\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] \right) + \sum_{t = 0}^{k - 1}\stepsize L \constLPH{0}\gamma_{t + 1}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}\right]‌ + 1\right) +\\
		& \sum_{t = 0}^{k - 1}\constLPH{0}|\gamma_{t} - \gamma_{t + 1}| \; \Exs\left[\norm{\theta_{t} - \thetastar}\right] + \constLPH{0}\left(\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}\right] - \gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right]\right)
	\end{align*}
	
	now it should be noticed that as long as the $\alpha$ satisfies $\stepsize \le \frac{\mu}{L^{2}}$, we have $\gamma_{t} \le \gamma_{t + 1}$. Thus, we can simplify the above upper bound and write it this way:
	\begin{align*}
		\Exs\left[\sum_{t = 0}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right)\right\rangle\right] \le & \sum_{t = 0}^{k - 1}\stepsize L \constLPH{1}\gamma_{t + 1}\left(5 + 8\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] \right) +\\
		& \sum_{t = 0}^{k - 1}\constLPH{0}\left(\left(\stepsize L + 1\right)\gamma_{t + 1} - \gamma_{t}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
		& \sum_{t = 0}^{k - 1}\stepsize L \constLPH{0}\gamma_{t + 1} +  \constLPH{0}\left(\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}\right] - \gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right]\right)
	\end{align*}
	
	Hence, using the derived upper bounds from the above terms, we have:
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] \le & \sum_{t = 0}^{k - 1}\stepsize L \constLPH{1}\gamma_{t + 1}\left(5 + 8\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] \right) + \sum_{t = 0}^{k - 1}\constLPH{0}\left(\left(\stepsize L + 1\right)\gamma_{t + 1} - \gamma_{t}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
		& \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}\right] - \constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] +\\ 
		& 2\stepsize^{2}L\constLPH{0}\left[\frac{1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k}}{1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)}\right] + \frac{\stepsize L^{2}\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k + 1}\right]}{-\stepsize L^{2} + \mu}\\
	\end{align*}
	
	for further notation simplicity we define $c_{1, t} \coloneq 2\stepsize^{2}L\constLPH{0}\left[\frac{1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}}{1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)}\right] + \frac{\stepsize L^{2}\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t + 1}\right]}{-\stepsize L^{2} + \mu}$ for $0 \le t \le k$. Now to write down this upper bound in a way in which it only depends on $\norm{\theta_{0} - \thetastar}$ related terms and constants, we can write:
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] \le & \sum_{t = 0}^{k - 1}\left[8\stepsize L \constLPH{1}\gamma_{t + 1}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + 5\stepsize L \constLPH{1}\gamma_{t + 1}\right] + \sum_{t = 0}^{k - 1}\constLPH{0}\left(\left(\stepsize L + 1\right)\gamma_{t + 1} - \gamma_{t}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
		& \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}\right] - \constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] + c_{1, k}\\
		= & \sum_{t = 0}^{k - 1}8\stepsize L \constLPH{1}\gamma_{t + 1}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + 5\stepsize L \constLPH{1}\sum_{t = 0}^{k - 1}\gamma_{t + 1} + \sum_{t = 0}^{k - 1}\constLPH{0}\left(\left(\stepsize L + 1\right)\gamma_{t + 1} - \gamma_{t}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
		& \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}\right] - \constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] + c_{1, k}\\
		= & \sum_{t = 0}^{k - 1}8\stepsize L \constLPH{1}\gamma_{t + 1}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \sum_{t = 0}^{k - 1}\constLPH{0}\left(\left(\stepsize L + 1\right)\gamma_{t + 1} - \gamma_{t}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
		& \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}\right] - \constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] + c_{1, k} + \\ 
		& \frac{10\stepsize^{2} L \constLPH{1}\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k}\right]}{\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\right]}\\
	\end{align*}
	
	where the last equality follows from the definition of $\gamma_{t}$s. Similarly we define $c_{2, t} \coloneq \frac{10\stepsize^{2} L \constLPH{1}\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}\right]}{\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\right]}$ for $0 \le t \le k$. So we can write it as
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] \le & \sum_{t = 0}^{k - 1}8\stepsize L \constLPH{1}\gamma_{t + 1}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \sum_{t = 0}^{k - 1}\constLPH{0}\left(\left(\stepsize L + 1\right)\gamma_{t + 1} - \gamma_{t}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
		& \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}\right] - \constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] + c_{1, k} + c_{2, k}
	\end{align*}
	\\
	Now for the second term on RHS, we note that since $L \ge 2\mu$,
	\begin{align*}
		\big( \stepsize L + 1 \big) \gamma_{t + 1} - \gamma_{t} \leq 2\stepsize L \gamma_{t + 1}, \quad \Exs  \big[ \norm{\theta_{t} - \thetastar} \big] \leq \sqrt{ \Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big]},
	\end{align*}
	and consequently
	\begin{align*}
		&\frac{1}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^k} \sum_{t = 0}^{k - 1} \constLPH{0} \big( ( \stepsize L + 1 \big) \gamma_{t + 1} - \gamma_{t}\big)\Exs \big[ \norm{\theta_{t} - \thetastar} \big] \\
		&\leq 4 \constLPH{0} L \stepsize^2  \sum_{t = 0}^{k - 1} \frac{1}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^{t + 1}} \sqrt{\Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big]}\\
		&\leq 4 \constLPH{0} L \stepsize^2 \sum_{t = 0}^{k - 1} \frac{1}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^{t + 1}}\left(\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + 1\right)\\
		&\leq 4 \constLPH{0} L \stepsize^2 \cdot \sum_{t = 0}^{k - 1} \frac{1}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^{t + 1}} \Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big] + 2\stepsize L\constLPH{0}\sum_{t = 0}^{k - 1}\gamma_{t + 1}.
	\end{align*}
	
	
	We also note that
	\begin{align*}
		\frac{\gamma_{k}}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k}}\Exs\left[\norm{\theta_{k} - \thetastar}\right] \leq \stepsize\frac{\Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k}} + \frac{\stepsize}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k}}.
	\end{align*}
	similarly
	\begin{align*}
		\frac{\gamma_0}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^k} \Exs \big[ \norm{\theta_0 - \thetastar} \big] \leq \stepsize \Exs \big[ \norm{\theta_0 - \thetastar}^2 \big] + \stepsize.
	\end{align*}
	and we also define for $0 \le t \le k$
	\begin{align*}
		c_{3, t} \coloneq \frac{1}{-\stepsize L^{2} +‌ \mu}\frac{2\stepsize L \constLPH{0}}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}} + \frac{\stepsize\constLPH{0}}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}} \stepsize\constLPH{0}
	\end{align*}
	to wrap up all the remainder terms.
	
	Substituting back and rearranging with also defining $c_{2, t}^{\prime} \coloneq \frac{c_{2, t}}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}}$ and $c_{1, t}^{\prime} \coloneq \frac{c_{1, t}}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}}$, yields
	\begin{align*}
		\frac{\Exs \big[ \norm{\theta_{k + 1} - \thetastar}^2 \big]}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^k} & \leq  \frac{\stepsize}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k}}\Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right] + \sum_{t = 0}^{k - 1}\frac{\stepsize\left(8L \constLPH{1} + 4\stepsize\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{-1}L\constLPH{0}\right)}{\left(1 - 2\stepsize \left(-\stepsize L^{2} + \mu\right)\right)^{t}}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \\
		& \stepsize\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] +‌ c_{1, k}^{\prime} + c_{2, k}^{\prime} + c_{3, k}.
	\end{align*}
	
	for sufficiently small $\stepsize$s, we have
	\begin{align*}
		\stepsize\left(\frac{3}{2}L \constLPH{1} + 2\stepsize\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{-1}L\constLPH{0}\right) \leq 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + 2L \constLPH{1}\right)
	\end{align*}
	using above simplification we can rewrite our upper bound as
	\begin{align*}
		\frac{\Exs \big[ \norm{\theta_{k + 1} - \thetastar}^2 \big]}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^k} & \leq 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + 2L \constLPH{1}\right)\sum_{t = 0}^{k}\frac{\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize \left(-\stepsize L^{2} + \mu\right)\right)^{t}} + \stepsize\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] +‌ c_{1, k}^{\prime} + c_{2, k}^{\prime} + c_{3, k}.
	\end{align*}
	
	
	For solving the above recursion, we first define $S_{t} \coloneq 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + 2L \constLPH{1}\right)\sum_{l = 0}^{t}\frac{\Exs\left[\norm{\theta_{l} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{l}}$ for $0 \le t \le k$. Also we use $C_{t} \coloneq c_{1, t}^{\prime} + c_{2, t}^{\prime} + c_{3, t}$ and for $0 \le t \le k$, defining constant terms. Now we can write
	\begin{align*}
		\frac{\Exs\left[\norm{\theta_{t + 1} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}} \le S_{t} + \stepsize\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] +‌ C_{t}.‌
	\end{align*}
	using this expansion, we should first notice that
	\begin{align*}
		\frac{S_{t}}{S_{t - 1}} & = \frac{S_{t - 1} + 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + 2L \constLPH{1}\right)\frac{\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}}}{S_{t - 1}} \\
		& = 1 + 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + 2L \constLPH{1}\right)\frac{S_{t - 1} + \stepsize\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + C_{t - 1}}{S_{t - 1}} \\
		& \leq 1 + 8\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + 2L \constLPH{1}\right).
	\end{align*}
	Now, since we have $S_{0} = 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + 2L\constLPH{1}\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right]$, thus
	\begin{align*}
		S_{t} \leq 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + 2L\constLPH{1}\right)\left[1 + 8\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + 2L\constLPH{1}\right)\right]^{t}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right].
	\end{align*}
	Substituting this upper bound into previous equations we get
	\begin{align*}
		& \Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] \leq\\
		& \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k}\left[4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + 2L\constLPH{1}\right)\left(1 + 8\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + 2L \constLPH{1}\right)\right)^{k}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \stepsize\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + C_{k}\right]
	\end{align*}
	
	Choosing $\stepsize$ sufficiently small, we can simplify the above inequality and write it as follows
	
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] \leq \tilde{c}_{1} \cdot \left(1 - 2\stepsize \mu\right)^{k + 1}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \tilde{c}_{2} \cdot 2\stepsize L\left(\constLPH{0} + \stepsize\constLPH{1}\right)
	\end{align*}
	where $\tilde{c}_{1}$ and $\tilde{c}_{2}$ are $\mathcal{O}\left(1\right)$ constants.
	
	
	
	
	
	\subsection{General Case}
	Similar to the previous case, we first prove a useful lemma:
	
	\textbf{Lemma3.} \textit{Using Assumptions 1, 3, 5, and $m, n \ge 1$, we have
		\begin{align*}
			\Exs\left[\norm{\theta_{t} - \theta_{t - 1}}^{m}\norm{\theta_{t - 1} - \thetastar}^{n}\right] \leq 2^{2m + n - 2}\stepsize^{m}L^{m}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{m + n}\right] + 1\right).
		\end{align*}
		Also when $m = 0$, this upper bound can be written as
		\begin{align}
			\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{n}\right] \leq 2^{n - 1}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{n}\right] + 1\right).
		\end{align}
	}
	
	\textbf{Proof.} We have
	\begin{align*}
		\Exs\left[\norm{\theta_{t} - \theta_{t - 1}}^{m}\norm{\theta_{t - 1} - \thetastar}^{n}\right] & \leq \stepsize^{m} \Exs\left[\norm{g\left(\theta_{t - 1}, X_{t}\right)  + \xi_{t}\left(\theta_{t - 1}\right)}^{m}\norm{\theta_{t - 1} - \thetastar}^{n}\right]\\
		& \leq 2^{m - 1}\stepsize^{m}\Exs\left[\left(\norm{g\left(\theta_{t - 1}, X_{t}\right)}^{m} + \norm{\xi_{t}\left(\theta_{t - 1}\right)}^{m}\right)\norm{\theta_{t - 1} - \thetastar}^{n}\right]\\
		& = 2^{m - 1}\stepsize^{m}\Exs\left[\Exs\left[\left(\norm{g\left(\theta_{t - 1}, X_{t}\right)}^{m} + \norm{\xi_{t}\left(\theta_{t - 1}\right)}^{m}\right) | \mathcal{F}_{t - 1}\right]\norm{\theta_{t - 1} - \thetastar}^{n}\right]\\
		& \leq 2^{m - 1}\stepsize^{m}\Exs\left[\left(L_{1}^{m}\Exs\left[\left(\norm{\theta_{t - 1} - \thetastar} + 1\right)^{m}\right] + L_{2}^{m}\Exs\left[\left(\norm{\theta_{t - 1} - \thetastar} + 1\right)^{m}\right]\right)\norm{\theta_{t - 1} - \thetastar}^{n}\right]\\
		& \leq 2^{m - 1}\stepsize^{m}L^{m}\Exs\left[\left(\norm{\theta_{t - 1} - \thetastar} + 1\right)^{m + n}\right]\\
		& \leq 2^{2m + n - 2}\stepsize^{m}L^{m}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{m + n}\right] + 1\right)
	\end{align*}
	where the second and the last line follows from the mclaurin's inequality. Also the fourth line follows from ??. Proof for the other case is a trivial consequence of these calculations.
	
	Now to do the proof in this case, we assume that the moment bound in [??] has been proven for $k \le n - 1$, we now proceed to show that the desired moment convergence holds for $n$ with $2 \le n \le p$.
	
	We start with the following decomposition of $\norm{\theta_{k + 1} - \thetastar}^{2n}$
	\begin{align*}
		\norm{\theta_{k + 1} - \thetastar}^{2n} & = \left(\norm{\theta_{k} - \thetastar}^{2} + 2\stepsize \left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)\right\rangle + \stepsize^{2}\norm{g\left(\theta_{x}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)}^{2} \right)^{n}\\
		& = \sum_{\substack{i, j, l \\ i + j + l = n}} \binom{n}{i, j, l}\norm{\theta_{k} - \thetastar}^{2i}\left(2\stepsize \left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)\right\rangle \right)^{j}\left(\stepsize \norm{g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)}\right)^{2l}
	\end{align*}
	We note the following cases.
	\begin{enumerate}
		\item $i = n$, $j = l = 0$. In this case, the summand is simply $\norm{\theta_{k} - \thetastar}^{2i}$.
		\item When $i = n - 1, j = 1$ and $l = 0$. In this case, the summand is of order $\stepsize$, i.e., $$\stepsize \cdot 2n\left \langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right) \right\rangle^{j} \norm{\theta_{k} - \thetastar}^{2(n - 1)}.$$ We can further decompose it as
		\begin{align*}
			& 2n\stepsize \left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right) \right\rangle\norm{\theta_{k} - \thetastar}^{2(n - 1)} \\
			& = \underbrace{2n\stepsize\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k+ 1}\right) - \bar{g}\left(\theta_{k}\right) + \xi_{k + 1}\left(\theta_{k}\right) \right\rangle \norm{\theta_{k} - \thetastar}^{2(n - 1)}}_{\constT{1}} + \underbrace{2n\stepsize \left\langle \theta_{k} - \thetastar, \bar{g}\left(\theta_{k}\right) \right\rangle \norm{\theta_{k} - \thetastar}^{2(n - 1)}}_{\constT{2}}.
		\end{align*}
		Note that, when $\left(X_{k}\right)$ is i.i.d or from a martingale noise, we have
		$$\Exs\left[\constT{1} | \theta_{k}\right] = 0$$
		However, when $\left(X_{k}\right)$ is Markovian, the above inequality does not hold and $\constT{1}$ requires careful analysis.\\
		Nonetheless, under the strong monotonicity assumption, we have
		$$\constT{2} \le -2n\stepsize\mu\norm{\theta_{k} - \thetastar}^{2n}.$$
		\item For the remaining terms, we see that they are of higher orders of $\stepsize$. Therefore, when $\stepsize$ is selected sufficiently small, these terms do not raise concern. 
	\end{enumerate}
	
	Therefore, to prove the desired moment bound, we spend the remaining section analyzing $\constT{1}$. Immediately, we note that
	\begin{align*}
		\Exs\left[\constT{1}\right] &= \Exs\left[2n\stepsize \left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right) + \Exs\left[\xi_{k + 1}\left(\theta_{k}\right) | \theta_{k}\right] \right\rangle\norm{\theta_{k} - \thetastar}^{2(n - 1)}\right]\\
		& = 2n\stepsize\Exs\left[\underbrace{\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right)\right\rangle \norm{\theta_{k} - \thetastar}^{2(n - 1)}}_{\constTprime{1}}\right].
	\end{align*}
	Subsequently, we focus on analyzing $\constTprime{1}$; but before that, we write the general recursion of the error bound. First, we define $\constTprime{1, t} \coloneq \left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right)\norm{\theta_{t} - \thetastar}^{2(n - 1)}\right\rangle$ to make $\constTprime{1}$ dependent on the iteration index. Now, following the above decomposition and taking the expectations, we have:
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2n}\right] \leq \Exs\left[\norm{\theta_{k} - \thetastar}^{2n}\right] + 2n\stepsize\Exs\left[\constTprime{1, k}\right] - 2n\stepsize\mu\Exs\left[\norm{\theta_{k} - \thetastar}^{2n}\right] + o\left(\stepsize\right) = \left(1 - 2n\stepsize\mu\right)\Exs\left[\norm{\theta_{k} - \thetastar}^{2n}\right] + 2n\stepsize\Exs\left[\constTprime{1, k}\right] + o\left(\stepsize\right)
	\end{align*}
	similarly to the previous case we define $\gamma_{t} \coloneq 2n\stepsize\left(1 - 2n\stepsize\mu\right)^{k - t}$ for $0 \le t \le k$. Solving the above recursion will give us
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2n}\right] \le \sum_{t = 0}^{k}\gamma_{t}\Exs\left[\constTprime{1, t}\right] + \gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2n}\right] + o\left(\stepsize\right)
	\end{align*}
	
	We have to upper bound the first term in the RHS above. For this purpose, we use a similar decomposition to our base case analysis:
	\begin{align*}
		\sum_{t = 0}^{k}\gamma_{t}\Exs\left[\constTprime{1, t}\right] = \Exs\left[\sum_{t = 0}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right)\right\rangle\norm{\theta_{t} - \thetastar}^{2(n - 1)}\right] = A_{1} + A_{2} + A_{3} + A_{4} + A_{5}
	\end{align*} 
	with
	\begin{align*}
		A_{1} \coloneq & \Exs\left[\sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, \hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) \right\rangle\norm{\theta_{t} - \thetastar}^{2(n - 1)}\right],\\
		A_{2} \coloneq & \Exs\left[\sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) - P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1}, X_{t} \right) \right\rangle\norm{\theta_{t} - \thetastar}^{2(n - 1)}\right],\\
		A_{3} \coloneq & \Exs\left[\sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \theta_{t - 1}, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1} X_{t}\right) \right\rangle \norm{\theta_{t} - \thetastar}^{2(n - 1)}\right],\\
		A_{4} \coloneq & \Exs\left[\sum_{t = 1}^{k}\left(\gamma_{t} - \gamma_{t - 1}\right)\left\langle \theta_{t - 1} - \thetastar, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1} - \thetastar, X_{t}\right) \right\rangle\norm{\theta_{t} - \thetastar}^{2(n - 1)}\right],\\
		A_{5} \coloneq & \Exs\left[\sum_{t = 1}^{k}\gamma_{t - 1}\left\langle \theta_{t - 1} - \thetastar, P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1} - \thetastar, X_{t}\right)\right\rangle\left(\norm{\theta_{t} - \thetastar}^{2(n - 1)} - \norm{\theta_{t - 1} - \thetastar}^{2(n - 1)}\right)\right],\\
		A_{6} \coloneq & \Exs\left[\gamma_{0}\left\langle \theta_{0} - \thetastar, \hat{g}\left(\theta_{0}, X_{0}\right) \right\rangle\norm{\theta_{0} - \thetastar}^{2(n - 1)}\right] - \Exs\left[\gamma_{k}\left\langle \theta_{k} - \thetastar, P_{\theta_{k}}\hat{g}\left(\theta_{k}, X_{k + 1}\right)\right\rangle\norm{\theta_{k} - \thetastar}^{2(n - 1)}\right].
	\end{align*}
	For $A_{1}$, we note that $\hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}} \hat{g}\left(\theta_{t}, X_{t}\right)$ is a martingale difference sequence [cf. ?] and therefore we have $A_{1} = 0$ by taking the total expectation.
	
	For $A_{2}$, applying Cauchy-Schwarz inequality and ??, we have
	\begin{align*}
		A_{2} & \leq \sum_{t = 1}^{k}\constLPH{1}\gamma_{t} \Exs\left[\norm{\theta_{t} - \theta_{t - 1}} \; \norm{\theta_{t} - \thetastar}^{2n - 1}\right]\\
		& \leq \sum_{t = 1}^{k}\constLPH{1}\gamma_{t}\Exs\left[\norm{\theta_{t} - \theta_{t - 1}}\left(\norm{\theta_{t} - \theta_{t - 1}} + \norm{\theta_{t - 1} - \thetastar}\right)^{2n - 1}\right]\\
		& \leq \sum_{t = 1}^{k}2^{2n - 2}\constLPH{1}\gamma_{t}\Exs\left[\norm{\theta_{t} - \theta_{t - 1}}^{2n} + \norm{\theta_{t} - \theta_{t - 1}} \; \norm{\theta_{t - 1} - \thetastar}^{2n - 1}\right]\\
		& \leq \sum_{t = 1}^{k}2^{2n - 2}\constLPH{1}\gamma_{t}\left(2^{4n}\stepsize^{2n}L^{2n}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n}\right] + 1\right) + 2^{2n - 1}\stepsize L \left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n}\right] + 1\right)\right)\\
		& \leq \sum_{t = 1}^{k}2^{4n - 2}\stepsize L \constLPH{1}\gamma_{t}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n}\right] + 1\right)
	\end{align*}
	where the second line follows from triangle inequality, third line from mclaurin's inequality, fourth line from Lemma 3, and the last line for $\stepsize < \frac{1}{4L}$.
	
	For $A_{3}$, we obtain
	\begin{align*}
		A_{3} & \leq \sum_{t = 1}^{k}\gamma_{t}\Exs\left[\norm{\theta_{t} - \theta_{t - 1}} \; \norm{P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1}, X_{t}\right)} \; \norm{\theta_{t} - \thetastar}^{2(n - 1)}\right]\\
		& \leq \sum_{t = 1}^{k}\constLPH{0}\gamma_{t}\Exs\left[\norm{\theta_{t} - \theta_{t - 1}}\left(\norm{\theta_{t} - \theta_{t - 1}} + \norm{\theta_{t - 1} - \thetastar}\right)^{2(n - 1)}\right]\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}\constLPH{0}\gamma_{t}\Exs\left[\norm{\theta_{t} - \theta_{t - 1}}^{2n - 1} + \norm{\theta_{t} - \theta_{t - 1}} \; \norm{\theta_{t - 1} - \thetastar}^{2(n - 1)}\right]\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}\constLPH{0}\gamma_{t}\left(2^{4n - 1}\stepsize^{2n - 1}L^{2n - 1}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n - 1}\right] + 1\right) + 2^{2n - 2}\stepsize L\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n - 1}\right] + 1\right)\right)\\
		& \leq \sum_{t = 1}^{k}2^{4n - 4}\stepsize L\constLPH{0}\gamma_{t}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n - 1}\right] + 1\right)
	\end{align*}
	where second line follows from triangle inequality, third line from mclaurin's inequality, fourth line from Lemma 3, and the last line for $\stepsize < \frac{1}{4L}$.
	
	For $A_{4}$, we have
	\begin{align*}
		A_{4} & \leq \sum_{t = 1}^{k}| \gamma_{t} - \gamma_{t - 1}| \; \Exs\left[\norm{\theta_{t - 1} - \thetastar} \norm{P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1}, X_{t}\right)} \; \norm{\theta_{t} - \thetastar}^{2(n - 1)}\right]\\
		& \leq \sum_{t = 1}^{k}\constLPH{0}| \gamma_{t} - \gamma_{t - 1}| \; \Exs\left[\norm{\theta_{t - 1} - \thetastar} \; \norm{\theta_{t} - \thetastar}^{2(n - 1)}\right]\\
		& \leq \sum_{t = 1}^{k}\constLPH{0}|\gamma_{t} - \gamma_{t - 1}| \Exs\left[\norm{\theta_{t - 1} - \thetastar}\left(\norm{\theta_{t} - \theta_{t - 1}} + \norm{\theta_{t - 1} - \thetastar}\right)^{2(n - 1)}\right]\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}\constLPH{0}|\gamma_{t} - \gamma_{t - 1}| \Exs\left[\norm{\theta_{t} - \theta_{t - 1}}^{2(n - 1)} \; \norm{\theta_{t - 1} - \thetastar}\ + \norm{\theta_{t - 1} - \thetastar}^{2n - 1}\right]\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}\constLPH{0}|\gamma_{t} - \gamma_{t - 1}| \left(2^{4n - 5}\stepsize^{2n - 2}L^{2n - 2}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n - 1}\right] + 1\right) + 2^{2n - 2}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n - 1}\right] + 1\right)\right)\\
		& \leq \sum_{t = 1}^{k}2^{4n - 4}\constLPH{0}|\gamma_{t} - \gamma_{t - 1}|\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n - 1}\right] + 1\right)
	\end{align*}
	where the third line follows from triangle inequality, fourth line from mclaurin's inequality, fifth line from Lemma 3, and the last line for $\stepsize < \frac{1}{4L}$. 
	
	Now for $A_{5}$, we have to first note that, using mean-value theorem and with $a \in [0, 1]$, we'll get
	\begin{align*}
		\norm{\theta_{t} - \theta^{*}}^{2(n - 1)} - \norm{\theta_{t - 1} - \thetastar}^{2(n - 1)} & = \norm{\theta_{t} - \theta_{t - 1}} \cdot 2(n - 1)\norm{a\left(\theta_{t} - \thetastar\right) + (1 - a)\left(\theta_{t - 1} - \thetastar\right)}^{2n - 3}\\
		& \leq \norm{\theta_{t} - \theta_{t - 1}} \cdot 2(n - 1)\norm{a\left(\theta_{t} - \theta_{t - 1}\right) + \theta_{t - 1} - \thetastar}^{2n - 3}\\
		& \leq 2^{2n - 3}(n - 1)\norm{\theta_{t} - \theta_{t - 1}}\left(\norm{\theta_{t} - \theta_{t - 1}}^{2n - 3} + \norm{\theta_{t - 1} - \thetastar}^{2n - 3}\right)
	\end{align*}
	where the last line follows using the mclaurin's inequality. Plugging in the above upper bound to $A_{5}$ gives us
	\begin{align*}
		A_{5} & \leq \sum_{t = 1}^{k}2^{2n - 3}(n - 1)\gamma_{t - 1}\Exs\left[\left\langle \theta_{t - 1} - \thetastar, P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1} - \thetastar, X_{t}\right)\right\rangle\norm{\theta_{t} - \theta_{t - 1}}\left(\norm{\theta_{t} - \theta_{t - 1}}^{2n - 3} + \norm{\theta_{t - 1} - \thetastar}^{2n - 3}\right)\right]\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}(n - 1)\gamma_{t - 1}\Exs\left[\norm{\theta_{t - 1} - \thetastar} \; \norm{P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1}, X_{t}\right)} \; \norm{\theta_{t} - \theta_{t - 1}}\left(\norm{\theta_{t} - \theta_{t - 1}}^{2n - 3} + \norm{\theta_{t - 1} - \thetastar}^{2n - 3}\right)\right]\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}(n - 1)\constLPH{0}\gamma_{t - 1}\Exs\left[\norm{\theta_{t - 1} - \thetastar} \; \norm{\theta_{t} - \theta_{t - 1}}\left(\norm{\theta_{t} - \theta_{t - 1}}^{2n - 3} + \norm{\theta_{t - 1} - \thetastar}^{2n - 3}\right)\right]\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}(n - 1)\constLPH{0}\gamma_{t - 1}\Exs\left[\norm{\theta_{t} - \theta_{t - 1}}^{2n - 2} \; \norm{\theta_{t - 1} - \thetastar} + \norm{\theta_{t} - \theta_{t - 1}} \; \norm{\theta_{t - 1} - \thetastar}^{2n - 2}\right]\\
		& \leq \sum_{t = 1}^{k} 2^{2n - 3}(n - 1)\constLPH{0}\gamma_{t - 1}\left(2^{4n - 5}\stepsize^{2n - 2}L^{2n - 2}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n - 1}\right] + 1\right) + 2^{2n - 2}\stepsize L\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n - 1}\right] + 1\right)\right)\\
		& \leq \sum_{t = 1}^{k}2^{4n - 4}(n - 1)\stepsize L \constLPH{0}\gamma_{t - 1}\left(\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n - 1}\right] + 1\right)
	\end{align*}
	where the fifth line follows from Lemma 3 and the last line follows for $\stepsize < \frac{1}{4L}$.
	
	Finally, for $A_{6}$, we obtain
	\begin{align*}
		A_{6} \leq \constLPH{0}\left(\gamma_{0}\norm{\theta_{0} - \thetastar}^{2n - 1} + \gamma_{k}\norm{\theta_{k} - \thetastar}^{2n - 1}\right)
	\end{align*}
	which follows from Cauchy-Schwarz inequality and ??.
	
	Combining the above terms gives us:
	\begin{align*}
		\sum_{t = 0}^{k}\gamma_{t}\Exs\left[\constTprime{1, t}\right] &= \Exs\left[\sum_{t = 0}^{k}\gamma_{t}\left\langle\theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right)\right\rangle \norm{\theta_{t} - \thetastar}^{2(n - 1)}\right]\\
		& \leq \sum_{t = 0}^{k - 1}2^{4n - 2}\stepsize L \constLPH{1}\gamma_{t + 1}\left(\Exs\left[\norm{\theta_{t} - \thetastar}^{2n}\right] + 1\right) + \sum_{t = 0}^{k - 1}2^{4n - 4}\stepsize L\constLPH{0}\gamma_{t + 1}\left(\Exs\left[\norm{\theta_{t} - \thetastar}^{2n - 1}\right] + 1\right) \\
		& + \sum_{t = 0}^{k - 1}2^{4n - 4}\constLPH{0}|\gamma_{t + 1} - \gamma_{t}|\left(\Exs\left[\norm{\theta_{t} - \thetastar}^{2n - 1}\right] + 1\right) + \sum_{t = 0}^{k - 1}2^{4n - 4}\left(n - 1\right)\stepsize L \constLPH{0}\gamma_{t + 1}\left(\Exs\left[\norm{\theta_{t} - \thetastar}^{2n - 1}\right] + 1\right) \\
		& + \constLPH{0}\left(\gamma_{0}\norm{\theta_{0} - \thetastar}^{2n - 1} + \gamma_{k}\norm{\theta_{k} - \thetastar}^{2n - 1}\right)\\
		& = \sum_{t = 0}^{k - 1}2^{4n - 2}\stepsize L \constLPH{1}\gamma_{t + 1}\left(\Exs\left[\norm{\theta_{t} - \thetastar}^{2n}\right] + 1\right) + \sum_{t = 0}^{k - 1}2^{4n - 4}\constLPH{0}\left(\left(n\stepsize L + 1\right)\gamma_{t + 1} - \gamma_{t}\right)\left(\Exs\left[\norm{\theta_{t} - \thetastar}^{2n - 1}\right] + 1\right)\\
		& + \constLPH{0}\left(\gamma_{0}\norm{\theta_{0} - \thetastar}^{2n - 1} + \gamma_{k}\norm{\theta_{k} - \thetastar}^{2n - 1}\right)
	\end{align*}
	where in the last equality we used the fact that for $\stepsize \leq \frac{\mu}{L^{2}}$, $\gamma_{t} \leq \gamma_{t + 1}$.
	
	Now, if we make use of the inequality $2|x|^{3} \leq x^{2} + x^{4}$ and consolidating the terms, for sufficiently small $\stepsize$, we have
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2n}\right] & \leq \sum_{t = 0}^{k - 1}2^{4n - 5}\left[8\stepsize L \constLPH{1}\gamma_{t + 1} + \constLPH{0}\left(\left(n\stepsize L + 1\right)\gamma_{t + 1} - \gamma_{t}\right)\right]\left(\Exs\left[\norm{\theta_{t} - \thetastar}^{2n}\right] + 1\right)\\ 
		& + \sum_{t = 0}^{k - 1}2^{4n - 5}\constLPH{0}\left(\left(n\stepsize L + 1\right)\gamma_{t + 1} - \gamma_{t}\right)\left(\Exs\left[\norm{\theta_{t} - \thetastar}^{2(n - 1)}\right] + 1\right) + \frac{\constLPH{0}}{2}\left(\gamma_{0}\norm{\theta_{0} - \thetastar}^{2(n - 1)} + \gamma_{k}\norm{\theta_{k} - \thetastar}^{2(n - 1)}\right)\\
		& + \left(\frac{\constLPH{0}}{2} + 1\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2n}\right] + \frac{\constLPH{0}}{2}\gamma_{k}\norm{\theta_{k} - \thetastar}^{2n} + o\left(\stepsize\right)\\
		& \leq \sum_{t = 0}^{k - 1}2^{4n - 4}\stepsize L \gamma_{t + 1}\left[4\constLPH{1} + n\constLPH{0}\right]\Exs\left[\norm{\theta_{t} - \thetastar}^{2n}\right] + \sum_{t = 0}^{k - 1}2^{4n - 4}n\stepsize L \constLPH{0}\Exs\left[\norm{\theta_{t} - \thetastar}^{2(n - 1)}\right]\\
		& + \frac{\constLPH{0}}{2}\left(\gamma_{0}\norm{\theta_{0} - \thetastar}^{2(n - 1)} + \gamma_{k}\norm{\theta_{k} - \thetastar}^{2(n - 1)}\right) + \left(\frac{\constLPH{0}}{2} + 1\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2n}\right] + \frac{\constLPH{0}}{2}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}^{2n}\right] + \tilde{c}_{1, k} + o\left(\stepsize\right)
	\end{align*}
	where the last inequality follows for $\left(n\stepsize L + 1\right)\gamma_{t + 1} - \gamma_{t} \leq 2n\stepsize L \gamma_{t +‌ 1}$ and also with
	\begin{align*}
		\tilde{c}_{1, t} \coloneq \sum_{l = 0}^{t - 1}2^{4n - 4}n\stepsize L \gamma_{t +‌1}\left[4\constLPH{1} + n\constLPH{0}\right] + \sum_{l = 0}^{t - 1}2^{4n - 4}n\stepsize L\constLPH{0}
	\end{align*}
	 being defined for $1 \le t \le k - 1$. It can be seen that for sufficiently small $\stepsize$, $\tilde{c}_{1, t}$ would be of order $4^{n}n\stepsize L\left(n\constLPH{0} + 4\stepsize \constLPH{1}\right)$. Now from induction hypothesis we know that
	\begin{align*}
		\Exs\left[\norm{\theta_{t} - \thetastar}^{2(n - 1)}\right] \leq \tilde{C}_{n - 1, 1} \cdot \left(1 - 2n\stepsize \mu\right)^{t}\Exs\left[\norm{\theta_{0} - \thetastar}^{2n}\right] + \tilde{C}_{n - 1, 2} \cdot 4^{n - 1}(n - 1)\stepsize L\left((n - 1)\constLPH{0} + 2\stepsize\constLPH{1}\right).
	\end{align*}
	Thus, if we plug in this upper bound into the previous inequality, for $\stepsize < \frac{1}{4^{n}}$ we will have
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2n}\right] & \leq \sum_{t = 0}^{k - 1}2^{4n - 4}\stepsize L\gamma_{t + 1}\left[4\constLPH{1} + n\constLPH{0}\right]\Exs\left[\norm{\theta_{t} - \thetastar}^{2n}\right]\\
		& + \frac{\constLPH{0}}{2}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}^{2n}\right] +‌ \tilde{c}_{2, k}\Exs\left[\norm{\theta_{0} - \thetastar}^{2n}\right] + \tilde{c}_{1, k} + \tilde{c}_{3, k} + o\left(\stepsize\right)
	\end{align*}
	in which $\tilde{c}_{3, k}$ would be of order $4^{n}n\stepsize L\left(n\constLPH{0} + 4\stepsize \constLPH{1}\right)$ and $\tilde{c}_{2, k}$ is an $\mathcal{O}(1)$ constant dependent to $k$.
	
	To solve the above recursion, we define $\tilde{S}_{t} \coloneq \frac{\constLPH{0}}{2}\gamma_{t}\Exs\left[\norm{\theta_{t} - \thetastar}^{2n}\right] + \sum_{l = 0}^{t - 1}2^{4n - 4}\stepsize L \gamma_{l + 1}\left[4\constLPH{1} + n\constLPH{0}\right]\Exs\left[\norm{\theta_{l} - \thetastar}^{2n}\right]$ for $1 \leq t \leq k$. Using this we can write
	\begin{align*}
		\Exs\left[\norm{\theta_{t + 1} - \thetastar}^{2n}\right] & \leq \tilde{S}_{t} + \tilde{c}_{2, t}\Exs\left[\norm{\theta_{0} - \thetastar}^{2n}\right] + \tilde{c}_{1, t} + \tilde{c}_{3, t} + o\left(\stepsize\right).
	\end{align*}
	Now notice that we have(recall that $\gamma_{t} = 2n\stepsize\left(1 - 2n\stepsize\mu\right)^{k - t}$.)
	\begin{align*}
		\frac{\tilde{S}_{t}}{\tilde{S}_{t - 1}} &‌ \leq \frac{\tilde{S}_{t - 1} + \left(2^{4n - 4}\stepsize L \gamma_{t}\left[4\constLPH{1} + n\constLPH{0}\right] - \frac{\constLPH{0}}{2}\gamma_{t - 1}\right)\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n}\right] + \frac{\constLPH{0}}{2}\gamma_{t}\Exs\left[\norm{\theta_{t} - \thetastar}^{2n}\right]}{\tilde{S}_{t - 1}}\\
		& = \frac{\tilde{S}_{t - 1} + \left(2^{4n - 4}\stepsize L \gamma_{t}\left[4\constLPH{1} + n\constLPH{0}\right] - \frac{\constLPH{0}}{2}\gamma_{t - 1}\right)\Exs\left[\norm{\theta_{t - 1} - \thetastar}^{2n}\right] + \frac{\constLPH{0}}{2}\gamma_{t}\left[\tilde{S}_{t - 1} + \tilde{c}_{2, t - 1}\Exs\left[\norm{\theta_{0} - \thetastar}^{2n}\right] + \tilde{c}_{1, t} + \tilde{c}_{3, t} + o\left(\stepsize\right)\right]}{\tilde{S}_{t - 1}}
	\end{align*}
	it is not very hard to see that the if we take $\stepsize$ small enough above fraction could get as close as possible to $1$. This means that we can write our final error bound as the following
	\begin{align*}
		\Exs\left[\norm{\theta_{k +‌ 1} - \thetastar}^{2n}\right] & \leq \tilde{C}_{n, 1} \cdot \left(1 - 2n\stepsize\mu\right)^{k + 1}\Exs\left[\norm{\theta_{0} - \thetastar}^{2n}\right] + \tilde{C}_{n, 2} \cdot 4^{n}n\stepsize L\left(n\constLPH{0} + 4\stepsize \constLPH{1}\right)
	\end{align*}
	in which $\tilde{C}_{n, 1}$ and $\tilde{C}_{n, 2}$ are $\mathcal{O}(1)$ constants and also $\stepsize$ has been taken small enough.
	
	\section{Bias Characterization}
	First, we recall that as a consequence of Assumption 2, the target equation $\bar{g}\left(\theta\right) = \Exs_{X \sim \pi_{\theta}}\left[g\left(\theta, X\right)\right] = 0$, has a unique solution which we denote by $\thetastar$. Moreover, we define $\xstar$ to be the samples being drawn from the stationary distribution of the Markov chain related to $\thetastar$; \textit{i.e.,} $\pi_{\thetastar}$. Note that, from stationarity we know this is equal to the case that $\{X^{*}_{n}, n \in \mathbb{N}\}$ is a Markov chain initialized same as the other $\{X_{n}\}$ chain and evolving w.r.t. the $P_{\thetastar}$. In this case we also take $\mathcal{F}_{n} = \{\theta_{0}, \{\xi_{m + 1}\}_{m \leq n - 1}, \{X_{m}\}_{m \leq n}, \{X^{*}_{m}\}_{m \leq n} \}$. For notational simplicity, we omit indexes related to the \textit{optimal Chain}(\textit{i.e.,} $\{X^{*}_{n}\}$).
	
	Now, following the differentiability assumption of $g$ in Assumption 1, we can apply Taylor expansion to $g$ and note the following notation on residuals.
	\begin{align*}
		g(\theta, x) &= g\left(\thetastar, x\right) + g^{\prime}\left(\thetastar, x\right)\left(\theta - \thetastar\right) + \frac{1}{2}g^{\prime\prime}\left(\thetastar, x\right)\left(\theta - \thetastar\right)^{\otimes 2} + R_{3}\left(\theta, x\right)\\
		&= g\left(\thetastar, x\right) + g^{\prime}\left(\thetastar, x\right)\left(\theta - \thetastar\right) + R_{2}\left(\theta, x\right).
	\end{align*}
	By Assumptions 1 and 2 and our boundedness of the error results,  we note that the residual $R_{n}\left(\theta, X\right)$ satisfies
	\begin{align*}
		\sup_{x \in \mathcal{X}, \theta \in \mathbb{R}^{d}}\{\norm{R_{n}\left(\theta, x\right)} / \norm{\theta - \thetastar}^{n}\} < +\infty.
	\end{align*}
	Hence, we have
	\begin{align*}
		\norm{R_{n}\left(\thetainf, \xinfPone\right)}_{L^{2}\left(\pi_{\thetainf}\right)} \lesssim \Exs\left[\norm{\thetainf - \thetastar}^{n}\right] = \mathcal{O}\left(\stepsize^{n / 2}\right), \quad n = 2, 3, 4.
	\end{align*}
	Lastly we denote
	\begin{align*}
		\bar{g}\left(\theta\right) \coloneq \Exs_{X \sim \pi_{\theta}}\left[g\left(\theta, X\right)\right],& \quad \bar{g}_{2}\left(\theta\right) \coloneq \Exs_{X \sim \pi_{\theta}}\left[\left(g\left(\theta, X\right)\right)^{\otimes 2}\right],\\
		\bar{g}^{(1)}\left(\theta\right) \coloneq \Exs_{X \sim \pi_{\theta}}\left[g^{\prime}\left(\theta, X\right)\right],& \quad \bar{g}_{2}^{(1)}\left(\theta\right) \coloneq \Exs_{X \sim \pi_{\theta}}\left[\left(g^{\prime}\left(\theta, X\right)\right)^{\otimes 2}\right], \quad \bar{g}^{(2)}\left(\theta\right) \coloneq \Exs_{X \sim \pi_{\theta}}\left[g^{\prime\prime}\left(\theta, X\right)\right].
	\end{align*}
	
	Proceeding to computing the bias, it is good to notice in the beginning that
	\begin{align*}
		\Exs\left[\thetainfpone - \thetastar\right] = \Exs\left[\thetainf - \thetastar\right] + \stepsize\left(\Exs\left[g\left(\thetainf, \xinfPone\right)\right] + \Exs\left[\xi_{\infty + 1}\left(\thetainf\right)\right]\right),
	\end{align*}
	which immediately implies that $0 = \Exs\left[g\left(\thetainf, \xinfPone\right)\right]$. Writing the Taylor approximation again for this term and expanding it would give us:
	
	\begin{align*}
		\Exs\left[g\left(\thetainf, \xinfPone\right)\right] &= \Exs\left[g\left(\thetastar, \xinfPone\right)\right] + \Exs\left[g^{\prime}\left(\thetastar, \xinfPone\right)\left(\thetainf - \thetastar\right)\right] + \frac{1}{2}\Exs\left[\left(g^{\prime\prime}\left(\thetastar, \xinfPone\right)\right)\left(\thetainf - \thetastar\right)^{\otimes 2}\right] + \Exs\left[R_{3}\left(\thetainf, \xinfPone\right)\right]\\  &= \Exs\left[g\left(\thetastar, \xinfPone\right) - g\left(\thetastar, \xstar\right)\right] + \Exs\left[g\left(\thetastar, \xstar\right)\right] \\
		& + \Exs\left[\left(g^{\prime}\left(\thetastar, \xinfPone\right) - g^{\prime}\left(\thetastar, \xstar\right)\right)\left(\thetainf - \thetastar\right)\right] + \Exs\left[g^{\prime}\left(\thetastar, \xstar\right)\left(\thetainf - \thetastar\right)\right]\\
		& + \frac{1}{2}\Exs\left[\left(g^{\prime\prime}\left(\thetastar, \xinfPone\right) - g^{\prime\prime}\left(\thetastar, \xstar\right)\right)\left(\thetainf - \thetastar\right)^{\otimes 2}\right] + \frac{1}{2}\Exs\left[g^{\prime\prime}\left(\thetastar, \xstar\right)\left(\thetainf - \thetastar\right)^{\otimes 2}\right]\\
		& + \Exs\left[R_{3}\left(\thetainf, \xinfPone\right)\right]
	\end{align*}
	 
	 Firstly, note that from the definition we have $\Exs\left[g\left(\thetastar, X^{*}\right)\right] = 0$. Secondly, the two terms on the r.h.s. of the above($\Exs\left[g^{\prime}\left(\thetastar, \xstar\right)\left(\thetainf - \thetastar\right)\right]$ and $\frac{1}{2}\Exs\left[g^{\prime\prime}\left(\thetastar, \xstar\right)\left(\thetainf - \thetastar\right)^{\otimes 2}\right]$), are equivalent to the two terms in Bias Characterization of \cite{huo2024collusion} which means that we do not need to analyze them here. Hence, it remains to analyze the three other terms. 
	 
	 To analyze the first term, we introduce a new variable $\tilde{X}_{n, k}$. This is the random variable which has initialized from $X_{0}$, evolved according to the kernel $P_{\theta_{i}}$ for $i = 0, \dots, k - 1$, and then with the kernel $P_{\thetastar}$ for $n - k$ times:
	 \begin{align*}
	 	X_{0} \quad \underset{P_{\theta_{0}}}{\longmapsto}  \quad X_{1} \quad \underset{P_{\theta_{1}}}{\longmapsto} \quad \dots \quad \underset{P_{\theta_{k - 1}}}{\longmapsto} \quad X_{k} = \tilde{X}_{k, k} \quad \underset{P_{\thetastar}}{\longmapsto} \quad \dots \quad \underset{P_{\thetastar}}{\longmapsto} \quad \tilde{X}_{n - 1, k} \quad \underset{P_{\thetastar}}{\longmapsto} \quad \tilde{X}_{n, k}
	 \end{align*}
	 In addition, since we are interested in the difference between $g\left(\thetastar, \xinfPone\right)$ and $g\left(\thetastar, \xstar\right)$, and the decision parameter of the function is fixed here, for notational simplicity we use $f\left( . \right) \coloneq g\left(\thetastar, .\right)$. Also note that $f\left(\xinfPone\right)$ and $f\left(\xinf\right)$ are the same kind of objects in distribution. Using these new notations, we can write
	 \begin{align*}
	 	\Exs\left[g\left(\thetastar, \xinfPone\right) - g\left(\thetastar, \xstar\right)\right] = \lim_{n \to \infty}\Exs\left[f\left(\tilde{X}_{n, n}\right) - f\left(\tilde{X}_{n, 0}\right)\right] = \lim_{n \to \infty}\Exs\left[\sum_{k = 0}^{n - 1}f\left(\tilde{X}_{n, k + 1}\right) - f\left(\tilde{X}_{n, k}\right)\right].
	 \end{align*}
	 From the above we see that we are interested in calculating $f\left(\tilde{X}_{n, k +‌ 1}\right) - f\left(\tilde{X}_{n, k}\right)$. Notice that $\tilde{X}_{n, k +‌ 1}$ and $\tilde{X}_{n, k}$ are the same random variables in their first $k$ rounds of evolution. This means that we can write $\tilde{X}_{n, k + 1} = P_{\thetastar}^{n - k - 1}P_{\theta_{k}}\tilde{X}_{k, k}$ and $\tilde{X}_{n, k} = P_{\thetastar}^{n - k}\tilde{X}_{k, k}$. 
	 \\
	 \\
	 \\
	 \\
	 \\
	 \\	 
	 Starting with the first one, we have 
	 \begin{align*}
	 	\Exs\left[g\left(\thetastar, \xinfPone\right) - g\left(\thetastar, \xstar\right)\right] &= \Exs\left[\Exs\left[g\left(\thetastar, \xinfPone\right) | \thetainf, \xinf\right] - g\left(\thetastar, \xstar\right)\right]\\
	 	&= \Exs\left[P_{\thetainf}g\left(\thetastar, \xinf\right) - g\left(\thetastar, \xstar\right)\right]\\
	 	&= \Exs\left[P_{\thetainf}g\left(\thetastar, \xinf\right) - P_{\thetastar}g\left(\thetastar, \xinf\right) + P_{\thetastar}g\left(\thetastar, \xinf\right) - P_{\thetastar}g\left(\thetastar, \xstar\right)\right]\\
	 	&= \Exs\left[\left(P_{\thetainf} - P_{\thetastar}\right)g\left(\thetastar, \xinf\right) + P_{\thetastar}\left(g\left(\thetastar, \xinf\right) - g\left(\thetastar, \xstar\right)\right)\right],
	 \end{align*}
	 where the third equality follows from the fact that $\xstar$ comes from the stationary distribution of $\thetastar$. Now notice that we have from Assumption 1 that $\Exs\left[\norm{g\left(\thetastar, \xinf\right)}\right] \leq L_{1}$. Additionally,
	 \begin{align*}
	 	\Exs\left[\norm{P_{\thetainf} - P_{\thetastar}}\right] \leq L_{P}\Exs\left[\norm{\thetainf - \thetastar}\right],
	 \end{align*}
	 from Assumption 7. For the remaining term we also have
	 \begin{align*}
	 	\Exs\left[\norm{P_{\thetastar}\left(g\left(\thetastar, \xinf\right) - g\left(\thetastar, \xstar\right)\right)}\right] &= \Exs\left[\norm{\int_{\mathbb{R}}P_{\thetastar}\left[g\left(\thetastar, x\right)\pi_{\thetainf}(x) - g\left(\thetastar, x\right)\pi_{\thetastar}(x)\right] dx}\right]\\
	 	& \leq \Exs\left[L_{S}\norm{\thetainf - \thetastar}\norm{\int_{\mathbb{R}}P_{\thetastar}g\left(\thetastar, x\right)dx}\right]\\
	 	& \leq L_{1}L_{S}\Exs\left[\norm{\thetainf - \thetastar}\right]
	 \end{align*}
	 where the second line follows from Assumption 8. Summing up all these would give us
	 \begin{align*}
	 	\Exs\left[\norm{g\left(\thetastar, \xinfPone\right) - g\left(\thetastar, \xstar\right)}\right] \leq L_{1}\left(L_{P} + L_{S}\right)\Exs\left[\norm{\thetainf - \thetastar}\right],
	 \end{align*}
	 which is of $\mathcal{O}(\stepsize^{1 / 2})$, considering our error bound.
	 
	 To analyze the first derivative related terms, we use a method similar to \cite{wu2020finite} and \cite{zou2019finite} by conditioning on the information of $\tau$ steps before(what is the exact definition for this such that the TV distance between distributions becomes equal to $\alpha$).

	 From Assumption 1 we know that $\norm{g\left(\theta_{\infty - \tau + k}, X_{\infty - \tau + k + 1}\right)} \leq L_{1}\left(\norm{\theta_{\infty - \tau + k} - \thetastar} + 1\right)$. Thus we can write
	 \begin{align*}
	 	&\Exs\left[\norm{\left(g^{\prime}\left(\thetastar, \xinfPone\right) - g^{\prime}\left(\thetastar, \xstar\right)\right)\left(\thetainf - \thetastar\right)}\right] \\
	 	&\leq\Exs\left[\Exs\left[\norm{g^{\prime}\left(\thetastar, \xinfPone\right) - g^{\prime}\left(\thetastar, \xstar\right)}\norm{\left(\theta_{\infty - \tau} + \stepsize\sum_{k = 0}^{\tau - 1}\left[g\left(\theta_{\infty - \tau + k}, X_{\infty - \tau + k + 1}\right) + \xi_{\infty - \tau + k + 1}\left(\theta_{\infty - \tau + k + 1}\right)\right] - \thetastar\right)} | \mathcal{F_{\infty - \tau}}\right]\right]\\
	 	& \leq \Exs\left[\Exs\left[\norm{g^{\prime}\left(\thetastar, \xinfPone\right) - g^{\prime}\left(\thetastar, \xstar\right)}\left(\norm{\theta_{\infty - \tau} - \thetastar} + \norm{\stepsize\sum_{k = 0}^{\tau - 1}\left[g\left(\theta_{\infty - \tau + k}, X_{\infty - \tau + k + 1}\right) + \xi_{\infty - \tau + k + 1}\left(\theta_{\infty - \tau + k + 1}\right)\right]}\right)|\mathcal{F}_{\infty - \tau}\right]\right]\\
	 	& \leq \Exs\left[\Exs\left[\norm{g^{\prime}\left(\thetastar, \xinfPone\right) - g^{\prime}\left(\thetastar, \xstar\right)}\left[\norm{\theta_{\infty - \tau} - \thetastar} + \stepsize\sum_{k = 0}^{\tau - 1}\left(\norm{g\left(\theta_{\infty - \tau + k}, X_{\infty - \tau + k + 1}\right)} + \norm{\xi_{\infty - \tau + k + 1}\left(\theta_{\infty - \tau + k + 1}\right)}\right)\right]| \mathcal{F}_{\infty - \tau}\right]\right]. 
	 \end{align*}
	  To handle the above expression, we separate two multiplications arising from the sum in the r.h.s term; one involving an $\stepsize$ and the other one which is constant when being conditioned on $\mathcal{F_{\infty - \tau}}$. The second part which has an $\stepsize$ coefficient, is easier to take care of and for that part we only upper bound the l.h.s term using the trivial upper bound that Assumption 1 gives us. Also for the other term, since the r.h.s term is constant when being conditioned, we use a similar procedure to the previous step and use the fact that in $\tau$ steps the chain has mixed enough to do the final bounding. Writing all of these in mathematical terms would give us
	  \begin{align*}
	  	&\Exs\left[\Exs\left[\norm{g^{\prime}\left(\thetastar, \xinfPone\right) - g^{\prime}\left(\thetastar, \xstar\right)}\norm{\theta_{\infty - \tau} - \thetastar}|\mathcal{F_{\infty - \tau}}\right]\right] \\
	  	&= \Exs\left[\Exs\left[\norm{\left(P_{\thetainf} - P_{\thetastar}\right)g^{\prime}\left(\thetastar, \xinf\right) + P_{\thetastar}\left(g^{\prime}\left(\thetastar, \xinf\right) - g^{\prime}\left(\thetastar, \xstar\right)\right)}\norm{\theta_{\infty - \tau} - \thetastar}|\mathcal{F_{\infty - \tau}}\right]\right]\\
	  	&\leq \Exs\left[\Exs\left[\left(\norm{\left(P_{\theta_{\infty}} - P_{\thetastar}\right)g^{\prime}\left(\thetastar, \xinf\right)} + \norm{P_{\thetastar}\left(g^{\prime}\left(\thetastar, \xinf\right) - g^{\prime}\left(\thetastar, \xstar\right)\right)}\right)\norm{\theta_{\infty - \tau} - \thetastar}|\mathcal{F_{\infty - \tau}}\right]\right]\\
	  	&\leq L_{1}L_{P}\Exs\left[\Exs\left[\norm{\thetainf - \thetastar}\norm{\theta_{\infty - \tau} - \thetastar}|\mathcal{F_{\infty - \tau}}\right]\right] + \Exs\left[\Exs\left[\norm{\int_{\mathbb{R}}P_{\thetastar}\left[g\left(\thetastar, x\right)\pi_{\thetainf}(x) - g\left(\thetastar, x\right)\pi_{\thetastar}(x)\right]}|\mathcal{F_{\infty - \tau}}\right]\norm{\theta_{\infty - \tau} - \thetastar}\right]\\
	  	& \leq \mathcal{O}\left(\stepsize\right)\Exs\left[\norm{\theta_{\infty - \tau} - \thetastar}\right] + L_{1}L_{S}\Exs\left[\norm{\thetainf - \thetastar} + \mathcal{O}\left(\stepsize\right)\right] = \mathcal{O}\left(\stepsize^{2}\right) + 2\mathcal{O}\left(\stepsize\right) = \mathcal{O}\left(\stepsize\right),
	  \end{align*}
	 in which the second line in the above has been done similar to the previous steps by replacing $g^{\prime}$ with $g$ and using the fact that equality of two expressions would result in equality of their norms. And,
	 \begin{align*}
	 	&\Exs\left[\Exs\left[\stepsize\norm{g^{\prime}\left(\thetastar, \xinfPone\right) - g^{\prime}\left(\thetastar, \xstar\right)}\sum_{k = 0}^{\tau - 1}\left(\norm{g\left(\theta_{\infty - \tau + k}, X_{\infty - \tau + k + 1}\right)} +‌ \norm{\xi_{\infty - \tau + k + 1}\left(\theta_{\infty - \tau + k + 1}\right)}\right)|\mathcal{F_{\infty - \tau}}\right]\right]\\
	 	&\leq \Exs\left[\Exs\left[2\stepsize L_{1}\sum_{k = 0}^{\tau - 1}\left(\norm{g\left(\theta_{\infty - \tau + k},X_{\infty - \tau + k + 1}\right)} +‌ \norm{\xi_{\infty - \tau + k + 1}\left(\theta_{\infty - \tau + k + 1}\right)}\right)|\mathcal{F_{\infty - \tau}}\right]\right]\\
	 	& \leq 2\stepsize L_{1}\Exs\left[\sum_{k = 0}^{\tau - 1}\left(\norm{g\left(\theta_{\infty - \tau + k}, X_{\infty - \tau + k + 1}\right)} + \norm{\xi_{\infty - \tau + k + 1}\left(\theta_{\infty - \tau + k + 1}\right)}\right)\right] = \mathcal{O}\left(\stepsize \tau\right).
	 \end{align*}
	 Where the last equality easily follows from our boundedness assumptions of the function $g$ and the noise field. Thus, we showed that the first order term is of $\mathcal{O}\left(\stepsize \tau\right)$.
	 
	 Lastly, for the remaining second order term, using a Cauchy-Scwarz inequality, we have from Assumption 1 that
	 \begin{align*}
		\frac{1}{2}\Exs\left[\norm{\left(g^{\prime\prime}\left(\thetastar, \xinfPone\right) - g^{\prime\prime}\left(\thetastar, \xstar\right)\right)\left(\thetainf - \thetastar\right)^{\otimes 2}}\right] \leq \stepsize L_{1}\Exs\left[\norm{\left(\thetainf - \thetastar\right)^{\otimes 2}}\right].
	 \end{align*}
	 It has been showed in \cite{huo2024collusion} that $\Exs\left[\norm{\left(\thetainf - \thetastar\right)^{\otimes 2}}\right]$ is of $\mathcal{O}\left(\stepsize^{2} \tau\right)$, which results in our term to be of $\mathcal{O}\left(\stepsize^{2}\tau\right)$ and completes our proof.
	 
\section*{Family of Transition Kernels: Theoretical Properties and Experimental Analysis}

Below we summarize the nine kernels used in our experiments. For each kernel, we detail its theoretical properties and connect them to its observed behavior in the convergence plots.

\subsection*{1. Langevin (Gaussian) Kernel}
\begin{itemize}
	\item[\bfseries 1.1] \textbf{Transition Rule.} A discretization of the Ornstein-Uhlenbeck (OU) process: \( P_\theta(x,dy) = \mathcal{N}\! \left( x - \tfrac{\epsilon^2}{2}(x-\theta), \epsilon^2 \right)(dy). \)
	\item[\bfseries 1.2] \textbf{Stationary Law.} The stationary distribution is Gaussian, \(\pi_\theta = \mathcal{N}(\theta,1)\). The mean is \(m(\theta)=\theta\).
	\item[\bfseries 1.3] \textbf{Derivatives.} \(m'(\theta)=1\) and \(m''(\theta)=0\). Both are constant and globally Lipschitz.
	\item[\bfseries 1.4] \textbf{Experimental Analysis.} As the archetypal "smooth" kernel, the Langevin process exhibits the expected \(O(\alpha)\) bias behavior. The convergence curves for different stepsizes are clearly separated, indicating that smaller \(\alpha\) values lead to significantly more accurate estimates. A subtle non-linear effect may be visible, as the improvement for the smallest stepsizes appears less pronounced, though this could also suggest that these runs require more iterations to reach their asymptotic floor.
\end{itemize}

\subsection*{2. Oscillatory Kernel}
\begin{itemize}
	\item[\bfseries 2.1] \textbf{Transition Rule.} An OU-type process where the mean function \(m(\theta)\) oscillates wildly near zero:
	\[ m(\theta)= \begin{cases} \theta+\theta^2\sin(1/\theta), & \theta\neq0 \\ 0, & \theta=0 \end{cases} \]
	The transition is \( P_\theta(x,dy)=\mathcal{N}\!\left(x-\tfrac{\epsilon^2}{2}(x-m(\theta)), \epsilon^2\right)(dy) \).
	\item[\bfseries 2.2] \textbf{Stationary Law.} \(\pi_\theta = \mathcal{N}(m(\theta),1)\).
	\item[\bfseries 2.3] \textbf{Derivatives of \(m(\theta)\) (for \(\theta\neq0\)).}
	\[ m'(\theta)=1+2\theta\sin(1/\theta)-\cos(1/\theta), \]
	\[ m''(\theta)=2\sin(1/\theta)-\frac{2}{\theta}\cos(1/\theta)-\frac{1}{\theta^2}\sin(1/\theta). \]
	\item[\bfseries 2.4] \textbf{Lipschitz Properties.} \(m'(\theta)\) is bounded on compact sets but is not globally Lipschitz. \(m''(\theta)\) is unbounded near \(\theta=0\).
	\item[\bfseries 2.5] \textbf{Experimental Analysis.} This kernel is a canonical example of pathological non-smoothness. The experiments clearly demonstrate this: the convergence curves for all stepsizes are tightly clustered, showing almost no improvement in accuracy as \(\alpha\) decreases. This aligns with the theoretical prediction of a slower, \(O(\sqrt{\alpha})\)-like bias, where the benefits of a smaller stepsize are severely diminished by the violent oscillations of the kernel's derivatives.
\end{itemize}

\subsection*{3. OU Uniform Smooth Kernel}
\begin{itemize}
	\item[\bfseries 3.1] \textbf{Transition Rule.} An OU process driven by uniform noise: \( y_{t+1} = x_t - \tfrac{\epsilon^2}{2}(x_t-\theta) + \xi_t, \quad \xi_t \sim \mathcal{U}(-\epsilon, \epsilon). \)
	\item[\bfseries 3.2] \textbf{Stationary Law.} The mean of the stationary distribution is \(\mathbb{E}_{\pi_\theta}[X] = \theta\).
	\item[\bfseries 3.3] \textbf{Derivatives.} The mean function is \(m(\theta)=\theta\), with globally Lipschitz derivatives \(m'(\theta)=1\) and \(m''(\theta)=0\).
	\item[\bfseries 3.4] \textbf{Experimental Analysis.} While theoretically as smooth as the Langevin kernel, its experimental behavior is less ideal. The curves for different \(\alpha\) are not as clearly separated, appearing only slightly better than the Oscillatory kernel. This suggests that while smoothness is a necessary condition for \(O(\alpha)\) bias, the magnitude of the derivatives also matters. Here, the constant and relatively small derivatives may lead to a smaller leading constant in the bias term, making the differences between stepsizes harder to distinguish within the given number of iterations.
\end{itemize}

\subsection*{4. OU Uniform Sin Kernel}
\begin{itemize}
	\item[\bfseries 4.1] \textbf{Transition Rule.} An OU process with uniform noise and a mean function \( m(\theta)=\tfrac12|\sin\theta| \).
	\item[\bfseries 4.2] \textbf{Stationary Law.} The mean of the stationary distribution is \(\mathbb{E}_{\pi_\theta}[X] = m(\theta)\).
	\item[\bfseries 4.3] \textbf{Derivatives of \(m(\theta)\) (for \(\theta\neq k\pi, k \in \mathbb{Z}\)).}
	\[ m'(\theta)=\frac{1}{2} \cos\theta \cdot
	\begin{cases}
		1 & \text{if } \sin\theta > 0 \\
		-1 & \text{if } \sin\theta < 0
	\end{cases},
	\quad m''(\theta)=-\tfrac12|\sin\theta|.
	\]
	\item[\bfseries 4.4] \textbf{Lipschitz Properties.} \(m'(\theta)\) has jump discontinuities but is bounded. \(m''(\theta)\) is continuous, bounded, and globally Lipschitz.
	\item[\bfseries 4.5] \textbf{Experimental Analysis.} This is one of the most interesting cases. Despite having a non-Lipschitz first derivative, its convergence behavior is remarkably similar to the smooth kernels, exhibiting a clear separation between the curves for different \(\alpha\). This strongly suggests that the nature of the non-smoothness is critical. Because the "kinks" in the derivative are isolated and predictable, and the second derivative is well-behaved, the system retains the faster \(O(\alpha)\) bias scaling.
\end{itemize}

\subsection*{5. OU Uniform SqrtOsc Kernel}
\begin{itemize}
	\item[\bfseries 5.1] \textbf{Transition Rule.} An OU process with uniform noise and a scaled oscillatory mean function:
	\[ m(\theta)= \begin{cases} 0.1\left(\theta+\theta^2\sin(1/\theta)\right), & \theta\neq0 \\ 0, & \theta=0 \end{cases} \]
	\item[\bfseries 5.2] \textbf{Derivatives.} The derivatives of \(m(\theta)\) are scaled versions of those from the Oscillatory Kernel (2). The first derivative is bounded but not Lipschitz, and the second derivative is unbounded.
	\item[\bfseries 5.3] \textbf{Experimental Analysis.} This kernel exhibits clear non-smooth properties, consistent with its highly irregular derivatives. As with the Oscillatory kernel, the final error levels are tightly clustered, indicating that decreasing the stepsize provides minimal benefit. This reinforces the conclusion that unbounded or wildly oscillating derivatives severely degrade the convergence rate of the bias.
\end{itemize}

\subsection*{6. Discrete Smooth 500 Kernel}
\begin{itemize}
	\item[\bfseries 6.1] \textbf{Transition Rule.} A 500-state birth-death process on a scaled state space \(\mathcal{X} \subset [-1, 1]\). It is designed to mimic the OU dynamic. The probability of moving right from state \(x\) is a smooth logistic function of the difference \((\theta - x)\), creating a mean-reverting drift towards \(\theta\).
	\item[\bfseries 6.2] \textbf{Stationary Law.} For each \(\theta\), there is a unique stationary distribution \(\pi_\theta\). At \(\theta=0\), the process is a symmetric random walk on a centered state space, ensuring \(\mathbb{E}_{\pi_0}[X]=0\).
	\item[\bfseries 6.3] \textbf{Derivatives.} The transition probabilities are infinitely differentiable with respect to \(\theta\). Consequently, the mean of the stationary distribution, \(m(\theta) = \mathbb{E}_{\pi_\theta}[X]\), is a smooth function with globally Lipschitz derivatives.
	\item[\bfseries 6.4] \textbf{Experimental Analysis.} As intended, this kernel successfully discretizes the properties of its continuous counterpart, `OU Smooth`. The plot shows a clear separation between the different \(\alpha\) curves, consistent with the \(O(\alpha)\) bias expected from a smooth kernel. This demonstrates that discretization, when done carefully to preserve smoothness properties, does not fundamentally alter the convergence behavior.
\end{itemize}

\subsection*{7. RW Hinge 500 Kernel}
\begin{itemize}
	\item[\bfseries 7.1] \textbf{Transition Rule.} A random walk on \(\{0,\dots,499\}\) where the right-move probability is given by the hinge function \(p(\theta) = \min(1, \max(0, \theta))\).
	\item[\bfseries 7.2] \textbf{Stationary Law.} For \(0 < p(\theta) < 1\), the stationary distribution is \(\pi_\theta(i) \propto \left(\frac{p(\theta)}{1-p(\theta)}\right)^i\).
	\item[\bfseries 7.3] \textbf{Derivatives of \(p(\theta)\).}
	\[
	p'(\theta)=
	\begin{cases}
		1, & \text{if } 0 < \theta < 1, \\
		0, & \text{if } \theta < 0\text{ or }\theta > 1,
	\end{cases}
	\quad p''(\theta)=0 \quad (\text{a.e.}).
	\]
	\item[\bfseries 7.4] \textbf{Lipschitz Properties.} \(p'(\theta)\) is not Lipschitz due to jump discontinuities at \(\theta=0\) and \(\theta=1\).
	\item[\bfseries 7.5] \textbf{Experimental Analysis.} This is another fascinating case. Like `OU Sin`, this is a non-smooth discrete kernel that exhibits behavior remarkably similar to the smooth kernels, with clear \(O(\alpha)\) separation. This further supports the hypothesis that isolated, well-behaved "kinks" in the derivative do not degrade the bias to \(O(\sqrt{\alpha})\). It is also observable that the smallest stepsize has not yet fully converged, suggesting that the combination of discrete states and small derivative magnitudes can lead to slow convergence dynamics.
\end{itemize}

\subsection*{8. RW ClipOsc 500 Kernel}
\begin{itemize}
	\item[\bfseries 8.1] \textbf{Transition Rule.} A random walk on \(\{0,\dots,499\}\) where the right-move probability is a clipped, oscillatory function of \(\theta\), \( p(\theta) = \min(1, \max(0, r(\theta))) \), where \( r(\theta)=\theta\sqrt{|\theta|}\,\sin(1/\theta) \).
	\item[\bfseries 8.2] \textbf{Derivatives.} The underlying function's derivative \(r'(\theta)\) oscillates unboundedly near \(\theta=0\), making the derivatives of the transition probability non-Lipschitz.
	\item[\bfseries 8.3] \textbf{Experimental Analysis.} This kernel clearly demonstrates its non-smooth properties. The final error levels show significant clustering, characteristic of a slower bias convergence rate. The plot also highlights extremely slow convergence for the two smallest stepsizes, which have not reached their asymptotic floors. This indicates that the combination of discrete states and pathological derivative behavior can make convergence exceptionally slow.
\end{itemize}

\subsection*{9. Two-State Centered Kernel}
\begin{itemize}
	\item[\bfseries 9.1] \textbf{Transition Rule.} A two-state Markov chain on \(\{-0.5, 0.5\}\) where the probability of switching states is \(p(\theta) = \cos^2(\theta)\).
	\item[\bfseries 9.2] \textbf{Stationary Law.} The stationary distribution is uniform, \(\pi_\theta(-0.5) = \pi_\theta(0.5) = 1/2\), so its mean is always zero.
	\item[\bfseries 9.3] \textbf{Derivatives.} The derivatives of the transition probability \(p(\theta)\) are \( p'(\theta) = -\sin(2\theta) \) and \( p''(\theta) = -2\cos(2\theta) \). Both are globally Lipschitz.
	\item[\bfseries 9.4] \textbf{Experimental Analysis.} As a simple, smooth discrete kernel, it displays the expected \(O(\alpha)\) behavior with clear separation between the curves. However, like other discrete kernels, the final convergence appears slow, suggesting that reaching the asymptotic limit requires a very large number of iterations.
\end{itemize}



\subsection*{Key Findings and Theoretical Justification}

\begin{itemize}
	\item \textbf{Universal Ergodicity and Foundational Assumptions:} All nine kernels are ergodic and satisfy the foundational assumption that the transition probability \(P_\theta\) is Lipschitz in \(\theta\) with respect to the total variation norm. This ensures a valid theoretical framework for the stochastic approximation analysis.
	
	\item \textbf{Correlation of Asymptotic Bias with Derivative Regularity:} The simulation results reveal a clear correlation between the asymptotic behavior of the iterates and the regularity of the kernel's parameter function derivatives. However, the initial hypothesis of a simple binary split between "smooth" ($O(\alpha)$ bias) and "non-smooth" ($O(\sqrt{\alpha})$ bias) kernels is insufficient. The experimental data suggests a more nuanced categorization based on the nature and magnitude of the derivative's behavior.
	
	\begin{itemize}
		\item \textbf{Class 1: Smooth Kernels with Clear \(O(\alpha)\) Bias.} This class includes the \textbf{Langevin}, \textbf{OU Smooth}, \textbf{Discrete Smooth 500}, and \textbf{Two-State} kernels. These kernels all possess globally Lipschitz first and second derivatives. As predicted by theory, they exhibit a distinct separation between the convergence curves for different stepsizes, confirming the expected \(O(\alpha)\) scaling of the asymptotic bias. For the Langevin kernel, this separation is particularly clear. For OU Smooth and its discrete counterpart, the effect is visible but less pronounced, suggesting that the smaller magnitude of their derivatives results in a smaller leading constant in the bias term, making the curves appear closer. The slow final convergence of the discrete kernels also highlights the impact of the Markov chain's mixing time.
		
		\item \textbf{Class 2: Pathological Kernels with Clustered Bias.} This class includes the \textbf{Oscillatory}, \textbf{OU SqrtOsc}, and \textbf{RW ClipOsc} kernels. These were designed with severe irregularities, such as wildly oscillating or unbounded derivatives near the origin. The experimental results are stark: the final error levels for all stepsizes are tightly clustered together. This indicates that decreasing the stepsize yields almost no improvement in accuracy, a hallmark of a much slower bias convergence rate, likely on the order of \(O(\sqrt{\alpha})\). The extremely slow convergence of the RW ClipOsc kernel for small \(\alpha\) further demonstrates how pathological derivative behavior, combined with the memory of a discrete chain, can severely hamper performance.
		
		\item \textbf{Class 3: Piecewise-Smooth Kernels Exhibiting \(O(\alpha)\) Bias.} This is the most insightful category, containing the \textbf{OU Sin} and \textbf{RW Hinge 500} kernels. Theoretically, both are "non-smooth" because their first derivatives are not Lipschitz, possessing jump discontinuities. However, their experimental behavior is nearly indistinguishable from the truly smooth kernels, showing clear \(O(\alpha)\) separation. This strongly suggests that the nature of the non-smoothness is paramount. Because the derivatives of these kernels are well-behaved and constant between isolated "kinks," the system retains the faster bias scaling. This challenges the simple smooth/non-smooth dichotomy and indicates that as long as the derivatives are not pathologically oscillatory, the algorithm's performance remains robust.
	\end{itemize}
\end{itemize}
	 
	\bibliographystyle{ieee}
	\bibliography{ref.bib}
	
\end{document}