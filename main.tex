\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{outline} \usepackage{pmgraph} \usepackage[normalem]{ulem}
\usepackage{graphicx} \usepackage{verbatim}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{tgbonum}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{lastpage}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
% \usepackage{minted} % need `-shell-escape' argument for local compile



\newcommand{\norm}[1]{\|#1 \|}
\newcommand{\Exs}{\mathbb{E}}
\newcommand{\thetastar}{\theta^*}
\newcommand{\constLPH}[1]{L_{PH}^{(#1)}}
\newcommand{\constT}[1]{T_{#1}}
\newcommand{\constTprime}[1]{T_{#1}^{\prime}}
\newcommand{\mwlcomment}[1]{{\color{orange} #1}}
\newcommand*{\htwai}[1]{\textbf{\textcolor{red}{To: #1}}}
\newcommand{\stepsize}{\alpha}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

\begin{document}
	
	\fontfamily{cmr}\selectfont
	\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{0.5pt} \\
		\LARGE \textbf{\uppercase{Nonlinear Markovian Stochastic Approximation}
			\HRule{2pt} \\ [0.5cm]
			\normalsize \today \vspace*{5\baselineskip}}
	}
	
	\date{}
	
	\author{
		Mohammadhadi Hadavi \\ 
		Prof. Hoi-To Wai - Chinese University of Hong Kong \\
		Prof. Wenlong Mou - University of Toronto}
	
	\maketitle
	\newpage
	%\tableofcontents
	%\newpage
	
	\section{Preliminaries}
	
	\textbf{Notations} The Euclidean norm is denoted by $\norm{.}$. The lowercase letter $c$ and its derivatives $c^{\prime}, c_{0},$ etc. denote universal numerical constants, whose value may change from line to line. As we are primarily interested in dependence of $\stepsize$ and $k$, we adopt the following big-$O$ notation: $\norm{f} = \mathcal{O}\left(h\left(\stepsize, k\right)\right)$ if it holds that $\norm{f} \le s \cdot \norm{h\left(\stepsize, k\right)}$ for some constant $s > 0$.
	
	We use of the following iteration scheme:
	\begin{align*}
		\theta_{t + 1} = \theta_{t} + \stepsize\left(g\left(\theta_{t}, X_{t + 1}\right) + \xi_{t + 1}\left(\theta_{t}\right)\right)
	\end{align*}
	
	
	\subsection{Assumptions}
	\textbf{Assumption 1} \textit{
		For each $X \in \mathcal{X}$, the function $g\left(\theta, X\right)$ is three times continuously differentiable in $\theta$ with uniformly bounded first to third derivatives, i.e., $\sup_{\theta \in \mathbb{R}^{d}}\norm{g^{(i)}\left(\theta, X\right)} < \infty$ for $i = 1, 2, 3, X \in \mathcal{X}$. Moreover, there exists a constant $L_{1} > 0$ such that (1) $\norm{g^{(i)}\left(\theta, X\right) - g^{(i)}\left(\theta^{\prime}, X\right)} \le L_{1}$, for all $\theta, \theta^{\prime} \in \mathbb{R}^{d}, i = 0, 1, 2$ and $X \in \mathcal{X}$, and (2) $\norm{g\left(0, X\right)} \le L_{1}$ for all $X \in \mathcal{X}$.
	}
	
	Assumption 1 implies that $g\left(\theta, X\right)$ is $L_{1}$-Lipschitz w.r.t $\theta$ uniformly in $X$. The above assumption immediately implies that the growth of $\norm{g}$ and $\norm{\bar{g}}$ will be at most linear in $\theta$, i.e., $\norm{g\left(\theta, X\right)} \le L_{1}\left(\norm{\theta - \thetastar} + 1\right)$ and $\norm{\bar{g}\left(\theta\right)} \le L_{1}\left(\norm{\theta - \thetastar} + 1\right)$. 
	\\
	\\
	\textbf{Assumption 2} \textit{
		There exists $\mu > 0$ such that $\left\langle \theta - \theta^{\prime}, \bar{g}(\theta) - \bar{g}(\theta^{\prime}) \right\rangle \le -\mu\norm{\theta - \theta^{\prime}}^{2}, \forall \theta, \theta^{\prime} \in \mathbb{R}^{d}$. Consequently, the target equation $\bar{g}(\theta) = 0$ has a unique solution $\thetastar$.
	}
	\\
	
	Denote by $\mathcal{F}_{k}$ the filtration generated by $\{X_{t + 1}, \theta_{t}, \xi_{t + 1}\}_{t = 0}^{k - 1} \cup \{X_{k + 1}, \theta_{k}\}$.
	\\
	\textbf{Assumption 3} \textit{
		Let $p \in \mathbb{Z}_{+}$ be given. The noise sequence $\left(\xi_{k}\right)_{k \ge 1}$ is a collection of i.i.d random fields satisfying the following conditions with $L_{2, p} > 0$:
		$$\Exs\left[\xi_{k + 1}(\theta) | \mathcal{F}_{k}\right] = 0 \quad \text{and} \quad \Exs^{1 / (2p)}\left[\norm{\xi_{1}(\theta)}^{2p}\right] \le L_{2, p}\left(\norm{\theta - \thetastar} + 1\right), \quad \forall \theta \in \mathbb{R}^{d}.$$
		Define $C(\theta) = \Exs\left[\xi_{1}(\theta)^{\otimes 2}\right]$ and assume that $C(\theta)$ is at least twice differentiable. There also exists $M_{\epsilon}, k_{\epsilon} \ge 0$ such that for $\theta \in \mathbb{R}^{d}$, we have $\max_{i = 1, 2}\norm{C^{(i)}(\theta)} \le M_{\epsilon}\{1 + \norm{\theta - \thetastar}^{k_{\epsilon}}\}$.
	}
	In the sequel, we set $L \coloneq L_{1} + L_{2}$, and without loss of generality, we assume $L \ge 1$.
	\\
	\\
	\textbf{Assumption 4} \textit{
		There exists a Borel measurable function $\hat{g}: \mathbb{R}^{d} \times \mathcal{X} \to \mathbb{R}^{d}$ where for each $\theta \in \mathbb{R}^{d}, X \in \mathcal{X}$,
		\begin{align*}
			\hat{g}\left(\theta, X\right) - P_{\theta}\hat{g}\left(\theta, X\right) = g\left(\theta, X\right) - \bar{g}\left(\theta\right).
		\end{align*}
	}
	\\
	\textbf{Assumption 5} \textit{
		There exists $\constLPH{0} <‌ \infty$ and $\constLPH{1} < \infty$ such that, for all $\theta \in \mathbb{R}^{d}$ and $X \in \mathcal{X}$, one has $\norm{\hat{g}\left(\theta, X\right)} \le \constLPH{0}$, $\norm{P_{\theta}\hat{g}\left(\theta, X\right)} \le \constLPH{0}$. Moreover, for $\left(\theta, \theta^{\prime}\right) \in \mathcal{H}^{2}$,
		\begin{align*}
			\sup_{X \in \mathcal{X}}\norm{P_{\theta}\hat{g}\left(\theta, X\right) - P_{\theta^{\prime}}\hat{g}\left(\theta^{\prime}, X\right)} \le \constLPH{1}\norm{\theta - \theta^{\prime}}.
		\end{align*}
	}
	\\
	\textbf{Assumption 6} \textit{
		For any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$, we have $\sup_{X \in \mathcal{X}}\norm{P_{\theta}\left(X, .\right) - P_{\theta^{\prime}}\left(X, .\right)}_{TV} \le L_{P}\norm{\theta - \theta^{\prime}}$.
	}
	\\
	\textbf{Assumption 7} \textit{
		For any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$, we have $\sup_{X \in \mathcal{X}}\norm{g\left(\theta, X\right) - g\left(\theta^{\prime}, X\right)} \le L_{H}\norm{\theta - \theta^{\prime}}$.
	}
	\\
	\textbf{Assumption 8} \textit{
		There exists $\rho < 1$, $K_{P} < \infty$ such that
		\begin{align*}
			\sup_{\theta \in \mathbb{R}^{d}, X \in \mathcal{X}} \norm{P_{\theta}^{n}\left(X, .\right) - \pi_{\theta}(.)}_{TV} \le \rho^{n}K_{P},
		\end{align*}
	}
	\\
	\textbf{Lemma 1} \textit{
		Assume that assumptions 6-8 hold. Then, for any $\theta \in \mathbb{R}^{d}$ and $X \in \mathcal{X}$,
		\begin{align*}
			\norm{\hat{g}\left(\theta, X\right)} \le \frac{\sigma K_{P}}{1 - \rho},
		\end{align*}
		\begin{align*}
			\norm{P_{\theta}\hat{g}\left(\theta, X\right)} \le \frac{\sigma \rho K_{P}}{1 - \rho}.
		\end{align*}
		Moreover, for any $\theta, \theta^{\prime} \in \mathbb{R}^{d}$ and $X \in \mathcal{X}$,
		\begin{align*}
			\norm{P_{\theta}\hat{g}\left(\theta, X\right) - P_{\theta^{\prime}}\hat{g}\left(\theta^{\prime}, X\right)} \le \norm{\theta - \theta^{\prime}},
		\end{align*}
		where
		\begin{align*}
			\constLPH{1} = \frac{K_{P}^{2}\sigma L_{P}}{(1 - \rho)^{2}}\left(2 + K_{P}\right) + \frac{K_{P}}{1 - \rho}L_{H}.
		\end{align*}
	}
	Proof of this lemma can be found in \cite{karimi2019non}, Lemma 7.
	\section{Error Bound}
	
	
	\subsection{Base Case}
	
	For the base case analysis, we can write:
	\begin{align*}
		& \Exs\left[\norm{\theta_{k +‌ 1} - \thetastar}^{2}\right] - \Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right] = \\
		& 2\stepsize \Exs\left[\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) \right\rangle\right] + \stepsize^{2}\Exs\left[\norm{g\left(\theta_{k}, X_{k + 1}\right)}^{2}\right] +‌ \stepsize^{2}\Exs\left[\norm{\xi_{k +‌ 1}\left(\theta_{k}\right)}^{2}\right] =\\
		& 2\stepsize\Exs\left[\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right)\right\rangle\right] + 2\stepsize\Exs\left[\left\langle \theta_{k} - \thetastar, \bar{g}\left(\theta_{k}\right) \right\rangle\right] + \stepsize^{2}\Exs\left[\norm{g\left(\theta_{k}, X_{k + 1}\right)}\right] + \stepsize^{2}\Exs\left[\norm{\xi_{k + 1}\left(\theta_{k}\right)}^{2}\right].
	\end{align*}
	It is easy to see that under Strong Monotonicity assumption, we have
	\begin{align*}
		\left\langle \theta_{k} - \thetastar, \bar{g}\left(\theta_{k}\right)\right\rangle = \left\langle \theta_{k} - \thetastar, \bar{g}\left(\theta_{k}\right) - \bar{g}\left(\thetastar\right)\right\rangle \le -\mu\norm{\theta_{k} - \thetastar}^{2}.
	\end{align*}
	Additionally, under Assumption 1 and 3, we have the following upper bound
	\begin{align*}
		&‌ \stepsize^{2}\left(\Exs\left[\norm{g\left(\theta_{k}, X_{k + 1}\right)}^{2}\right] + \Exs\left[\norm{\xi_{k + 1}\left(\theta_{k}\right)}^{2}\right]\right)\\
		& \le \stepsize^{2}\left(L_{1}^{2}\Exs\left[\left(\norm{\theta_{k} - \thetastar} + 1\right)^{2}\right] + L_{2}^{2}\Exs\left[\left(\norm{\theta_{k} - \thetastar} + 1\right)^{2}\right]\right)\\
		& \le 2\stepsize^{2}L^{2}\left(\Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right] + 1\right).
	\end{align*}
	\\
	Therefore, we have
	\begin{align*}
		\Exs\left[\norm{\theta_{k +‌ 1} - \thetastar}^{2}\right] \le \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right] +‌ 2\stepsize^{2}L^{2} + 2\stepsize\Exs\left[\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right)\right\rangle\right]
	\end{align*}
	Solving this recursion gives us the following inequality:
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] & \le \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k + 1}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] \\
		& + \sum_{t = 0}^{k}\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}2\stepsize^{2}L^{2} \\
		& + \sum_{t = 0}^{k}2\stepsize\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k - t}\Exs\left[\left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right) \right\rangle\right].
	\end{align*}
	
	For notational simplicity we define $\gamma_{t} \coloneq 2\stepsize\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k - t}$ for $0 \le t \le k$.
	
	The second term above is just a geometric series. According to Lemma 12 of \cite{kaledin2020finite} this equals to $\frac{\stepsize L^{2}\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k + 1}\right]}{-\stepsize L^{2} + \mu}$.
	
	Now, we can upper bound the third summand using below decomposition:
	\begin{align*}
		\Exs\left[ \sum_{t = 0}^{k} \gamma_{t}\left\langle \theta_{t} - \thetastar, g(\theta_{t}, X_{t + 1}) - \bar{g}(\theta_{t}) \right\rangle \right] = \Exs\left[ A_{1} + A_{2} + A_{3} + A_{4} + A_{5}\right]
	\end{align*}
	with
	\begin{align*}
		A_{1} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, \hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) \right\rangle,\\
		A_{2} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) - P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1}, X_{t} \right) \right\rangle,\\
		A_{3} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \theta_{t - 1}, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1}, X_{t}\right) \right\rangle,\\
		A_{4} \coloneq & \sum_{t = 1}^{k}\left(\gamma_{t} - \gamma_{t - 1}\right)\left\langle \theta_{t - 1} - \thetastar, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1} - \thetastar, X_{t}\right) \right\rangle,\\
		A_{5} \coloneq & \gamma_{0}\left\langle \theta_{0} - \thetastar, \hat{g}\left(\theta_{0}, X_{0}\right) \right\rangle + \gamma_{k}\left\langle \theta_{k} - \thetastar, P_{\theta_{k}}\hat{g}\left(\theta_{k}, X_{k + 1}\right)\right\rangle
	\end{align*}
	
	For $A_{1}$, we note that $\hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right)$ is a martingale difference sequence [cf. ?] and therefore we have $\Exs[A_{1}] = 0$ by taking the total expectation.
	
	For $A_{2}$, applying Cauchy-Schwarz inequality and \ref{MCLipschitzness}, we have
	\begin{align*}
		A_{2} & \le \sum_{t = 1}^{k}\constLPH{1}\gamma_{t}\norm{\theta_{t} - \thetastar}\;\norm{\theta_{t} - \theta_{t - 1}}\\
		& = \sum_{t = 1}^{k}\stepsize \constLPH{1}\gamma_{t}\norm{\theta_{t} - \thetastar}\;\norm{g(\theta_{t}, X_{t + 1}) + \xi_{t +‌1}(\theta_{t})}\\
		& \le \sum_{t = 1}^{k}\stepsize \constLPH{1} \gamma_{t}\norm{\theta_{t} - \thetastar}\left( L_{1}\left(\norm{\theta_{t} - \thetastar} + 1\right) + L_{2}\left(\norm{\theta_{t} - \thetastar} + 1\right)\right)\\
		& \le \sum_{t = 1}^{k}\frac{\stepsize L \constLPH{1}\gamma_{t}}{2}\left(3\norm{\theta_{t} - \thetastar}^{2} + 1\right)
	\end{align*}
	
	
	
	
	where the third line follows from the Lipschitzness condition and the assumption of
	$$\Exs^{1 / 2}\left[\norm{\xi_{t + 1}\left(\theta_{t}\right)}^{2} | \mathcal{F}_{t}\right] \le L_{2}\left(\norm{\theta_{t}} + 1\right)$$
	also, last line follows from the identity $u \le \frac{1}{2}(1 + u^{2})$.
	
	For $A_{3}$, we obtain
	\begin{align*}
		A_{3} & \le \sum_{t = 1}^{k}\gamma_{t}\norm{\theta_{t} - \theta_{t - 1}} \; \norm{P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1}, X_{t}\right)}\\
		& \le \sum_{t = 1}^{k}\stepsize \constLPH{0}\gamma_{t}\norm{g\left(\theta_{t}, X_{t + 1}\right)‌ + \xi_{t + 1}(\theta_{t})}\\
		& \le \sum_{t = 1}^{k}\stepsize\constLPH{0}\gamma_{t}\left(L_{1}\left(\norm{\theta_{t} - \thetastar} + 1\right) + L_{2}\left(\norm{\theta_{t} - \thetastar} + 1\right)\right)\\
		& \le \sum_{t = 1}^{k}\stepsize L \constLPH{0}\gamma_{t}\left(\norm{\theta_{t} - \thetastar} + 1\right)
	\end{align*}
	where second line follows from \ref{MCLipschitzness} and third line is similarly done to the previous part, using Lipschitzness condition and noise assumption.
	
	For $A_{4}$, we have
	\begin{align*}
		A_{4} & \le \sum_{t = 1}^{k}|\gamma_{t} - \gamma_{t - 1}|\; \norm{\theta_{t - 1} - \thetastar} \; \norm{P_{\theta_{t - 1}}\hat{g}\left(\theta_{t- 1}, X_{t}\right)}\\
		& \le \sum_{t = 1}^{k}\constLPH{0}|\gamma_{t} - \gamma_{t - 1}| \; \norm{\theta_{t - 1} - \thetastar}
	\end{align*}
	
	Finally, for $A_{5}$, we obtain
	\begin{align*}
		A_{5} & \le \constLPH{0}\left(\gamma_{0}\norm{\theta_{0} - \thetastar} + \gamma_{k}\norm{\theta_{k} - \thetastar}\right)
	\end{align*}
	which follows from Cacuhy-Scwarz inequality and \ref{MCLipschitzness}.
	
	Combining the above terms and taking expectations, gives us:
	\begin{align*}
		\Exs\left[\sum_{t = 0}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1} - \bar{g}\left(\theta_{t}\right)\right)\right\rangle\right] \le & \sum_{t = 1}^{k}\frac{\stepsize L \constLPH{1}\gamma_{t}}{2}\left(1 + 3\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] \right) + \sum_{t = 1}^{k}\stepsize L \constLPH{0}\gamma_{t}\left(\Exs\left[\norm{\theta_{t} - \thetastar}\right]‌ + 1\right) +\\
		& \sum_{t = 0}^{k - 1}\constLPH{0}|\gamma_{t} - \gamma_{t + 1}| \; \Exs\left[\norm{\theta_{t} - \thetastar}\right] + \constLPH{0}\left(\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right]\right)
	\end{align*}
	now it should be noticed that as long as the $\alpha$ satisfies $\stepsize \le \frac{\mu}{L^{2}}$, we have $\gamma_{t} \le \gamma_{t + 1}$. Thus, we can simplify the above upper bound and write it this way:
	\begin{align*}
		\Exs\left[\sum_{t = 0}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1} - \bar{g}\left(\theta_{t}\right)\right)\right\rangle\right] \le & \sum_{t = 1}^{k}\frac{\stepsize L \constLPH{1}\gamma_{t}}{2}\left(1 + 3\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] \right) +\\
		& \sum_{t = 1}^{k - 1}\constLPH{0}\left(\left(\stepsize L - 1\right)\gamma_{t} + \gamma_{t + 1}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
		& \sum_{t = 1}^{k}\stepsize L \constLPH{0}\gamma_{t} +  \constLPH{0}\left(\gamma_{1}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \left(\stepsize L + 1\right)\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right]\right)
	\end{align*}
	
	Hence, using the derived upper bounds from the above terms, we have:
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] \le & \sum_{t = 1}^{k}\frac{\stepsize L \constLPH{1}\gamma_{t}}{2}\left(1 + 3\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] \right) + \sum_{t = 1}^{k - 1}\constLPH{0}\left(\left(\stepsize L - 1\right)\gamma_{t} + \gamma_{t + 1}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
		& \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{1}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \left(\stepsize L + 1\right)\constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] +\\ 
		& \left(\frac{L}{\constLPH{0}} + 1\right)\frac{\gamma_{1}\left(1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k}\right)}{\left(1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\right)} + \frac{\stepsize L^{2}\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k + 1}\right]}{-\stepsize L^{2} + \mu}\\
	\end{align*}
	for further notation simplicity we define $c_{1, t} \coloneq \left(\frac{L}{\constLPH{0}} + 1\right)\frac{\gamma_{1}\left(1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}\right)}{\left(1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\right)} + \frac{\stepsize L^{2}\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t + 1}\right]}{-\stepsize L^{2} + \mu}$ for $0 \le t \le k$. Now to write down this upper bound in a way in which it only depends on $\norm{\theta_{0} - \thetastar}$ related terms and constants, we can write:
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] \le & \sum_{t = 1}^{k}\left[\frac{3\stepsize L \constLPH{1}\gamma_{t}}{2}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \frac{\stepsize L \constLPH{1}\gamma_{t}}{2}\right] + \sum_{t = 1}^{k - 1}\constLPH{0}\left(\left(\stepsize L - 1\right)\gamma_{t} + \gamma_{t + 1}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
		& \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{1}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \left(\stepsize L + 1\right)\constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] + c_{1, k}\\
		= & \sum_{t = 1}^{k}\frac{3\stepsize L \constLPH{1}\gamma_{t}}{2}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \frac{\stepsize L \constLPH{1}}{2}\sum_{t = 1}^{k}\gamma_{t} + \sum_{t = 1}^{k - 1}\constLPH{0}\left(\left(\stepsize L - 1\right)\gamma_{t} + \gamma_{t + 1}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
		& \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{1}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \left(\stepsize L + 1\right)\constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] + c_{1, k}\\
		= & \sum_{t = 1}^{k}\frac{3\stepsize L \constLPH{1}\gamma_{t}}{2}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \sum_{t = 1}^{k - 1}\constLPH{0}\left(\left(\stepsize L - 1\right)\gamma_{t} + \gamma_{t + 1}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
		& \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{1}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \left(\stepsize L + 1\right)\constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] + c_{1, k} + \\ 
		& \frac{L \constLPH{1}\gamma_{1}\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k}\right]}{4\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\right]}\\
	\end{align*}
	where the last equality follows from the definition of $\gamma_{t}$s. Similarly we define $c_{2, t} \coloneq \frac{L \constLPH{1}\gamma_{1}\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}\right]}{4\left[1 - \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\right]}$ for $0 \le t \le k$. So we can write it as
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] \le & \sum_{t = 1}^{k}\frac{3\stepsize L \constLPH{1}\gamma_{t}}{2}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \sum_{t = 1}^{k - 1}\constLPH{0}\left(\left(\stepsize L - 1\right)\gamma_{t} + \gamma_{t + 1}\right)\Exs\left[\norm{\theta_{t} - \thetastar}\right] +\\
		& \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)\gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \constLPH{0}\gamma_{1}\Exs\left[\norm{\theta_{0} - \thetastar}\right] + \left(\stepsize L + 1\right)\constLPH{0}\gamma_{k}\Exs\left[\norm{\theta_{k} - \thetastar}\right] + c_{1, k} + c_{2, k}
	\end{align*}
	\\
	Now for the second term on RHS, we note that
	\begin{align*}
		\big( \stepsize L - 1 \big) \gamma_{t} + \gamma_{t + 1} \leq \stepsize L \gamma_{t + 1}, \quad \Exs  \big[ \norm{\theta_{t} - \thetastar} \big] \leq \sqrt{ \Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big]},
	\end{align*}
	and consequently
	\begin{align*}
		&\frac{1}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^k} \sum_{t = 1}^{k - 1} \constLPH{0} \big( ( \stepsize L - 1 \big) \gamma_{t} + \gamma_{t + 1}\big)\Exs \big[ \norm{\theta_{t} - \thetastar} \big] \\
		&\leq 2 \constLPH{0} L \stepsize^2  \sum_{t = 1}^{k - 1} \frac{1}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^{t + 1}} \sqrt{\Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big]}\\
		&\leq 2 \constLPH{0} L \stepsize^2  \Big(  \sum_{t = 1}^{k - 1} \frac{1}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^{t + 1}} \Big)^{1/2} \Big( \sum_{t = 1}^{k - 1} \frac{1}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^{t + 1}} \Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big] \Big)^{1/2}\\
		&\leq 2 \constLPH{0} L \stepsize^2 \cdot \sum_{t = 1}^{k - 1} \frac{1}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^{t + 1}} \Exs \big[ \norm{\theta_{t} - \thetastar}^2 \big] + \frac{1}{-\stepsize L^{2} + \mu} \cdot \frac{2 \constLPH{0} L \stepsize}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^k}.
	\end{align*}
	We also note that
	\begin{align*}
		\frac{\gamma_k}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^k} \Exs \big[ \norm{\theta_k - \thetastar} \big] \leq \stepsize \frac{\Exs \big[ \norm{\theta_k - \thetastar}^2 \big]}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^k} + \frac{\stepsize}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^k}.
	\end{align*}
	similarly
	\begin{align*}
		\frac{\gamma_1}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^k} \Exs \big[ \norm{\theta_0 - \thetastar} \big] \leq \stepsize \frac{\Exs \big[ \norm{\theta_0 - \thetastar}^2 \big]}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^1} + \frac{\stepsize}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^1}.
	\end{align*}
	and we also define for $0 \le t \le k$
	\begin{align*}
		c_{3, t} \coloneq \frac{1}{-\stepsize L^{2} + \mu}\frac{2\stepsize\constLPH{0}L}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}} + \frac{\stepsize\left(\stepsize L + 1\right)\constLPH{0}}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}} + \frac{\stepsize\constLPH{0}}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)}
	\end{align*}
	to wrap up all the remainder terms.
	
	Substituting back and rearranging with also defining $c_{2, t}^{\prime} \coloneq \frac{c_{2, t}}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}}$ and $c_{1, t}^{\prime} \coloneq \frac{c_{1, t}}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}}$, yields
	\begin{align*}
		\frac{\Exs \big[ \norm{\theta_{k + 1} - \thetastar}^2 \big]}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^k} & \leq  \frac{\stepsize \left(\left(\stepsize L + 1\right)\constLPH{0} + \frac{3}{2}L \constLPH{1}\right)}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k}}\Exs\left[\norm{\theta_{k} - \thetastar}^{2}\right] + \sum_{t = 1}^{k - 1}\frac{\stepsize\left(\frac{3}{2}L \constLPH{1} + 2\stepsize\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{-1}L\constLPH{0}\right)}{\left(1 - 2\stepsize \left(-\stepsize L^{2} + \mu\right)\right)^{t}}\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right] + \\
		& \left(\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right) + \stepsize L \constLPH{1}\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{-1}\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] +‌ c_{1, k}^{\prime} + c_{2, k}^{\prime} + c_{3, k}.
	\end{align*}
	for sufficiently small $\stepsize$s, we have
	\begin{align*}
		\stepsize\left(\frac{3}{2}L \constLPH{1} + 2\stepsize\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{-1}L\constLPH{0}\right) \le 2\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + L \constLPH{1}\right)
	\end{align*}
	and again in a similar fashion we have
	\begin{align*}
		\left(\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right) + \stepsize L \constLPH{1}\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{-1}\right) \le 2\stepsize L \constLPH{1} + 1
	\end{align*}
	using above simplifications we can rewrite our upper bound as
	\begin{align*}
		\frac{\Exs \big[ \norm{\theta_{k + 1} - \thetastar}^2 \big]}{(1 - 2 \stepsize (-\stepsize L^2 + \mu))^k} & \leq 2\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + L \constLPH{1}\right)\sum_{t = 1}^{k}\frac{\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize \left(-\stepsize L^{2} + \mu\right)\right)^{t}} + \left(2\stepsize L \constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] +‌ c_{1, k}^{\prime} + c_{2, k}^{\prime} + c_{3, k}.
	\end{align*}
	
	\pagebreak
	
	For solving the above recursion, we first define $S_{t} \coloneq 2\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + L \constLPH{1}\right)\sum_{l = 1}^{t}\frac{\Exs\left[\norm{\theta_{l} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{l}}$ for $1 \le t \le k$. Also we use $C_{t} \coloneq c_{1, t}^{\prime} + c_{2, t}^{\prime} + c_{3, t}$ and for $0 \le t \le k$, defining constant terms. Now we can write
	\begin{align*}
		\frac{\Exs\left[\norm{\theta_{t + 1} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}} \le S_{t} + \left(2\stepsize L \constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] +‌ C_{t}.‌
	\end{align*}
	using this expansion, we should first notice that
	\begin{align*}
		\frac{S_{t}}{S_{t - 1}} & = \frac{S_{t - 1} + 2\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + L \constLPH{1}\right)\frac{\Exs\left[\norm{\theta_{t} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{t}}}{S_{t - 1}} \\
		& = 1 + 2\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + L \constLPH{1}\right)\frac{S_{t - 1} + \left(2\stepsize L \constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + C_{t - 1}}{S_{t - 1}} \\
		& \leq 1 + 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + L \constLPH{1}\right).
	\end{align*}
	Now, since we have $S_{0} = 0$ and
	\begin{align*}
		S_{1} = 2\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + L \constLPH{1}\right)\frac{\Exs\left[\norm{\theta_{1} - \thetastar}^{2}\right]}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)} = \frac{2\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + L \constLPH{1}\right)\left(\left(2\stepsize L \constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + C_{0}\right)}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)}
	\end{align*}
	it follows that $S_{t} \leq \left(1 + 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + L \constLPH{1}\right)\right)^{t - 1}\left[\frac{2\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + L \constLPH{1}\right)\left(\left(2\stepsize L \constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + C_{0}\right)}{\left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)}\right].$ Substituting this upper bound into previous equations we get
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] &\leq\\
		& \left(1 - 2\stepsize\left(-\stepsize L^{2} + \mu\right)\right)^{k - 1}\left(1 + 4\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + L \constLPH{1}\right)\right)^{k}\left[2\stepsize\left(\left(\stepsize L + 1\right)\constLPH{0} + L \constLPH{1}\right)\left(\left(2\stepsize L \constLPH{1} + 1\right)\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + C_{0}\right)\right].
	\end{align*}
	
	Choosing $\stepsize$ sufficiently small, we can simplify the above inequality and write it as follows
	
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2}\right] \leq \tilde{c}_{1} \cdot \left(1 - 2\stepsize \mu\right)^{k}\Exs\left[\norm{\theta_{0} - \thetastar}^{2}\right] + \tilde{c}_{2} \cdot 2\stepsize L\left(\constLPH{0} + \constLPH{1}\right)
	\end{align*}
	where $\tilde{c}_{1}$ and $\tilde{c}_{2}$ are $\mathcal{O}\left(1\right)$ constants.
	
	
	
	
	
	\subsection{General Case}
	In this case, we assume that the moment bound in [??] has been proven for $k \le n - 1$, we now proceed to show that the desired moment convergence holds for $n$ with $2 \le n \le p$.
	
	We start with the following decomposition of $\norm{\theta_{k + 1} - \thetastar}^{2n}$
	\begin{align*}
		\norm{\theta_{k + 1} - \thetastar}^{2n} & = \left(\norm{\theta_{k} - \thetastar}^{2} + 2\stepsize \left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)\right\rangle + \stepsize^{2}\norm{g\left(\theta_{x}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)}^{2} \right)^{n}\\
		& = \sum_{\substack{i, j, l \\ i + j + l = n}} \binom{n}{i, j, l}\norm{\theta_{k} - \thetastar}^{2i}\left(2\stepsize \left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)\right\rangle \right)^{j}\left(\stepsize \norm{g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right)}\right)^{2l}
	\end{align*}
	We note the following cases.
	\begin{enumerate}
		\item $i = n$, $j = l = 0$. In this case, the summand is simply $\norm{\theta_{k} - \thetastar}^{2i}$.
		\item When $i = n - 1, j = 1$ and $l = 0$. In this case, the summand is of order $\stepsize$, i.e., $$\stepsize \cdot 2n\left \langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right) \right\rangle^{j} \norm{\theta_{k} - \thetastar}^{2(n - 1)}.$$ We can further decompose it as
		\begin{align*}
			& 2n\stepsize \left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) + \xi_{k + 1}\left(\theta_{k}\right) \right\rangle\norm{\theta_{k} - \thetastar}^{2(n - 1)} \\
			& = \underbrace{2n\stepsize\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k+ 1}\right) - \bar{g}\left(\theta_{k}\right) + \xi_{k + 1}\left(\theta_{k}\right) \right\rangle \norm{\theta_{k} - \thetastar}^{2(n - 1)}}_{\constT{1}} + \underbrace{2n\stepsize \left\langle \theta_{k} - \thetastar, \bar{g}\left(\theta_{k}\right) \right\rangle \norm{\theta_{k} - \thetastar}^{2(n - 1)}}_{\constT{2}}.
		\end{align*}
		Note that, when $\left(X_{k}\right)$ is i.i.d or from a martingale noise, we have
		$$\Exs\left[\constT{1} | \theta_{k}\right] = 0$$
		However, when $\left(X_{k}\right)$ is Markovian, the above inequality does not hold and $\constT{1}$ requires careful analysis.\\
		Nonetheless, under the strong monotonicity assumption, we have
		$$\constT{2} \le -2n\stepsize\mu\norm{\theta_{k} - \thetastar}^{2n}.$$
		\item For the remaining terms, we see that they are of higher orders of $\stepsize$. Therefore, when $\stepsize$ is selected sufficiently small, these terms do not raise concern. 
	\end{enumerate}
	
	Therefore, to prove the desired moment bound, we spend the remaining section analyzing $\constT{1}$. Immediately, we note that
	\begin{align*}
		\Exs\left[\constT{1}\right] &= \Exs\left[2n\stepsize \left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right) + \Exs\left[\xi_{k + 1}\left(\theta_{k}\right) | \theta_{k}\right] \right\rangle\norm{\theta_{k} - \thetastar}^{2(n - 1)}\right]\\
		& = 2n\stepsize\Exs\left[\underbrace{\left\langle \theta_{k} - \thetastar, g\left(\theta_{k}, X_{k + 1}\right) - \bar{g}\left(\theta_{k}\right)\right\rangle \norm{\theta_{k} - \thetastar}^{2(n - 1)}}_{\constTprime{1}}\right].
	\end{align*}
	Subsequently, we focus on analyzing $\constTprime{1}$; but before that, we write the general recursion of the error bound. First, we define $\constTprime{1, t} \coloneq \left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right)\norm{\theta_{t} - \thetastar}^{2(n - 1)}\right\rangle$ to make $\constTprime{1}$ dependent on the iteration index. Now, following the above decomposition and taking the expectations, we have:
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2n}\right] \le \Exs\left[\norm{\theta_{k} - \thetastar}^{2n}\right] + \Exs\left[\constTprime{1, k}\right] - 2n\stepsize\mu\Exs\left[\norm{\theta_{k} - \thetastar}^{2n}\right] + o\left(\stepsize\right) = \left(1 - 2n\stepsize\mu\right)\Exs\left[\norm{\theta_{k} - \thetastar}^{2n}\right] + \Exs\left[\constTprime{1, k}\right] + o\left(\stepsize\right)
	\end{align*}
	similarly to the previous case we define $\gamma_{t} \coloneq 2n\stepsize\left(1 - 2n\stepsize\mu\right)^{k - t}$ for $0 \le t \le k$. Solving the above recursion will give us
	\begin{align*}
		\Exs\left[\norm{\theta_{k + 1} - \thetastar}^{2n}\right] \le \sum_{t = 0}^{k}\gamma_{t}\Exs\left[\constTprime{1, t}\right] + \gamma_{0}\Exs\left[\norm{\theta_{0} - \thetastar}^{2n}\right] + o\left(\stepsize\right)
	\end{align*}
	
	We have to upper bound the first term in the RHS above. For this purpose, we use a similar decomposition to our base case analysis:
	\begin{align*}
		\sum_{t = 0}^{k}\gamma_{t}\Exs\left[\constTprime{1, t}\right] = \left[\sum_{t = 0}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, g\left(\theta_{t}, X_{t + 1}\right) - \bar{g}\left(\theta_{t}\right)\right\rangle\norm{\theta_{t} - \thetastar}^{2(n - 1)}\right] = \Exs\left[ A_{1} + A_{2} + A_{3} + A_{4} + A_{5}\right]
	\end{align*} 
	with
	\begin{align*}
		A_{1} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, \hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) \right\rangle\norm{\theta_{t} - \thetastar}^{2(n - 1)},\\
		A_{2} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \thetastar, P_{\theta_{t}}\hat{g}\left(\theta_{t}, X_{t}\right) - P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1}, X_{t} \right) \right\rangle\norm{\theta_{t} - \thetastar}^{2(n - 1)},\\
		A_{3} \coloneq & \sum_{t = 1}^{k}\gamma_{t}\left\langle \theta_{t} - \theta_{t - 1}, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1} X_{t}\right) \right\rangle \norm{\theta_{t} - \thetastar}^{2(n - 1)}\\
		A_{4} \coloneq & \sum_{t = 1}^{k}\left(\gamma_{t} - \gamma_{t - 1}\right)\left\langle \theta_{t - 1} - \thetastar, P_{\theta_{t - 1}}\hat{g}\left( \theta_{t - 1} - \thetastar, X_{t}\right) \right\rangle\norm{\theta_{t} - \thetastar}^{2(n - 1)},\\
		A_{5} \coloneq & \sum_{t = 1}^{k}\gamma_{t - 1}\left\langle \theta_{t - 1} - \thetastar, P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1} - \thetastar, X_{t}\right)\right\rangle\left(\norm{\theta_{t} - \thetastar}^{2(n - 1)} - \norm{\theta_{t - 1} - \thetastar}^{2(n - 1)}\right)\\
		A_{6} \coloneq & \gamma_{0}\left\langle \theta_{0} - \thetastar, \hat{g}\left(\theta_{0}, X_{0}\right) \right\rangle\norm{\theta_{0} - \thetastar}^{2(n - 1)} + \gamma_{k}\left\langle \theta_{k} - \thetastar, P_{\theta_{k}}\hat{g}\left(\theta_{k}, X_{k + 1}\right)\right\rangle\norm{\theta_{k} - \thetastar}^{2(n - 1)}
	\end{align*}
	For $A_{1}$, we note that $\hat{g}\left(\theta_{t}, X_{t + 1}\right) - P_{\theta_{t}} \hat{g}\left(\theta_{t}, X_{t}\right)$ is a martingale difference sequence [cf. ?] and therefore we have $\Exs\left[A_{1}\right] = 0$ by taking the total expectation.
	
	For $A_{2}$, applying Cauchy-Schwarz inequality and ??, we have
	\begin{align*}
		A_{2} & \leq \sum_{t = 1}^{k}\constLPH{1}\gamma_{t}\norm{\theta_{t} - \thetastar}^{2n - 1} \; \norm{\theta_{t} - \theta_{t - 1}}\\
		& = \sum_{t = 1}^{k}\stepsize \constLPH{1}\gamma_{t}\norm{\theta_{t} - \thetastar}^{2n - 1} \; \norm{g\left(\theta_{t}, X_{t +‌ 1}\right) + \xi_{t + 1}\left(\theta_{t}\right)}\\
		& \leq \sum_{t = 1}^{k}\stepsize \constLPH{1}\gamma_{t}\norm{\theta_{t} - \thetastar}^{2n - 1}\left(L_{1}\left(\norm{\theta_{t} - \thetastar} + 1\right) + L_{2}\left(\norm{\theta_{t} - \thetastar} + 1\right)\right)\\
		& \leq \sum_{t = 1}^{k}\frac{\stepsize L \constLPH{1}\gamma_{t}}{2}\left(3\norm{\theta_{t} - \thetastar}^{2n} + 1\right)
	\end{align*}
	where the third line follows from the Lipschitzness condition and the assumption of
	\begin{align*}
		\Exs^{1 / 2}\left[\norm{\xi_{t + 1}\left(\theta_{t}\right)}^{2} | \mathcal{F}_{t}\right] \leq L_{2}\left(\norm{\theta_{t}} + 1\right).
	\end{align*}
	Also, last line follows from the identity $u \leq \frac{1}{2}\left(1 + u^{2}\right)$.
	
	For $A_{3}$, we obtain
	\begin{align*}
		A_{3} & \leq \sum_{t = 1}^{k}\gamma_{t}\norm{\theta_{t} - \theta_{t - 1}} \; \norm{P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1}, X_{t}\right)} \; \norm{\theta_{t} - \thetastar}^{2(n - 1)}\\
		& \leq \sum_{t = 1}^{k}\stepsize \constLPH{0}\gamma_{t}\norm{g\left(\theta_{t}, X_{t + 1}\right) + \xi_{t + 1}\left(\theta_{t}\right)} \; \norm{\theta_{t} - \thetastar}^{2(n - 1)}\\
		& \leq \sum_{t = 1}^{k}\stepsize \constLPH{0}\gamma_{t}\left(L_{1}\left(\norm{\theta_{t} - \thetastar} + 1\right) + L_{2}\left(\norm{\theta_{t} - \thetastar} + 1\right)\right) \; \norm{\theta_{t} - \thetastar}^{2(n - 1)}\\
		& \leq \sum_{t = 1}^{k}\frac{\stepsize L \constLPH{0}\gamma_{t}}{2}\left(3\norm{\theta_{t} - \thetastar}^{2n - 1} + 1\right)
	\end{align*}
	where second line follows from ?? and third line is similarly done to the previous part, using Lipschitzness condition and noise assumption.
	
	For $A_{4}$, we have
	\begin{align*}
		A_{4} & \leq \sum_{t = 1}^{k}| \gamma_{t} - \gamma_{t - 1}| \; \norm{\theta_{t - 1} - \thetastar} \norm{P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1}, X_{t}\right)} \; \norm{\theta_{t} - \thetastar}^{2(n - 1)}\\
		& \leq \sum_{t = 1}^{k}\constLPH{0}| \gamma_{t} - \gamma_{t - 1}| \; \norm{\theta_{t - 1} - \thetastar} \; \norm{\theta_{t} - \thetastar}^{2(n - 1)}\\
		& = \sum_{t = 1}^{k}\constLPH{0}|\gamma_{t} - \gamma_{t - 1}| \; \norm{\theta_{t - 1} - \theta_{t} + \theta_{t} - \thetastar} \; \norm{\theta_{t} - \thetastar}^{2(n - 1)}\\
		& \leq \sum_{t = 1}^{k}\constLPH{0}|\gamma_{t} - \gamma_{t - 1}| \;\left(\norm{\theta_{t - 1} - \theta_{t}} +‌ \norm{\theta_{t} - \thetastar}\right)\norm{\theta_{t} - \thetastar}^{2(n - 1)}\\
		& \leq \sum_{t = 1}^{k}\constLPH{0}|\gamma_{t} - \gamma_{t - 1}|\left(\frac{\stepsize L}{2}\left(3\norm{\theta_{t} - \thetastar}^{2n - 1} + 1\right) + \norm{\theta_{t} - \thetastar}^{2n - 1}\right)\\
		& \leq \sum_{t = 1}^{k}4\constLPH{0}|\gamma_{t} - \gamma_{t - 1}|\norm{\theta_{t} - \thetastar}^{2n - 1}
	\end{align*}
	where the fourth line follows from the triangle inequality, fifth line uses a similar argument to the previous part and the last line uses the fact that $\stepsize < \frac{1}{L}$.
	
	Now for $A_{5}$, we have to first note that, using mean-value theorem and with $a \in [0, 1]$, we'll get
	\begin{align*}
		\norm{\theta_{t} - \theta^{*}}^{2(n - 1)} - \norm{\theta_{t - 1} - \thetastar}^{2(n - 1)} & = \norm{\theta_{t} - \theta_{t - 1}} \cdot 2(n - 1)\norm{a\left(\theta_{t} - \thetastar\right) + (1 - a)\left(\theta_{t - 1} - \thetastar\right)}^{2n - 3}\\
		& \leq \norm{\theta_{t} - \theta_{t - 1}} \cdot 2(n - 1)\norm{a\left(\theta_{t} - \theta_{t - 1}\right) + \theta_{t - 1} - \thetastar}^{2n - 3}\\
		& \leq 2^{2n - 3}(n - 1)\norm{\theta_{t} - \theta_{t - 1}}\left(\norm{\theta_{t} - \theta_{t - 1}}^{2n - 3} + \norm{\theta_{t - 1} - \thetastar}^{2n - 3}\right)
	\end{align*}
	where the last line follows using the mclaurin's inequality. Plugging in the above upper bound to $A_{5}$ gives us
	\begin{align*}
		A_{5} & \leq \sum_{t = 1}^{k}2^{2n - 3}(n - 1)\gamma_{t - 1}\norm{\theta_{t} - \theta_{t - 1}}\left\langle \theta_{t - 1} - \thetastar, P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1} - \thetastar, X_{t}\right)\right\rangle\left(\norm{\theta_{t} - \theta_{t - 1}}^{2n - 3} + \norm{\theta_{t - 1} - \thetastar}^{2n - 3}\right)\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}(n - 1)\gamma_{t - 1}\norm{\theta_{t - 1} - \thetastar} \; \norm{P_{\theta_{t - 1}}\hat{g}\left(\theta_{t - 1}, X_{t}\right)} \; \norm{\theta_{t} - \theta_{t - 1}}\left(\norm{\theta_{t} - \theta_{t - 1}}^{2n - 3} + \norm{\theta_{t - 1} - \thetastar}^{2n - 3}\right)\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}(n - 1)\constLPH{0}\gamma_{t - 1}\norm{\theta_{t - 1} - \thetastar} \; \norm{\theta_{t} - \theta_{t - 1}}\left(\norm{\theta_{t} - \theta_{t - 1}}^{2n - 3} + \norm{\theta_{t - 1} - \thetastar}^{2n - 3}\right)\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}(n - 1)\stepsize L\constLPH{0}\gamma_{t - 1}\norm{\theta_{t - 1} - \thetastar}\left(\norm{\theta_{t} - \thetastar} + 1\right)\left(\norm{\theta_{t} - \theta_{t - 1}}^{2n - 3} + \norm{\theta_{t - 1} - \thetastar}^{2n - 3}\right)\\
		& \leq \sum_{t = 1}^{k} 2^{2n - 3}(n - 1)\stepsize L \constLPH{0}\gamma_{t - 1}\left(\norm{\theta_{t} - \theta_{t - 1}} + \norm{\theta_{t} - \thetastar}\right)\left(\norm{\theta_{t} - \thetastar} + 1\right)\left(\norm{\theta_{t} - \theta_{t - 1}}^{2n - 3} +‌ \norm{\theta_{t - 1} - \thetastar}^{2n - 3}\right)\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}(n - 1)\stepsize L \constLPH{0}\gamma_{t - 1}\left(\stepsize L + 1\right)\left(\norm{\theta_{t} - \thetastar} + 1\right)\left(\norm{\theta_{t} - \theta_{t - 1}}^{2n - 3} + \norm{\theta_{t - 1} - \thetastar}^{2n - 3}\right)\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}(n - 1)\stepsize L \constLPH{0}\gamma_{t - 1}\left(\stepsize L + 1\right)\left(\norm{\theta_{t} - \thetastar} + 1\right)\left(\stepsize^{2n - 3}L^{2n - 3}\left(\norm{\theta_{t} - \thetastar} + 1\right)^{2n - 3} + \norm{\left(\theta_{t - 1} - \theta_{t}\right) + \left(\theta_{t} - \thetastar\right)}^{2n - 3}\right)\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}(n - 1)\stepsize L \constLPH{0}\gamma_{t - 1}\left(\stepsize L + 1\right)\left(\norm{\theta_{t} - \thetastar} + 1\right)\left(\stepsize^{2n - 3}L^{2n - 3}\left(\norm{\theta_{t} - \thetastar} + 1\right)^{2n - 3} + 2^{2n - 4}\left(\norm{\theta_{t} - \theta_{t - 1}}^{2n - 3} + \norm{\theta_{t} - \thetastar}^{2n - 3}\right)\right)\\
		& \leq \sum_{t = 1}^{k}2^{2n - 3}(n - 1)\stepsize L \constLPH{0} \gamma_{t - 1}\left(\stepsize L + 1\right)\left(\norm{\theta_{t} - \thetastar} + 1\right)\left(\left(2^{2n - 4} + 1\right)\stepsize^{2n - 3}L^{2n - 3}\left(\norm{\theta_{t} - \thetastar} + 1\right)^{2n - 3} + 2^{2n - 4}\norm{\theta_{t} - \thetastar}^{2n - 3}\right)
	\end{align*}
	where in the fourth, sixth and seventh line we used $\norm{\theta_{t} - \theta_{t - 1}} \leq \stepsize L\left(\norm{\theta_{t} - \thetastar} + 1\right)$ which we have showed in our previous calculations. Also in the fifth and eight line, we used triangle inequality. Mclaurin's inequality is another time used in eight line too.
	
	Finally, for $A_{6}$, we obtain
	\begin{align*}
		A_{6} \leq \constLPH{0}\left(\gamma_{0}\norm{\theta_{0} - \thetastar}^{2n - 1} + \gamma_{k}\norm{\theta_{k} - \thetastar}^{2n - 1}\right)
	\end{align*}
	which follows from Cauchy-Schwarz inequality and ??.
	\bibliographystyle{ieee}
	\bibliography{ref.bib}
	
\end{document}